{
  
    
        "post0": {
            "title": "DiffEdit Paper Implementation",
            "content": "The goal of this notebook is to walk through the DiffEdit Paper and to implement the paper to the best of my understanding. While this task is primarily to help my own understanding of the process, I also want to help the reader of my post understand the process better as well. . Before I start, I want to give a big thanks to the DiffEdit authors (Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord) for publishing this paper. Without the openness and willingness to share, this type of implementation would not be possible. I also want to thank the fast.ai community for helping me solve problems when I was unclear about how to move forward. Finally, I want to thank Jonathan Whitaker, the auther of the Stable Diffusion Deep Dive notebook. This was the notebook that I started with in my DiffEdit implementation. . If anybody reading this blog post works in manufacturing and cares about improving product quality (or knows somebody that does), please reach out to me at kevin@problemsolversguild.com. . # !pip install -q --upgrade transformers diffusers ftfy pillow . Loading the models . This code (and that in the next section) comes from the Huggingface example notebook. . This will download and set up the relevant models and components we&#39;ll be using. Let&#39;s just run this for now and move on to the next section to check that it all works before diving deeper. . If you&#39;ve loaded a pipeline, you can also access these components using pipe.unet, pipe.vae and so on. . vae = AutoencoderKL.from_pretrained(&quot;CompVis/stable-diffusion-v1-4&quot;, subfolder=&quot;vae&quot;) # Load the tokenizer and text encoder to tokenize and encode the text. tokenizer = CLIPTokenizer.from_pretrained(&quot;openai/clip-vit-large-patch14&quot;) text_encoder = CLIPTextModel.from_pretrained(&quot;openai/clip-vit-large-patch14&quot;) # The UNet model for generating the latents. unet = UNet2DConditionModel.from_pretrained(&quot;CompVis/stable-diffusion-v1-4&quot;, subfolder=&quot;unet&quot;) # The noise scheduler scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=&quot;scaled_linear&quot;, num_train_timesteps=1000) # scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=&quot;scaled_linear&quot;, clip_sample=False, set_alpha_to_one=False) . vae = vae.to(torch_device) text_encoder = text_encoder.to(torch_device) unet = unet.to(torch_device); . The first thing we need to do is choose an image that we want to use as a starting point. I chose to go with a picture that was similar to one of the images used in the paper, but maybe a little harder. . p = FastDownload().download(&#39;https://negativespace.co/wp-content/uploads/2020/11/negative-space-horses-in-field-with-trees-1062x705.jpg&#39;) init_image = Image.open(p).convert(&quot;RGB&quot;) # init_image = init_image.resize((init_image.size[0]//2, init_image.size[1]//2)) init_image = init_image.resize((512,512)) init_image . Now that I have found an image, let&#39;s define the reference_text and the query_text. These are defined in the paper as R and Q. Let&#39;s follow the paper here and keep Q and R simple. . reference_text = &quot;Two horses&quot; query_text = &quot;Two zebras&quot; . A good amount of the code in the next few cells is coming from the StableDiffusionImg2ImgPipeline function in the diffusers library. This was very helpful in creating the implementation I ended up with . def preprocess(image): w, h = image.size w, h = map(lambda x: x - x % 32, (w, h)) # resize to integer multiple of 32 image = image.resize((w, h), resample=PIL.Image.Resampling.LANCZOS) image = np.array(image).astype(np.float32) / 255.0 image = image[None].transpose(0, 3, 1, 2) image = torch.from_numpy(image) return 2.0 * image - 1.0 . def get_text_embeddings(prompt, negative_prompt, tokenizer, text_encoder, do_classifier_free_guidance, device): #outputs text_embeddings # get prompt text embeddings text_inputs = tokenizer(prompt, padding=&quot;max_length&quot;, max_length=tokenizer.model_max_length, return_tensors=&quot;pt&quot;, truncation=True) text_input_ids = text_inputs.input_ids text_embeddings = text_encoder(text_input_ids.to(device))[0] # text_embeddings = text_embeddings.repeat_interleave(num_images_per_prompt, dim=0) if negative_prompt is None: uncond_tokens = [&quot;&quot;] else: uncond_tokens = negative_prompt max_length = text_input_ids.shape[-1] uncond_input = tokenizer(uncond_tokens, padding=&quot;max_length&quot;, max_length=max_length, return_tensors=&quot;pt&quot;, truncation=True) with torch.no_grad(): uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0] # duplicate unconditional embeddings for each generation per prompt # uncond_embeddings = uncond_embeddings.repeat_interleave(batch_size * num_images_per_prompt, dim=0) # For classifier free guidance, we need to do two forward passes. # Here we concatenate the unconditional and text embeddings into a single batch # to avoid doing two forward passes text_embeddings = torch.cat([uncond_embeddings, text_embeddings]) return text_embeddings . def get_timestamps(scheduler, num_inference_steps, strength, device): scheduler.set_timesteps(num_inference_steps) # get the original timestep using init_timestep offset = scheduler.config.get(&quot;steps_offset&quot;, 0) init_timestep = int(num_inference_steps * strength) + offset init_timestep = min(init_timestep, num_inference_steps) timesteps = scheduler.timesteps[-init_timestep] timesteps = torch.tensor([timesteps], device=device) t_start = max(num_inference_steps - init_timestep + offset, 0) return timesteps, t_start . def encode_image(init_image, latents_dtype, device): # encode the init image into latents and scale the latents init_image = preprocess(init_image) init_image = init_image.to(device=device, dtype=latents_dtype) with torch.no_grad(): init_latent_dist = vae.encode(init_image).latent_dist init_latents = init_latent_dist.sample(generator=generator) init_latents = 0.18215 * init_latents return init_latents . def img2noise(init_image, prompt, mask=None, strength = 0.5, num_inference_steps = 50, guidance_scale = 5, negative_prompt=None, generator = None, output_type = &quot;pil&quot;, return_dict = True, callback = None, callback_steps = 1, device=&#39;cuda&#39; ): do_classifier_free_guidance = guidance_scale &gt; 1.0 text_embeddings = get_text_embeddings(prompt, negative_prompt, tokenizer, text_encoder, do_classifier_free_guidance, device) latents_dtype=text_embeddings.dtype timesteps, t_start = get_timestamps(scheduler, num_inference_steps, strength, device) # encode the init image into latents and scale the latents init_latents = encode_image(init_image, latents_dtype, device) # add noise to latents using the timesteps noise = torch.randn(init_latents.shape, generator=generator, device=device, dtype=latents_dtype) noisy_latents = scheduler.add_noise(init_latents, noise, timesteps) latents = noisy_latents # Some schedulers like PNDM have timesteps as arrays # It&#39;s more optimized to move all timesteps to correct device beforehand timesteps = scheduler.timesteps[t_start:].to(device) noise_preds = torch.tensor([], device=&#39;cuda&#39;) for i, t in enumerate(timesteps): # expand the latents if we are doing classifier free guidance latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents latent_model_input = scheduler.scale_model_input(latent_model_input, t) # predict the noise residual with torch.no_grad(): noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample # perform guidance if do_classifier_free_guidance: noise_pred_uncond, noise_pred_text = noise_pred.chunk(2) noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond) noise_preds = torch.concat((noise_preds, noise_pred)) # compute the previous noisy sample x_t -&gt; x_t-1 latent_step = scheduler.step(noise_pred, t, latents) latents = latent_step.prev_sample if mask is not None: latents = mask*latents+(1-mask)*init_latents # noise_preds = torch.concat((noise_preds, noise_pred)) #This performs much worse than in the for-loop latents = 1 / 0.18215 * latents with torch.no_grad(): image = vae.decode(latents).sample image = (image / 2 + 0.5).clamp(0, 1) image = image.cpu().permute(0, 2, 3, 1).numpy() images = (image * 255).round().astype(&quot;uint8&quot;) pil_images = [Image.fromarray(image) for image in images] # for img in pil_images: display(img) return pil_images, noise_preds . . Estimate noise conditioned to Reference Text R . generator = torch.cuda.manual_seed(32) reference_noises = torch.tensor([], device=&#39;cuda&#39;) for _ in range(10): reference_pil, reference_noise = img2noise(init_image, strength=0.5, prompt=reference_text, generator=generator) reference_noises = torch.concat((reference_noises, reference_noise)) . Estimate noise conditioned to Query Q . generator = torch.cuda.manual_seed(32) query_noises = torch.tensor([], device=&#39;cuda&#39;) for _ in range(10): query_pil, query_noise = img2noise(init_image, strength=0.5, prompt=query_text, generator=generator) query_noises = torch.concat((query_noises, query_noise)) . View Latent Noise Channels . reference_noises.mean(0).shape . torch.Size([4, 64, 64]) . fig, axs = plt.subplots(1, 4, figsize=(16, 4)) for c in range(4): axs[c].imshow(reference_noises.mean(0, keepdim=True)[0][c].cpu())#, cmap=&#39;Greys&#39;) . fig, axs = plt.subplots(1, 4, figsize=(16, 4)) for c in range(4): axs[c].imshow(query_noises.mean(0, keepdim=True)[0][c].cpu())#, cmap=&#39;Greys&#39;) . While there isn&#39;t much that looks interesting when looking at the reference_noises or query_noises individually, let&#39;s look at the difference between the two. . diff_noises = (reference_noises.mean(0, keepdim=True) - query_noises.mean(0, keepdim=True)) . fig, axs = plt.subplots(1, 4, figsize=(16, 4)) for c in range(4): axs[c].imshow(diff_noises[0][c].cpu())#, cmap=&#39;Greys&#39;) . Now, we are seeing some signs that the noise that is being removed is quite different over the horse area of the picture and pretty similar outside of that area. One thing to note on these channels is that some of them are darker surrounding the horses and some are lighter. . diff_noises.min(), diff_noises.max() . (tensor(-0.8503, device=&#39;cuda:0&#39;), tensor(0.6133, device=&#39;cuda:0&#39;)) . One thing I&#39;ve found improves this is to determine the distance away from the midpoint, so I take the absolute value to make sure the intensity, not the direction, is being taken into consideration. The thought here is that whether the zebra query or the horse reference is activating that noise a lot, it is probably a pixel we should include in the mask. . diff_noises_abs = diff_noises.abs() . fig, axs = plt.subplots(1, 4, figsize=(16, 4)) for c in range(4): axs[c].imshow(diff_noises_abs[0][c].cpu())#, cmap=&#39;Greys&#39;) . Now, let&#39;s scale all of our values to be between 0 and 1 by subtracting the smallest value and then dividing everything by the largest value. I am not removing any values when doing this although I will note here that the paper does say to &quot;remove extreme values in noise predictions&quot;. I wasn&#39;t sure how to go about doing this. . diff_noise_normed = (diff_noises_abs - diff_noises_abs.min())/(diff_noises_abs - diff_noises_abs.min()).max() . diff_noise_normed.min(), diff_noise_normed.max() . (tensor(0., device=&#39;cuda:0&#39;), tensor(1., device=&#39;cuda:0&#39;)) . . mask = diff_noise_normed.mean(dim=1).squeeze() . . . . . import cv2 . The next step is to binarize the mask so that everything is either a 0 or a 1. To acheive this, I used Otsu thresholding which I believe deviates from the version that is used in the paper which seems more straightforward: &quot;binarized with a threshold, which we set to 0.5 by default&quot;. I was never able to get a mask that I was happy with when using that strategy. . def extract_channel_mask(img, do_inverse=False): kernel = np.ones((3,3),np.uint8) img = (img*255).squeeze().cpu().to(torch.uint8).numpy() if do_inverse: ret2,img2 = cv2.threshold(img,0,1,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU) else: ret2,img2 = cv2.threshold(img,0,1,cv2.THRESH_BINARY+cv2.THRESH_OTSU) opening = cv2.dilate(img2, kernel) return opening . final_mask = extract_channel_mask(mask) . plt.imshow(final_mask) . &lt;matplotlib.image.AxesImage at 0x7fd0526a19d0&gt; . import torch.nn.functional as F . Since our mask is generated in Latent Space, we have to find a way to go from latent space back to the original image space. For this, I decided to use F.interpolate. . horse_sized_mask = F.interpolate(torch.from_numpy(final_mask).unsqueeze(0).unsqueeze(0), mode=&#39;nearest&#39;, size=(init_image.size[1],init_image.size[0])).squeeze() . The mask came out looking pretty good for this easy prompt. The horses are definitely well masked and seem to match what would intuitively make sense. I have done some other prompts that didn&#39;t look good including trying to change the grass into sand and changing the forest into a mushroom forest. Both of these included the horses in the mask still. . plt.imshow(init_image) plt.imshow(horse_sized_mask.squeeze().cpu(), alpha=0.5) . &lt;matplotlib.image.AxesImage at 0x7fd052663eb0&gt; . Most of the interesting results from this paper are coming from step 1. The other two steps are hard to really build up to, but I will share them here. . Step 2: Encoding is replicating the step that was done before where we add noise back into the latent. This is already being done so there was no new code that was needed to accomplish this. This is the code where that step occurs: . # add noise to latents using the timesteps noise = torch.randn(init_latents.shape, generator=generator, device=device, dtype=latents_dtype) noisy_latents = scheduler.add_noise(init_latents, noise, timesteps) . Step 3: Decode with mask-wise correction is interesting, but also a step that doesn&#39;t add much code. The idea of step 3 is that anything outside of the mask that was generated from step 1 will be reverted to the original pixel value. So only the pixels that are inside of the masked areas will be changed which helps accomplish the goal of keeping most of the original image the same as before the prompt. . Here is the equation of the concept I described: $ tilde{y}_t = M*y_t + (1−M)*x_t$ where $y_t$ is the new image and $x_t$ is the original image (both encoded in latent space). . Let&#39;s look at the results: . for i in range(0,20): generator = torch.cuda.manual_seed(i) final_pil, _= img2noise(init_image, prompt=query_text, mask=torch.from_numpy((final_mask)).cuda(), generator=generator, strength=0.5) print(i) display(final_pil[0]) . 0 . 1 . 2 . 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . 13 . 14 . 15 . 16 . 17 . 18 . 19 . Overall, it seems that the process is doing what we would expect. I am seeing some decent results, but there is definitely still room for innovation in this space. If you noticed any typos or have any ideas for improving this blog post, please reach out to kevin@problemsolversguild.com and let me know! Thank you for reading and I hope this has been helpful in your learning process. .",
            "url": "https://blog.problemsolversguild.com/technical/research/2022/11/02/DiffEdit-Implementation.html",
            "relUrl": "/technical/research/2022/11/02/DiffEdit-Implementation.html",
            "date": " • Nov 2, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "5 Ways to Improve Your Quality Inspection Process",
            "content": "5 Ways to Improve Your Quality Inspection Process . If you work in manufacturing and are struggling to with rising costs, returned products, and unhappy customers, check out some of my ideas below on how to improve your quality inspection process. If you have any other ones to add, I would love to hear from you in the comments below! . 1. Identify KPIs and Loopholes . Key Performance Indicators (KPIs) are used to measure a process’ success. One problem with KPIs is that while they may track progress towards a goal, they can often be gamed and measurements can be tweaked to make a metric look better than it truly is. A good KPI is something that is automatically captured from machines and requires little if any human input. To help identify a good KPI, first ask your team what the goal is. For some groups, this may be reducing the amount of defective product shipped to a customer. Others may just want to generate the most value for the plant. These are both great goals, but they are too slow to be effective KPIs. Once you have the end goal in mind, dig a layer deeper. What actions go into improving that goal? Let’s use shipping less defective product to end customers as our example goal. This is a worthy goal, but how do you know if you are moving in the right direction? An effective KPI will help answer that question hourly or daily rather than quarterly. Maybe your plant currently has a lot of manual inspection throughout the production line. If this is the case, tracking the amount of product being discarded at each station may be a good KPI to track. A loophole here might be that more product is let through from one station to the next. Keep these potential loopholes in mind and mitigate them where possible. Each plant is different and the best KPIs to track will often differ. Consider what your goals are, what can be tracked to help reach that goal, and what types of unintended consequences could result. . 2. Create a Process Diagram . Work with your team to create a Swim Lane Diagram. This will give you a better idea what tasks need to be done by what team at what point in the process. If there are multiple tasks that are done in parallel, which task takes longer to complete? Focusing on the tasks and teams that are involved in a process will help to understand where resources should be put in order to speed up the overall process. . 3. Simplify the Tasks . “Make everything as simple as possible, but not simpler” - Albert Einstein . Once you have identified your process bottlenecks, work with your team to break the task down. The goal here is to ensure that any work that is being done is adding value to the process and requires a person. At this point, start considering whether a process can be either partially or fully automated. . 4. Focus on Quick Wins . One mistake that many companies make when improving their quality inspection process is that they try to tackle too hard of a problem right away. This is an advanced strategy and unless you have a very solid team, it is probably better to start with a smaller process and generate value quicker. Once your team has started improving the workflow, the larger processes will be easier to optimize. If you feel like there are no smaller processes that need to be optimized, this probably means that the tasks should be broken down into smaller pieces. . 5. Work with Your Subject Matter Experts . Don’t try to improve the process without working with the people that know the process best. There are typically years of expertise ready and willing to assist in process improvement. If you want to generate the highest return on investment from your quality inspection improvements, these team members will help you get there. . Conclusion . Hopefully these tasks give you some ideas on how you can improve your quality inspection process. If you are thinking about diving into your quality inspection process and you want an outside perspective to help you, I would love to hear from you. We can help you automate your quality inspection process whether you need deep learning to differentiate defects, a custom application built to make it easier to track your data, or a web scraping tool to retrieve data for you before you even request it. We will work with you no matter what stage of your journey you are in and will align our pricing to ensure we are both working towards the same goal .",
            "url": "https://blog.problemsolversguild.com/manufacturing/quality/2022/09/26/5-Ways-to-Improve-Your-Quality-Inspection-Process.html",
            "relUrl": "/manufacturing/quality/2022/09/26/5-Ways-to-Improve-Your-Quality-Inspection-Process.html",
            "date": " • Sep 26, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Exploring Timm vs Torchvision Resnet18 Difference",
            "content": "I was recently doing some experiments on imagenette testing out the new PolyLoss paper and I noticed that when I was running my baseline model using resnet18 from torchvision, I was consistently getting ~78% after 5 epochs, but that same baseline model was around 72% consistently when I used resnet18 from timm instead. For this post, I&#39;m going to stick to one run per model, but this really should use at least 5 runs to make sure the issue isn&#39;t a poorly seeded run. . The first thing I am going to do is import fastai&#39;s vision module and download imagenette which is a dataset to test techniques that is lighter than imagenet. . from fastai.vision.all import * imagenette_url = &#39;https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz&#39; data_path = untar_data(imagenette_url, archive=&#39;/data/archive&#39;, data=&#39;/data/data&#39;) . Timm Vanilla . Now, let&#39;s train a model using timm&#39;s resnet18 architecture. The newest version of fastai makes this really slick by allowing a user to pass resnet18 in as a string. This is a signal to fastai&#39;s vision_learner to look for this in the timm model library. . dls = ImageDataLoaders.from_folder(data_path, valid=&#39;val&#39;, item_tfms=Resize(256)) learn_timm = vision_learner(dls, &#39;resnet18&#39;, pretrained=False, metrics=accuracy) learn_timm.fit_one_cycle(5) . epoch train_loss valid_loss accuracy time . 0 | 2.412845 | 2.793077 | 0.292484 | 00:25 | . 1 | 1.847957 | 2.888684 | 0.314904 | 00:24 | . 2 | 1.413061 | 1.247371 | 0.608662 | 00:24 | . 3 | 1.124863 | 1.019361 | 0.675414 | 00:24 | . 4 | 0.963823 | 0.848421 | 0.726879 | 00:24 | . 72.7% accuracy when we run timm&#39;s resnet18 for 5 epochs. . TorchVision Vanilla . Now let&#39;s do the same training but instead, let&#39;s use torchvision&#39;s resnet18 which can be called by passing resnet18 to the vision_learner (not the string version). . dls = ImageDataLoaders.from_folder(data_path, valid=&#39;val&#39;, item_tfms=Resize(256)) learn_torchvision = vision_learner(dls, resnet18, pretrained=False, metrics=accuracy) learn_torchvision.fit_one_cycle(5) . epoch train_loss valid_loss accuracy time . 0 | 2.369731 | 3.082903 | 0.196178 | 00:24 | . 1 | 1.618597 | 2.213618 | 0.449936 | 00:24 | . 2 | 1.195862 | 2.348811 | 0.519236 | 00:24 | . 3 | 0.949387 | 0.815663 | 0.735541 | 00:25 | . 4 | 0.721939 | 0.674278 | 0.781656 | 00:24 | . 78.2% accuracy when we run torchvision&#39;s resnet18 for 5 epochs. . What is causing this difference? . This difference was pointed out in the fastai discord channel and Ross Wightman, the creator of timm had some ideas. The first was to try running this experiment multiple times due to variance. This was easy enough to test so I went ahead and saw a similar pattern for the next 5 runs. The next thing he mentioned was something called zero_init which I hadn&#39;t heard of before. The argument may be referred to as zero_init_residual or zero_init_last_bn. The timm library defaults this variable to True and torchvision defaults this to False. First, let&#39;s confirm that this difference fixes our discrepancy between timm and torchvision, then I&#39;ll explain what it is doing, and lastly I will explain which is the better option. . Timm zero_init_last_bn=False . dls = ImageDataLoaders.from_folder(data_path, valid=&#39;val&#39;, item_tfms=Resize(256)) learn_timm_no_zero = vision_learner(dls, &#39;resnet18&#39;, pretrained=False, metrics=accuracy, zero_init_last_bn=False) learn_timm_no_zero.fit_one_cycle(5) . epoch train_loss valid_loss accuracy time . 0 | 2.349849 | 2.469171 | 0.282803 | 00:24 | . 1 | 1.621092 | 3.614518 | 0.263949 | 00:24 | . 2 | 1.222227 | 1.627461 | 0.538599 | 00:24 | . 3 | 0.960997 | 0.816477 | 0.741401 | 00:24 | . 4 | 0.718745 | 0.682136 | 0.787516 | 00:24 | . TorchVision zero_init_residual=True . dls = ImageDataLoaders.from_folder(data_path, valid=&#39;val&#39;, item_tfms=Resize(256)) learn_tv_zero_bn = vision_learner(dls, partial(resnet18, zero_init_residual=True), pretrained=False, metrics=accuracy) learn_tv_zero_bn.fit_one_cycle(5) . epoch train_loss valid_loss accuracy time . 0 | 2.453036 | 3.386052 | 0.208408 | 00:24 | . 1 | 1.851731 | 2.367089 | 0.373758 | 00:24 | . 2 | 1.414447 | 1.292271 | 0.581401 | 00:24 | . 3 | 1.109155 | 0.982513 | 0.684331 | 00:24 | . 4 | 0.927408 | 0.845072 | 0.729682 | 00:24 | . So what is this option that is swinging our accuracy by 5%? It is an option that says whether we should start the second batchnorm layer (bn2) of our resnet model at 0 or at 1. . dls = ImageDataLoaders.from_folder(data_path, valid=&#39;val&#39;, item_tfms=Resize(256)) learn_timm = vision_learner(dls, &#39;resnet18&#39;, pretrained=False, metrics=accuracy) . learn_timm.model[0].model.layer1 . Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act1): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act2): ReLU(inplace=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act1): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act2): ReLU(inplace=True) ) ) . learn_timm.model[0].model.layer1[0].bn2.weight . Parameter containing: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True) . dls = ImageDataLoaders.from_folder(data_path, valid=&#39;val&#39;, item_tfms=Resize(256)) learn_torchvision = vision_learner(dls, resnet18, pretrained=False, metrics=accuracy) . learn_torchvision.model[0][4] . Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) . learn_torchvision.model[0][4][0].bn2.weight . Parameter containing: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True) . The other thing that may catch your eye here is that timm has a second relu, but this is actually just a difference in torchvision using the same relu twice in the forward function so it doesn&#39;t quite show the full picture. . Conclusion . After changing these two defaults, I am now able to see similar performance where TorchVision performs at a lower accuracy level and timm performs at a higher accuracy level. So clearly, setting this option to False is best right? Not so fast. Ross says that while this option will perform better on short epoch runs when this option is set to False, it is not the case on a longer training run and actually will outperform the non-zero out version. . Next Steps . The next thing to do is to test the claim that the zero&#39;d version performs better and also to try other initializations as well. This is also not an issue if using pretrained weights since the bn2 weights will be specified by the pretrained weights already so this is only something that will occur if new performance metrics are being compared as was the case when this question arose. .",
            "url": "https://blog.problemsolversguild.com/fastai/technical/exploration/2022/05/02/zero_init_last_resnet18_performance.html",
            "relUrl": "/fastai/technical/exploration/2022/05/02/zero_init_last_resnet18_performance.html",
            "date": " • May 2, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "PyTorch torch.lerp Exploration",
            "content": "Introduction . torch.lerp stands for linear interpolation is a handy function that combines two tensors using a provided weight. Let&#39;s explore how it can be used! . import torch . Let&#39;s start off with something easy. We have two items and we want to combine them by taking 75% of item1 and 25% of item2. Mathematically, this could be represented as $output = 0.75*item1+0.25*item2$. A more general form of this can be represented as $output = pct*item1+(1-pct)*item2$. This is a very common piece of code in machine learning papers. That&#39;s why pytorch has the handy torch.lerp function! . item1 = torch.tensor(2.) item2 = torch.tensor(6.) weight = 1/4 # This means that we will use 3/4 of item1 and 1/4 of item2 . output1 = (1-weight)*item1+(weight)*item2 . output2 = torch.lerp(item1, item2, weight) . output1 . tensor(3.) . output2 . tensor(3.) . Here is an example in the mixup paper of lerp being used in practice: . import matplotlib.pyplot as plt . np_april = plt.imread(&#39;notebook_images/pets/april.jpg&#39;) april = torch.from_numpy(np_april) . april_smaller = april[600:600+1224,1100:1100+1124,:]/255. . x_i = april_smaller #simulated image #1 x_j = torch.rand_like(x_i) #simulated image #2 lam=0.1 # Let&#39;s set lam to 0.5 which will blend equal parts of xi and xj. . Now, let&#39;s blend these two &#39;images&#39; . fig, axs = plt.subplots(ncols=5, sharey=True, figsize=(18,3)) for i,lam in enumerate([0, 0.25, 0.5, 0.75, 1]): x_hat = torch.lerp(x_j,x_i,lam) axs[i].set_title(f&#39;{i}:λ={lam}&#39;) axs[i].imshow(x_hat) . from fastcore.all import test_close, test_eq . x_hat = torch.lerp(x_j,x_i,weight=0.5) . test_close((x_j + x_i)/2, x_hat, eps=1e-6) . As we expected, these two are equal (within a small amount of error due to float math) . Linear interpolation is also often used in exponentially weighted decay which allows us to not entirely discard previous weight results while only keeping track of the most recent value. . Here is what exponential weighted decay looks like in the Adam Optimizer formula: ) . This algorithm actually contains two linear interpolations: $m_t = beta_1*m_{t-1}+(1- beta_1)*g_t$ $v_t = beta_2*v_{t-1}+(1- beta_2)*g_t^2$ . and here is what they look like in code: . m_tm1=torch.tensor(0.) v_tm1=torch.tensor(0.) g_t = torch.tensor(0.5) beta1=torch.tensor(0.99) beta2=torch.tensor(0.999) . m_t = torch.lerp(m_tm1, g_t, beta1) v_t = torch.lerp(v_tm1, g_t**2, beta2) . m_t . tensor(0.4950) . v_t . tensor(0.2498) . Hope this was helpful and gave a better understanding of what torch.lerp is and where it is used. If you have any suggestions or questions, please feel free to reach out and I would be happy to address them! .",
            "url": "https://blog.problemsolversguild.com/python/technical/exploration/2022/02/22/Exploring-Torch.lerp.html",
            "relUrl": "/python/technical/exploration/2022/02/22/Exploring-Torch.lerp.html",
            "date": " • Feb 22, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "A cron job that will run conda, run python script, and email when it fails",
            "content": "I have searched high and low trying to create a scheduled task (cron job) that will: . Use a conda environment | Run a python script | Email if/when errors occur | Here is my final script: . #!/bin/bash source /opt/conda/etc/profile.d/conda.sh conda activate my_environment EMAIL=0 echo -e &#39; n&gt;&gt;&gt;&gt;&gt; Backup &gt;&gt;&gt;&gt;&gt;&#39; &gt; daily.log python backup.py &amp;&gt;&gt; daily.log || EMAIL=1 echo -e &#39; n&gt;&gt;&gt;&gt;&gt; Cleanup &gt;&gt;&gt;&gt;&gt;&#39; &gt;&gt; daily.log python cleanup.py &amp;&gt;&gt; daily.log || EMAIL=1 if [[ $EMAIL == 1 ]]; then echo &quot;Daily Task Failure ($0) Time: $(date) Hostname: $(hostname -f) Some of the daily tasks failed. Details: $(cat daily.log) Have a nice day!&quot; | mailx -s &quot;Daily Task Failure&quot; dev@example.com admin@example.com exit; fi . In the end, I realized it is much easier to write a bash script that handles emailing rather than configuring the cron to do so with MAILTO, output redirects, etc. . In the next post, we will show you how to automate the creation of this cron job and other required setup using Ansible. . . . My web searches while creating the above script . Cron basics and MAILTO . https://www.digitalocean.com/community/tutorials/how-to-use-cron-to-automate-tasks-ubuntu-1804 . How to make cron send email only when script throws errors? . 20 6-10 * * 1-5 ~/job_failure_test.sh &gt; ~/job_fail.log 2&gt;&amp;1 || mail -s &quot;Errors&quot; myemail@something.com &lt; ~/job_fail.log . https://unix.stackexchange.com/a/314647 . Why can’t my cron job send emails? . Before you uninstall Postfix and install Sendmail (not because you prefer Sendmail, but because you are not receiving emails) check your spam/junk folder!. That would have saved me a couple hours of struggle. . https://tecadmin.net/install-sendmail-on-ubuntu/ . How to catch an exception thrown by a python script in shell script (so I can do something like sending an email) . ./script.py || { # Python script script.py failed. Do something } . https://stackoverflow.com/a/24208293/6999874 . How can I use conda in bash script? I get an error: CommandNotFoundError: Your shell has not been properly configured to use conda activate. To initialize your shell, run $ conda init &lt;SHELL_NAME&gt; . Conda puts something like this to your .bashrc or .zshrc when you install it. Just put this simplified version in the script before conda activate: . source /opt/conda/etc/profile.d/conda.sh . https://unix.stackexchange.com/a/577347 . https://askubuntu.com/a/1218657 .",
            "url": "https://blog.problemsolversguild.com/technical/2022/02/17/Cron_job_failure_email.html",
            "relUrl": "/technical/2022/02/17/Cron_job_failure_email.html",
            "date": " • Feb 17, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Create SSH key pair and using it",
            "content": "1. Create SSH key pair . It is fairly simple to create a SSH key pair as ssh-keygen will guide you through the whole process. . ssh-keygen -t ed25519 -C &quot;sunny@example.com&quot; . -C is a comment that will be added at the end of the public key. . The series of questions after you run the above code looks something like: . Generating public/private ed25519 key pair. Enter file in which to save the key (/home/sunny/.ssh/id_ed25519): Enter passphrase (empty for no passphrase): Enter same passphrase again: . You can choose to use all default values or enter your own. . Once it is complete: . The public key is now located in /home/sunny/.ssh/id_ed25519.pub | The private key is now located in /home/sunny/.ssh/id_ed25519 | . 2. Copy public key to the remote server: . Let’s say, the remote server is running SSH on a non-standard port. If your remote server is using the standard port, you can omit the port settings: . ssh-copy-id -i ~/.ssh/id_ed25519 -p 123 sunny@remote_server_address . .pub is automatically added if you specify a filename that does not end with .pub. . This will add your public key to the remote server’s ~/.ssh/authorized_keys file. If you open it up, you will see the comment you added when generating via -C flag in step 1. . 3. Add private key to the SSH agent: . [Optional] Start the ssh-agent if it isn’t already. If you are not sure, skip it for now and come back if the next step fails. . eval &quot;$(ssh-agent -s)&quot; . Add your SSH private key to the ssh-agent. . ssh-add ~/.ssh/id_ed25519 . If you are curious which keys ssh-agent knows about, run: . ssh-add -L . 4. Add the remote host to SSH config (just for convenience): . For simplicity, you can add the remote host to your SSH config file (~/.ssh/config). If this file does not exist, you can create it manually (touch ~/.ssh/config). . Add a block like this: . Host remote_server_name HostName 123.45.678.90 User sunny IdentityFile ~/.ssh/id_ed25519 Port 123 . Now you can SSH to the server by: . ssh remote_server_name . Instead of: . ssh -i ~/.ssh/id_ed25519 -p 123 sunny@123.45.678.90 . . References . https://www.ssh.com/academy/ssh/copy-id | https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent | https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys-2 | https://linuxize.com/post/using-the-ssh-config-file/ | .",
            "url": "https://blog.problemsolversguild.com/technical/2022/02/15/SSH-key.html",
            "relUrl": "/technical/2022/02/15/SSH-key.html",
            "date": " • Feb 15, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Python Function Default Argument Value",
            "content": "Introduction . This post is an exploration into when a python function gets the default argument from a function signature. Here is the scenario that got me to this point: . I had a chunk of code that had a filename defined. . from datetime import datetime from time import sleep def save_file_w_timestamp(filename=f&#39;{datetime.now().isoformat()}/file.csv&#39;): print(filename) . save_file_w_timestamp() sleep(5) save_file_w_timestamp() . 2022-02-02T21:00:58.160733/file.csv 2022-02-02T21:00:58.160733/file.csv . My initial thought was that these two function calls would return the time that the function was called, but it doesn&#39;t. That is what we will explore in this blog post. . Let&#39;s explore how to make this work as expected. . If you&#39;re in a hurry: . def save_file_w_None(filename=None): if filename is None: filename=f&#39;{datetime.now().isoformat()}/file.csv&#39; print(filename) . Failed Attempt #1: passing through a function . def get_current_datetime(): return datetime.now().isoformat() def save_file_w_timestamp(filename=f&#39;{get_current_datetime()}/file.csv&#39;): print(filename) . save_file_w_timestamp() sleep(5) save_file_w_timestamp() . 2022-02-02T21:01:03.198774/file.csv 2022-02-02T21:01:03.198774/file.csv . Attempt #2: Brute force . def save_file_w_timestamp(filename=None): if filename is None: filename=f&#39;{datetime.now().isoformat()}/file.csv&#39; print(filename) . save_file_w_timestamp() sleep(5) save_file_w_timestamp() . 2022-02-02T21:01:17.708874/file.csv 2022-02-02T21:01:22.712349/file.csv . Exciting Attempt #3: Mutable Madness . This third example surprised me a lot (thank you to miwojc on the fastai discord for bringing it to my attention!). If you have an empty list as your default value, it seems innocent enough. Naive Kevin from yesterday would have assumed that this code would create an empty list if x was not passed. Naive Kevin would be sadly mistaken. This is a really good example of what is actually happening above. This creates a variable x that starts as an empty list, but let&#39;s see what happens when we call the function. . def mutable_madness(x=[]): x.append(1) print(x) . mutable_madness() . [1] . What do you think the value is going to be here? . mutable_madness() . [1, 1] . . How about if we pass an empty list in? . mutable_madness([]) . [1] . . And what about now? . mutable_madness() . [1, 1, 1] . . I got all of these wrong when I was initially coding this so if it doesn&#39;t seem intuitive to you, just know you aren&#39;t alone. This is a fairly common gotcha that can lead to frustrating bugs. Here is another good blog post for further reading: https://docs.python-guide.org/writing/gotchas/. . Just to explore a few more ideas from this concept, I am going to add a few more examples below. . j=1 def immutable_nonmadness(y=j): #global j #This could be added to allow j to be used inside and outside the function. y+=1 print(y) . My initial thought with this example was that j would keep incrementing because we are setting j inside of our function. This is actually a good lesson about context which I won&#39;t get into a ton except to mention that the j in line 1 and line 2 are the same j and the j in line 3 is a different j which is only accessible inside of the function. If we wanted this to behave similarly to the functions above which kept using the default value from above, the global argument would need to be added but this really is using a different concept to keep incrementing the value once we introduce global. . immutable_nonmadness() . 2 . immutable_nonmadness() . 2 . . In this example, because the value 1 is an immutable object, it doesn&#39;t hold onto the previous value but if instead, we had put an empty list in j, it would act the same way as the examples from above. This is because a variable is neither mutable nor immutable. A variable is give its type and therefor its mutability based on the object it is storing. . j=[] def function_1(y=j): y+=[1] print(y) . function_1() . [1] . function_1() . [1, 1] . This behavior will happen with lists, dicts, sets, and most custom classes. . def function_dict(value, x={}): x[value] = len(x) print(x) . function_dict(&#39;thing 0&#39;) . {&#39;thing 0&#39;: 0} . function_dict(&#39;thing 1&#39;) . {&#39;thing 0&#39;: 0, &#39;thing 1&#39;: 1} . def function_set(value, x=set()): x.add(value) print(x) . function_set(&#39;thing 0&#39;) . {&#39;thing 0&#39;} . function_set(&#39;thing 1&#39;) . {&#39;thing 0&#39;, &#39;thing 1&#39;} . I hope you learned something going through this blog post and if you take one thing away from this it is to be mindful when setting your default arguments. .",
            "url": "https://blog.problemsolversguild.com/python/technical/exploration/2022/02/07/Python-Function-Default-Argument-Value.html",
            "relUrl": "/python/technical/exploration/2022/02/07/Python-Function-Default-Argument-Value.html",
            "date": " • Feb 7, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Using Makefile to Create Git Hook",
            "content": "In our previous post, we created a Git Hook that will strip all the superfluous metadata automatically when you commit notebooks. We wanted to make the installation of this hook easier by creating a Makefile target. . Here is what we came up with: . SHELL := /bin/bash .PHONY: hook hook: @echo -e &#39;#!/bin/sh nfor file in $$(git diff --diff-filter=d --cached --name-only | grep -E &#39;&quot;&#39;&quot;&#39; .ipynb$$&#39;&quot;&#39;&quot;&#39;) ndo n tjupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace &quot;$$file&quot; n tgit add &quot;$$file&quot; ndone n&#39; &gt; .git/hooks/pre-commit @chmod +x .git/hooks/pre-commit . [Download] . Run make hook and Voila! You have your Git Hook that will reduce merge conflicts headaches. . . My web searches while creating this Makefile . How to convert multiline file into a string with newline n characters: . awk &#39;$1=$1&#39; ORS=&#39; n&#39; filename . This is how I converted an existing hook file (pre-commit) into a single string. I had to add tabs ( t) manually, but this awk got me close enough. . awk &#39;$1=$1&#39; ORS=&#39; n&#39; pre-commit . will return: . #!/bin/sh nfor file in $(git diff --diff-filter=d --cached --name-only | grep -E &#39; .ipynb$&#39;) ndo njupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace &quot;$file&quot; ngit add &quot;$file&quot; ndone n . . How to write a multiline file with echo: . echo -e &quot;Line 1 r nLine2&quot; &gt;&gt; readme.txt . . How to escape single quotes in single quoted strings: . &#39;&quot;&#39;&quot;&#39; is interpreted as &#39; . . How to escape dollar signs $ in Makefile: . Escape $ by adding another $ (i.e. $$) . . References . https://stackoverflow.com/a/26451573 | https://unix.stackexchange.com/a/219270 | https://stackoverflow.com/a/1250279 | https://til.hashrocket.com/posts/k3kjqxtppx-escape-dollar-sign-on-makefiles | .",
            "url": "https://blog.problemsolversguild.com/technical/2022/02/06/Makefile_to_create_githook.html",
            "relUrl": "/technical/2022/02/06/Makefile_to_create_githook.html",
            "date": " • Feb 6, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Using Git Hook to Clean Jupyter Notebook on Commit",
            "content": "To avoid merge issues and make PR review easier, we wanted a Git Hook which will: . Clear output of Jupyter Notebook | Only clean the files that were modified for this PR | . There are many hook samples in .git/hooks folder. You want to create a file called pre-commit with the following code: . #!/bin/sh for file in $(git diff --diff-filter=d --cached --name-only | grep -E &#39;customers/.+ .ipynb$&#39;) do jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace &quot;$file&quot; git add &quot;$file&quot; done . [Download] . Run chmod +x pre-commit to make the file executable. . That’s it! . In the next post, we will show you how to create this Git Hook with Makefile. . . . If the above code gives you the following error: . [NbConvertApp] WARNING | Config option `template_path` not recognized by `NotebookExporter`. . You need to run pip install -U nbconvert to get a newer version (https://github.com/jupyter/nbconvert/issues/1229#issuecomment-608721332). .",
            "url": "https://blog.problemsolversguild.com/technical/2022/02/05/GitHook_to_clean_notebook.html",
            "relUrl": "/technical/2022/02/05/GitHook_to_clean_notebook.html",
            "date": " • Feb 5, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Texas Rate Inspector Overview",
            "content": "What&#39;s the Problem? . Currently Texas&#39; rate selection process is heavily driven by the lowest cost for a typical user. However, this limits the imagination of Retail Electricity Providers (REP) and ends up hurting residential customers by creating a single variable rate plan that every REP optimizes for. We at the Problem Solvers Guild have a different way of measuring plans that utilizes a customers&#39; true historical usage. Now the question is, which of the 159 plans currently available in almost any zip code in Texas is right for an individual? . . What&#39;s the Solution? . The Problem Solvers Guild has developed a method to take any electric rate plan and apply an individual&#39;s usage to calculate a more accurate price-per-kilowatt than what is typically provided by REPs. With this solution, we help residential customers identify plans they would have previously overlooked. . We grab usage by working with Smart Meter Texas. A site that was created to allow residential customers to access their 15-minute usage data. This site also allows customers to share this access with companies with just a few pieces of information. . All of the rate plans that we currently use come from Power to Choose which is a site where REPs are able to advertise their available rates to residential customers. Each plan has an electricity facts label (EFL) that describes the plan in detail. We use these EFLs to model different rate plans that are offered. . Once we have all of this information, we combine the data and find the rate that would have given an individual the lowest price-per-kilowatt over the past year. If this is something you are interested in knowing for yourself, fill out our form and we will run an analysis for you. If you are a company that would like to use our library to create better rate plans or to drive your rate engine, feel free to contact us and we can find a solution based on your needs as well. . How does it work? . The Texas Rate Inspector was developed by The Problem Solvers Guild as a tool that allows an individual to quickly simulate any rate plan with their historical usage. Let me take you on a quick tour of how this tool works. . Here are three rate plans to choose from. . Plan 1 (Flat Rate) charges $0.119 per kWh plus a $9.95 monthly charge if your usage for that month is less than 1000 kWh. | Plan 2 (Free Weekends) charges $0.200 per kWh between 12:00 AM Monday to 7:00 PM Friday plus a $4.95 monthly charge. | Plan 3 (Tiered Rate) charges $0.089 per kWh between 0 and 1200 kWh of usage and $0.145 per kWh past that. This plan also has a $9.95 monthly charge and a $30 credit if you use more than 800 kWh. | Now that I&#39;ve given you all the information you need, which of these plans is the best option for a residential customer in Texas? . You can close your Excel sheet. I will walk you through each of these scenarios and at the end, I will tell you which of these is the best plan for the user. . Grabbing Historical Usage . First, we grab a user&#39;s usage from Smart Meter Texas. note: If you are interested in providing your usage information to us, fill out our form and we will request access to your usage. . from rate_inspector.usage import * start_date = pd.to_datetime(datetime.date(2021, 7, 1)) end_date = pd.to_datetime(datetime.date(2021, 7,31)) data = pd.read_csv(&#39;../assets/example_usage.csv&#39;, parse_dates=[&#39;USAGE_DATE&#39;]) data = get_data(data, start_date, end_date) july_usage_data = UsageComponent.from_historical_df(data) . Here is a single day of historical usage (96 15-minute intervals). . july_usage_data.plot_usage(0,96) . Now that we have historical usage, let&#39;s model each of the three plans. . Plan 1: Flat Rate . Plan 1 (Flat Rate) charges $0.119 per kWh plus a $9.95 monthly charge if your usage for that month is less than 1000 kWh . from rate_inspector.rate import * from rate_inspector.plan import * . First, we identify the rate components. This first plan has a FlatThreshold component that has a flat charge up to a threshold. . base_w_peak = FlatThreshold(&#39;Flat Charge&#39;, 9.95, max_thresh=1000) kwh_charge = TimeOfUseRate(&#39;Energy Charge&#39;, 0.119) . Once all of the rate components have been defined, we create a rate plan that contains all of the rate components. . plan1 = RatePlan(&#39;Flat Rate&#39;, [base_w_peak, kwh_charge]) . Next, we create a statement which combines the plan and the usage. . statement1 = Statement(plan1, july_usage_data) . Since total_usage was under 1000 kWh in this case, the $9.95 base charge is applied. . statement1.print_line_item() . Flat Charge: 9.9500 Energy Charge: 96.2753 . statement1.total_usage . 809.0359999999996 . So our total cost per kwh is $0.131/kWh for Plan 1 . statement1.price_per_kwh . 0.1312985874546989 . Plan 2: Free Weekends . Plan 2 (Free Weekends) charges $0.200 per kWh between 12:00 AM Monday to 7:00 PM Friday plus a $4.95 monthly charge . energy_kwh_rest_of_week = TimeOfUseRate(&#39;RoW Energy Charge&#39;, 0.20, days_of_week=[calendar.MONDAY, calendar.TUESDAY, calendar.WEDNESDAY, calendar.THURSDAY]) energy_kwh_friday = TimeOfUseRate(&#39;Friday Energy Charge&#39;, 0.20, days_of_week=[calendar.FRIDAY], end_time=datetime.time(7)) base = Flat(&#39;Base Charge&#39;, 4.95) . plan2 = RatePlan(&#39;Free Weekends&#39;, [energy_kwh_rest_of_week, energy_kwh_friday, base]) . statement2 = Statement(plan2, july_usage_data) . statement2.print_line_item() . RoW Energy Charge: 82.3296 Friday Energy Charge: 1.7952 Base Charge: 4.9500 . So our total cost per kwh is $0.11/kWh for Plan 2 even though the advertised price on PowerToChoose shows the minimum price per kWh to be $0.132 (@ 2000 kWh) . statement2.price_per_kwh . 0.11009992138792346 . Plan 3: Tiered Rate . Plan 3 (Tiered Rate) charges $0.089 per kWh between 0 and 1200 kWh of usage and $0.145 per kWh past that. This plan also has a $9.95 monthly charge and a $30 credit if you use more than 800 kWh. . base = Flat(&#39;Base Charge&#39;, 9.95) tier1 = Tier(&#39;0-1200 kWh&#39;, 0.089, max_kwh=1200) tier2 = Tier(&#39;&gt; 1200 kWh&#39;, 0.145, min_kwh=1200) credit = FlatThreshold(&#39;800 kwh credit&#39;, -30, min_thresh=800) . plan3 = RatePlan(&#39;Tiered Rate&#39;, [base, tier1, tier2, credit]) . statement3 = Statement(plan3, july_usage_data) . statement3.print_line_item() . Base Charge: 9.9500 0-1200 kWh: 72.0042 &gt; 1200 kWh: 0.0000 800 kwh credit: -30.0000 . Plan 3 ends up being a great option for the usage. Our user would have paid just $0.064/kWh for plan number three. One thing to keep in mind is that we are only simulating a single month at this point. . statement3.price_per_kwh . 0.06421741924957602 . Conclusion . This blog post shows an example of how the Texas Rate Inspector works. If you are interested in getting your electric rates modeled or would like to hear more about what we do, please reach out. We would love to take your usage and help you find your next rate plan. .",
            "url": "https://blog.problemsolversguild.com/texas/rate/2021/09/19/Texas-Rate-Inspector.html",
            "relUrl": "/texas/rate/2021/09/19/Texas-Rate-Inspector.html",
            "date": " • Sep 19, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Normalizing fastai Tabular Inputs",
            "content": "Normalizing data is an important aspect of data science that seemed like magic to me for the longest time. I could train a model for 10 epochs on un-normalized data and make little or no progress towards an effective model, then restart and run the same data through my model with normalization applied and have a perfect model. Why was this the case? What was happening that caused such a big difference? Let&#39;s explore that question. . import numpy as np import matplotlib.pyplot as plt . What is Normalization? . Normalization is the process of converting all of the numbers so the full set of numbers has a mean of 0 and a standard deviation of 1. My guess is that for most people reading this, those numbers and terms don&#39;t mean a lot. So let&#39;s dig in a bit deeper. Normalization is the process of putting all of our values on a common scale. By ensuring that the overall number range has a mean of 0, each side of zero will have half of the weight of the numbers. That means that all of the values are balanced around 0. Let&#39;s get a real example. . Let&#39;s assume we have a set of numbers: [1, 5, 10, 25, 100, 1000]. . example_1 = np.array([1, 5, 10, 25, 100, 1000]); example_1 . array([ 1, 5, 10, 25, 100, 1000]) . How would we transform these numbers to have a mean of 0? Let&#39;s start by finding the current mean of these numbers. . example_1.mean() . 190.16666666666666 . 190.16666 is the mean of this set of numbers. In order to make this 0, we should be able to just subtract each number by that amount. . example_1_mean_0 = example_1-example_1.mean(); example_1_mean_0 . array([-189.16666667, -185.16666667, -180.16666667, -165.16666667, -90.16666667, 809.83333333]) . f&quot;The mean is now: {example_1_mean_0.mean()} (AKA: 0)&quot; . &#39;The mean is now: 1.8947806286936004e-14 (AKA: 0)&#39; . Instead of being distributed around the number 190.1666, the numbers are now distributed around the number 0. With the numbers now balanced correctly, we want to remove scale and units from our dataset. statista.com has the following example to explain standard deviation: . 1,000 people were questioned about their monthly phone bill. The mean value is $40 and the standard deviation 27. Which means that the average distance of all answers (=values) to the mean is $27. . Let&#39;s consider what it means then to divide by the standard deviation. For this example, we would divide by 27 so that the standard deviation is 1 and that is going put us into the same scale whether we started with a huge scale like monthly mortgage amount or monthly phone bill. It strips the original scaling away from the dataset and puts onto a scale where 1 is the distance from the center point. This is helpful because it means that a step in any direction will change the loss the same amount. Another way of putting this is that it smooths the loss landscape so all attributes have the same amount of pull. . example_1_mean_0_std_1 = example_1_mean_0/example_1_mean_0.std(); example_1_mean_0_std_1 . array([-0.52008301, -0.50908566, -0.49533897, -0.45409891, -0.2478986 , 2.22650516]) . example_1_mean_0_std_1.mean(), example_1_mean_0_std_1.std() . (7.401486830834377e-17, 1.0) . Notice that in all three graphs, the distribution looks the same. Lots of numbers on the left-hand side and only the one number on the right. But to our neural network, there is a huge difference between taking numbers ranging from -0.52 and 2.23 and taking numbers ranging from 1 to 1000. The weights can make a lot finer adjustments on the normalized version and each incoming data point will be on the same scale. That&#39;s why when an image model has been pretrained on Imagenet, you have to keep the same standard deviation and mean. By using those numbers, you are taking all of the pixels in your image and converting them to the normal range for Imagenet. . Testing the Theory with fastai&#39;s Tabular Application . Now that we have an idea of what is happening, let&#39;s see if we can prove that it actually works how we anticipate. To do this, I will be creating a simple tabular learner and a sample dataloader. I will first train the model on an normalized input and then train the same model using the un-normalized input. My assumption is that the normalized inputs will be much better at converting the x value into y than the un-normalized version. . Experiment Setup . x1 + xN = y . from fastai.tabular.all import * x = torch.rand((200000,10)) scale_amt = torch.randint(0,100,size=(10,)) #x = x*scale_amt df = pd.DataFrame(x, columns=[f&quot;x{n}&quot; for n in range(1,11)]) #df[&#39;x1&#39;]=df[&#39;x1&#39;]*1000 #df.columns #df[&#39;y&#39;] = df[&#39;x1&#39;] + df[&#39;x2&#39;] + df[&#39;x3&#39;] + df[&#39;x4&#39;] + df[&#39;x5&#39;] + df[&#39;x6&#39;] + df[&#39;x7&#39;] + df[&#39;x8&#39;] + df[&#39;x9&#39;] + df[&#39;x10&#39;] df[&#39;y&#39;] = (df.values*scale_amt.tolist()).sum(axis=1) splits = RandomSplitter()(df) df.head() . x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 y . 0 0.075054 | 0.456406 | 0.745085 | 0.930838 | 0.109559 | 0.498177 | 0.565746 | 0.743462 | 0.893542 | 0.796160 | 343.622546 | . 1 0.823718 | 0.925904 | 0.418757 | 0.579081 | 0.645427 | 0.502683 | 0.689346 | 0.439689 | 0.281477 | 0.984495 | 380.632963 | . 2 0.212132 | 0.928440 | 0.068775 | 0.974991 | 0.274955 | 0.107026 | 0.274451 | 0.439982 | 0.731169 | 0.026224 | 295.966817 | . 3 0.874167 | 0.900319 | 0.221668 | 0.755159 | 0.525875 | 0.861770 | 0.347837 | 0.993839 | 0.466206 | 0.554722 | 442.779244 | . 4 0.836071 | 0.795962 | 0.009992 | 0.531759 | 0.619634 | 0.196202 | 0.648332 | 0.899659 | 0.893503 | 0.094362 | 408.136464 | . monitor_parameters is a hook that can be used to monitor values inside of a model. It is a little bit of a hack, but it works really well for getting a better idea what is happening inside the model. . def monitor_parameters(m, i, o): m.weight_track.append(list(m.parameters())[0].tolist()[0]) m.bias_track.append(list(m.parameters())[1].tolist()) . Normalized . to_normalized = TabularPandas(df, cont_names=[&#39;x1&#39;, &#39;x2&#39;, &#39;x3&#39;, &#39;x4&#39;, &#39;x5&#39;, &#39;x6&#39;, &#39;x7&#39;, &#39;x8&#39;, &#39;x9&#39;, &#39;x10&#39;], y_names=&#39;y&#39;, procs=[Normalize], splits=splits) dls_normalized = to_normalized.dataloaders(verbose=True, shuffle=False) learn_normalized = tabular_learner(dls_normalized, layers=[], config=tabular_config(use_bn=False, bn_cont=False)) learn_normalized.lr_find(start_lr=1e-3, end_lr=1000000) . Setting up after_item: Pipeline: Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: ReadTabBatch . SuggestedLRs(valley=3.981071710586548) . learn_normalized.model.layers[0][0].bias_track = [] learn_normalized.model.layers[0][0].weight_track = [] learn_normalized.model.layers[0][0].register_full_backward_hook(monitor_parameters) . &lt;torch.utils.hooks.RemovableHandle at 0x7f44a05ccf70&gt; . learn_normalized.fit_one_cycle(10, 10) . epoch train_loss valid_loss time . 0 | 1.142768 | 0.722641 | 00:08 | . 1 | 6.045738 | 0.614625 | 00:08 | . 2 | 16.018417 | 7.314993 | 00:08 | . 3 | 8.353395 | 5.835221 | 00:08 | . 4 | 3.468160 | 0.493669 | 00:08 | . 5 | 2.660133 | 1.251976 | 00:08 | . 6 | 0.722458 | 0.373421 | 00:08 | . 7 | 0.367568 | 0.272525 | 00:08 | . 8 | 0.025171 | 0.010871 | 00:08 | . 9 | 0.000455 | 0.000518 | 00:08 | . learn_normalized.recorder.plot_loss(skip_start=1000) . learn_normalized.show_results(ds_idx=0, shuffle=False) . x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 y y_pred . 0 1.562167 | -1.569042 | -1.114254 | 0.678972 | -0.447562 | -0.721848 | 0.950705 | -0.506051 | 1.427110 | 1.305289 | 318.861755 | 318.871887 | . 1 -0.558561 | 1.409515 | -1.694918 | 0.051202 | -1.187221 | 0.865891 | 1.003036 | -0.675074 | 1.489744 | -1.504115 | 319.647339 | 319.605621 | . 2 0.580085 | 1.621262 | 1.555035 | -1.474297 | 0.485046 | 1.653549 | -1.121346 | 0.184075 | 1.324292 | -0.353307 | 442.348450 | 442.299805 | . 3 1.634533 | 1.501764 | 0.694681 | -0.052227 | 0.998929 | -0.802163 | 0.824090 | 1.452214 | 0.217072 | -0.867735 | 456.612335 | 456.587646 | . 4 0.305931 | 0.446660 | 0.606511 | 0.404355 | -0.881533 | -0.470554 | 1.398422 | 0.411920 | 0.697609 | 1.205599 | 364.134827 | 364.130890 | . 5 -0.953506 | -1.129230 | 1.685477 | 0.200103 | 1.446620 | 1.669933 | 0.654975 | -0.218070 | 0.763460 | 1.622134 | 378.366058 | 378.378632 | . 6 -1.729204 | 1.402740 | 1.129965 | -1.550283 | -0.174763 | -1.702276 | 0.349754 | -1.626720 | -0.736905 | 0.554198 | 219.651688 | 219.670380 | . 7 1.051801 | 0.651880 | -0.606515 | -0.211603 | 1.065033 | 1.140438 | 1.395582 | -1.302371 | 0.548107 | 0.569542 | 379.511841 | 379.522766 | . 8 0.845884 | -0.547829 | -1.420988 | 0.976506 | -1.266368 | -0.449748 | -0.053614 | -0.578055 | 0.252759 | -0.031057 | 266.948334 | 266.938416 | . plt.plot(learn_normalized.model.layers[0][0].bias_track) plt.show() . plt.plot(learn_normalized.model.layers[0][0].weight_track) plt.show() . Not Normalized . to_not_normalized = TabularPandas(df, cont_names=[&#39;x1&#39;, &#39;x2&#39;, &#39;x3&#39;, &#39;x4&#39;, &#39;x5&#39;, &#39;x6&#39;, &#39;x7&#39;, &#39;x8&#39;, &#39;x9&#39;, &#39;x10&#39;], y_names=[&#39;y&#39;], splits=splits) dls_not_normalized = to_not_normalized.dataloaders(verbose=True, shuffle=False) dls_not_normalized.one_batch() learn_not_normalized = tabular_learner(dls_not_normalized, layers=[], config=tabular_config(use_bn=False, bn_cont=False), train_bn=False) learn_not_normalized.lr_find(start_lr=1e-3, end_lr=1000000) . Setting up after_item: Pipeline: Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: ReadTabBatch . SuggestedLRs(valley=3.2359366416931152) . learn_not_normalized.model.layers[0][0].bias_track = [] learn_not_normalized.model.layers[0][0].weight_track = [] learn_not_normalized.model.layers[0][0].register_full_backward_hook(monitor_parameters) . &lt;torch.utils.hooks.RemovableHandle at 0x7f4473678e80&gt; . learn_not_normalized.fit_one_cycle(10, 10) . epoch train_loss valid_loss time . 0 | 154.066040 | 153.006943 | 00:08 | . 1 | 453.208710 | 498.198853 | 00:08 | . 2 | 450.821014 | 405.338318 | 00:08 | . 3 | 462.034576 | 507.816345 | 00:08 | . 4 | 402.913025 | 391.320465 | 00:08 | . 5 | 299.577545 | 273.074860 | 00:08 | . 6 | 187.224609 | 182.667740 | 00:08 | . 7 | 95.384758 | 89.960159 | 00:08 | . 8 | 57.199078 | 54.958164 | 00:08 | . 9 | 27.854624 | 28.252810 | 00:08 | . learn_not_normalized.recorder.plot_loss(skip_start=500) . learn_not_normalized.show_results(ds_idx=0, shuffle=False) . x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 y y_pred . 0 0.950224 | 0.048134 | 0.178727 | 0.696215 | 0.370312 | 0.293320 | 0.776244 | 0.353630 | 0.912490 | 0.877288 | 318.861755 | 318.913208 | . 1 0.339225 | 0.906561 | 0.010991 | 0.514821 | 0.156830 | 0.751503 | 0.791369 | 0.304868 | 0.930580 | 0.066351 | 319.647339 | 318.007935 | . 2 0.667278 | 0.967587 | 0.949804 | 0.074028 | 0.639484 | 0.978802 | 0.177347 | 0.552729 | 0.882794 | 0.398533 | 442.348450 | 431.576630 | . 3 0.971073 | 0.933147 | 0.701273 | 0.484935 | 0.787802 | 0.270143 | 0.739647 | 0.918583 | 0.563008 | 0.250043 | 456.612335 | 444.280334 | . 4 0.588292 | 0.629064 | 0.675804 | 0.616864 | 0.245058 | 0.365837 | 0.905650 | 0.618462 | 0.701796 | 0.848512 | 364.134827 | 360.096039 | . 5 0.225438 | 0.174889 | 0.987485 | 0.557846 | 0.917016 | 0.983530 | 0.690767 | 0.436712 | 0.720815 | 0.968745 | 378.366058 | 374.773651 | . 6 0.001953 | 0.904608 | 0.827014 | 0.052072 | 0.449048 | 0.010392 | 0.602547 | 0.030322 | 0.287482 | 0.660485 | 219.651688 | 226.905151 | . 7 0.803183 | 0.688209 | 0.325397 | 0.438884 | 0.806881 | 0.830731 | 0.904829 | 0.123895 | 0.658617 | 0.664914 | 379.511841 | 374.775940 | . 8 0.743857 | 0.342450 | 0.090121 | 0.782187 | 0.133986 | 0.371841 | 0.485960 | 0.332858 | 0.573315 | 0.491550 | 266.948334 | 270.995636 | . plt.plot(learn_not_normalized.model.layers[0][0].bias_track) plt.show() . plt.plot(learn_not_normalized.model.layers[0][0].weight_track) plt.show() . scale_amt . tensor([79, 93, 48, 48, 84, 53, 18, 92, 93, 22]) . list(learn_not_normalized.model.layers[0][0].parameters()) . [Parameter containing: tensor([[73.1407, 84.1651, 45.1576, 45.1585, 77.2888, 49.8361, 16.7273, 83.5148, 84.1355, 20.6937]], device=&#39;cuda:0&#39;, requires_grad=True), Parameter containing: tensor([25.1835], device=&#39;cuda:0&#39;, requires_grad=True)] . list(learn_normalized.model.layers[0][0].parameters()) . [Parameter containing: tensor([[22.7578, 26.8047, 13.8633, 13.8707, 24.2422, 15.2929, 5.2018, 26.5390, 26.8526, 6.3496]], device=&#39;cuda:0&#39;, requires_grad=True), Parameter containing: tensor([315.1483], device=&#39;cuda:0&#39;, requires_grad=True)] . Results . The results ended up matching our theory but in a slightly different way than I had anticipated. You can see that the non-normalized weights line up a lot closer to the scale_amt and after thinking about the reason for that, I think I understand. When the input values are normalized, all of the numbers are put onto the same scale so the easiest way for the gradients to be adjusted is to just move the bias. But in the non-normalized version when adjusting the bias, all of the individual weights would be moving at different rates so it is a lot less likely that the bias will be moved as the best step for the model. Instead the weights are adjusted more during the training process. .",
            "url": "https://blog.problemsolversguild.com/technical/2021/08/16/Tabular_Normalization_Exploration.html",
            "relUrl": "/technical/2021/08/16/Tabular_Normalization_Exploration.html",
            "date": " • Aug 16, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "5 Lessons from Richard Feynman",
            "content": "I recently listened to a biography about Richard Feynman and I had some takeaways that surprised me. I had always vaguely known that Richard Feynman was an interesting guy and a great professor, but this book went so much deeper and taught me some valuable lessons. . Lesson 1: Don’t Stay in Your Lane . Whether you are a writer that has decided to explore a topic that isn’t your forte or an athlete that has taken up a social justice campaign, expanding your boundaries and getting out of your comfort zone will have surprising results. Even if it doesn’t become a passion, it will still give you a perspective you wouldn’t have otherwise had. Being confined to a single topic just because you are good at it can also lead to burnout and won’t let you reach your full potential. . Lesson 2: Don’t Trust and Always Verify . Just because other people agree with the results, doesn’t mean that the results are correct. Being able to reproduce results and understanding the assumptions that have been made by previous researchers can at a minimum help you obtain a more accurate mental model and might even bring up some questions about why certain assumptions were made. Sometimes this might have to be done in stages. Maybe when you are first looking at a new paper, trust everything except for their results and then as you are able to reproduce the results at that layer, go back a layer and re-consider the assumptions that were made. Was X really the best choice for this task? What other options would have been available as alternatives for that task. . Lesson 3: Ask Questions (Even the “Obvious” ones) . If you don’t understand a concept that is being discussed, ask somebody that has more experience to explain it in another way. A lot of the time, these questions will lead to interesting discussions, unexplored knowledge gaps, and sometimes entirely new projects to work on. One great way to learn is to listen to people talk about things that they have learned and ask them questions. . Lesson 4: “I Don’t Know” . Not knowing an answer to a question is not something that should discourage you. On the contrary, knowing that you don’t know the answer to something is a really positive step in the right direction. This also gives you a great chance to improve your knowledge and learn something that you don’t currently know. . Lesson 5: Define the Problem . Whether you are working by yourself or with a group of people, focus on defining the problem before trying to come up with a solution. Use real-world examples when discussing what the problem actually is. Having different viewpoints is great when brainstorming, but make sure everybody is at least focusing on the same problem and not talking past each other. . Conclusion . These five lessons are just the start of what I learned from this book which is also full of entertaining stories and facts about Richard Feynman’s life. It’s well worth the read (or listen). .",
            "url": "https://blog.problemsolversguild.com/advice/2021/08/03/5_Lessons_From_Richard_Feynman.html",
            "relUrl": "/advice/2021/08/03/5_Lessons_From_Richard_Feynman.html",
            "date": " • Aug 3, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Python Native __slots__ Attribute Exploration",
            "content": "Introduction . While reading the python data model documentation, I came across something I hadn&#39;t seen before. __slots__ is an optional argument that allows users to &quot;explicitly declare data members&quot;. It is an interesting concept that I haven&#39;t seen utilized, but perhaps the reason is that not many people are aware it exists. I am going to explore this attribute that is available to see if it might provide value for my future projects. According to this blog post, __slots__ can significantly reduce the amount of ram required to create objects (40-50%!). Now let&#39;s dive in and figure out how it&#39;s used! . Python Version Check: 3.8.8 (default, Apr 13 2021, 19:58:26) [GCC 7.3.0] . Example #1: Typical Case . The first example we look at is the working example. we will have a class A1 with slots set to accept one var named var1. . class A1: __slots__ = [&#39;var1&#39;] def __init__(self, value_passed_through_here): self.var1 = value_passed_through_here . a1 = A1(1); a1.var1 . 1 . a has been created and everything is going well. Let&#39;s try adding another attribute. . a1.var2 = &quot;but can I set another var?&quot; . AttributeError Traceback (most recent call last) &lt;ipython-input-6-468e53fb51e0&gt; in &lt;module&gt; -&gt; 1 a1.var2 = &#34;but can I set another var?&#34; AttributeError: &#39;A1&#39; object has no attribute &#39;var2&#39; . a.var2 fails as expected because it isn&#39;t in the __slots__ list and __slots__ is read-only so it cannot be updated. . a1.__slots__ = [&#39;var2&#39;] . AttributeError Traceback (most recent call last) &lt;ipython-input-7-5213b9df3654&gt; in &lt;module&gt; -&gt; 1 a1.__slots__ = [&#39;var2&#39;] AttributeError: &#39;A1&#39; object attribute &#39;__slots__&#39; is read-only . a1.__slots__ = [&#39;var1&#39;, &#39;var2&#39;] . AttributeError Traceback (most recent call last) &lt;ipython-input-8-47cd4105f78c&gt; in &lt;module&gt; -&gt; 1 a1.__slots__ = [&#39;var1&#39;, &#39;var2&#39;] AttributeError: &#39;A1&#39; object attribute &#39;__slots__&#39; is read-only . When __slots__ is used, the __dict__ value is not set. Let&#39;s explore that a little further though. . a1.__dict__ . AttributeError Traceback (most recent call last) &lt;ipython-input-9-79636e85e82f&gt; in &lt;module&gt; -&gt; 1 a1.__dict__ AttributeError: &#39;A1&#39; object has no attribute &#39;__dict__&#39; . Example #2a: Exploring __dict__ Without Using __slots__ . class A2A: def __init__(self, value_passed_through_here): self.var1 = value_passed_through_here . a2a = A2A(1) . var1 shows up as expected when creating an object . a2a.__dict__ . {&#39;var1&#39;: 1} . a2a.var2 = &#39;adding a second thing&#39; . Adding a second variable adds it to the __dict__ as expected . a2a.__dict__ . {&#39;var1&#39;: 1, &#39;var2&#39;: &#39;adding a second thing&#39;} . Example #2b: Exploring __dict__ When Using __slots__ . class A2B: __slots__ = [&#39;var1&#39;, &#39;__dict__&#39;] def __init__(self, value_passed_through_here): self.var1 = value_passed_through_here . a2b = A2B(1) . a2b.__dict__ . {} . __dict__ exists now since we added it to __slots__, but it isn&#39;t populating the __dict__ like normal. We are still able to call the attribute var1 though. . a2b.var1 . 1 . a2b.var2 = &quot;test if we can add new variables now&quot; . Surprisingly, once we add __dict__ to the __slots__ list, adding a new var works. . a2b.__dict__ . {&#39;var2&#39;: &#39;test if we can add new variables now&#39;} . When we look at __dict__ after adding var2, there is an entry in __dict__ as well. . a2b.var2 . &#39;test if we can add new variables now&#39; . So if we enable __dict__ we are able to add new items to the __dict__, but __dict__ has to be explicitly defined to work. . Example 3: Inheritance . Now that we&#39;ve explored __slots__, let&#39;s see how it behaves when one class is inherited from another. . class A3: __slots__ = [&#39;a&#39;] def __init__(self, a): self.a = a . a3 = A3(1); a3.a . 1 . class B3A(A3): def __init__(self): self.a = 1 self.b = 2 . b3a = B3A() . b3a.__slots__ . [&#39;a&#39;] . b3a.__dict__ . {&#39;b&#39;: 2} . b3a.a . 1 . b3a.c = &quot;can I set c?&quot; . b3a.__dict__ . {&#39;b&#39;: 2, &#39;c&#39;: &#39;can I set c?&#39;} . So when B3A is inherited from A3, it uses the slots class, but it also reverts back in a lot of ways to a normal, non-slots, class again. The last thing I&#39;m going to try is actually setting a __slots__ in B3B just to see what happens . class B3B(A3): __slots__ = [&#39;a&#39;,&#39;b&#39;] def __init__(self): self.a = 1 self.b = 2 . b3b = B3B() . b3b.c = &quot;can I set this?&quot; . AttributeError Traceback (most recent call last) &lt;ipython-input-33-032b9cd26579&gt; in &lt;module&gt; -&gt; 1 b3b.c = &#34;can I set this?&#34; AttributeError: &#39;B3B&#39; object has no attribute &#39;c&#39; . So now that we have given B3B a __slots__ it is no longer behaves the same way that B3A is. . Here is what the official documentation says about this: . The action of a __slots__ declaration is not limited to the class where it is defined. __slots__ declared in parents are available in child classes. However, child subclasses will get a __dict__ and __weakref__ unless they also define __slots__ (which should only contain names of any additional slots). (https://docs.python.org/3.8/reference/datamodel.html#notes-on-using-slots 5th bullet) . Conclusion . __slots__ is an interesting concept that is built into Python that I hadn&#39;t heard of and wanted to explore. Hopefully this notebook is informative to other Python users as well. Things didn&#39;t always behave as I would have expected and that&#39;s part of the fun of actually testing out the code to see how things work in practice. .",
            "url": "https://blog.problemsolversguild.com/python/technical/exploration/2021/05/21/Python_slots_Exploration.html",
            "relUrl": "/python/technical/exploration/2021/05/21/Python_slots_Exploration.html",
            "date": " • May 21, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "5 Tips for a New CIO",
            "content": "Congratulations on your new role as CIO! As the CIO, your job is to remove roadblocks, provide a vision, and enable your business partners without spending more than is required to do so. Here are 5 tips that I have gathered as an employee watching multiple CIO transitions. . 1. Include Your Employees in your 5-year Plan . Every new CIO wants to create a vision, but one of the biggest mistakes that a new CIO can make is relying too much on external consultants or upper management to lay out this vision. Unfortunately, this type of top-down leadership strategy tends to lead to low adoption rates. While it is a great way to create a bold vision and great slide-deck, the resulting plan typically lacks adoption and leads to bland execution. . If you want buy-in to your vision, make sure you listen and absorb as you enter the new landscape and involve your employees when creating an execution plan. Finding a way to blend your 5-year plan with an execution roadmap that will get to that point is what will separate a great CIO from an average CIO. Give your staff a vision and operating rules and let them help you get to the end-state. Your job is to eliminate any roadblocks in the way of your vision and to make sure that everything that is being done today is in line with where you are trying to get to. . 2. Utilize Consultants Sparingly . Consultants are often used as scapegoats when a project or initiative isn’t on track, but consultants can be a great resource for an organization when utilized properly. The key with utilizing consultants is to make sure you are getting the experience that consultants can bring with them. Ask yourself and your team if the value being provided is worth the money being paid and also consider the technical debt that is being added by using a consultant. Always ensure that a full-time employee is paired with the consultant to learn from them and to take over the project before a consultant leaves. Any work done by consultants must include meaningful documentation and meaningful tests. Consultants can also be utilized to spin up an environment, but make sure an internal resource is involved as well because once the environment is up, you don’t want to rely on a consultant to handle ongoing support. . 3. Choose Your Advisors Carefully . Along with eliminating roadblocks, another role as a new CIO is to identify who you listen to around the organization. Identifying people you should take advice from can be daunting task but it is the only way to effectively guide the organization. One way to help identify who you should take advice from in an organization is to engage in brainstorming sessions. The idea behind these sessions is to help develop a vision that everybody buys into and to get everybody talking in the same conversations. Not everybody is comfortable with the same medium so it is a good idea to follow up on these brainstorming sessions with output items as well based on the conversations. This is a great way to figure out who is going to execute at a high level. Another important thing to keep in mind with this group of advisors is that they have to be willing to tell you their true opinion even if it’s not popular. You should be encouraging this to the entire staff, but really emphasize it with your advisor team. . Once these projects start to gain traction, go out of your way to recognize members of your team and solicit advice from them. Ask them specifically what is working well, what isn’t working well, and see if they have any ideas to help improve things going forward. Focus on the areas that they talk about and ask follow-up questions. Identify the strengths and weaknesses of yourself and your advisors. . 4. Learn From Outages Together . Issues can appear in many forms: missed SLAs, unresponsive servers, hanging jobs. Unplanned issues and outages should be viewed at as a huge opportunity to get your team a better understanding of how the overall technical landscape works at your company. Any time a member of your team is spending time fixing systems or chasing down issues, they aren’t working on the problems that are guiding the organization towards the 5-year vision. Understanding why system issues occur, how the issue was detected, how the solution was identified, and what can be done to reduce issues in the future either in severity or detection time are the key areas that should be focused on in this type of post-mortem. Be mindful that this doesn’t turn into a blame game. This is a collaborative team-building exercise and everybody should be growing from it. This is a great exercise that can help you understand how different pieces of the puzzle fit together and get everybody on the same page. . 5. Guide the Ship . As the CIO, you are responsible for crafting a draft of your organization’s roadmap and guiding your team. There will be times when you have to make changes to the 5-year vision, but it’s important to keep it in mind as you make decisions and enable your team to make decisions. Make yourself intentionally available to your organization. Keep an open door policy and advertise it. If you solicit advice and nobody responds, you probably need to re-consider how you are reaching out to your staff. Try meeting with small group, virtual suggestion boxes, or even one-on-one discussions until you find a medium that connects. Your actions will play a big part in setting the tone of the entire organization. .",
            "url": "https://blog.problemsolversguild.com/advice/2021/05/18/5_Tips_for_a_New_CIO.html",
            "relUrl": "/advice/2021/05/18/5_Tips_for_a_New_CIO.html",
            "date": " • May 18, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "fastai's Text DLs Exploration",
            "content": "Introduction . This post is an exploration into how to convert data into dls objects that can be used by fastai&#39;s learner. I was having issues creating a dls object that had the ability to show_batch and met my arbitrarily custom needs. So I set out to figure out how to create dls that worked well for my needs. . This blog post uses the Human Numbers dataset which is a dataset that counts sequentially from 1 to 9999 but in english text rather than numerical form. This is an interesting problem because there is quite a bit of repetition, but also new tokens and patterns being introduced regularly that a model will need to figure out. . My goal was to create a dls that would have X=1,2,3 and y=4. Over the course of this blog post, I will show ~4 ways to create dls that enable show_batch to work as expected. . from fastai.text.all import * . import fastai, fastcore . fastai.__version__,fastcore.__version__ . (&#39;2.3.1&#39;, &#39;1.3.20&#39;) . path = untar_data(URLs.HUMAN_NUMBERS) . First, I create a tokenizer, combine all of the text into a single string, and tokenize each word . tokenizer = Tokenizer(WordTokenizer()) . Testing the Tokenizer . tokenizer(&#39;one two three&#39;) . (#4) [&#39;xxbos&#39;,&#39;one&#39;,&#39;two&#39;,&#39;three&#39;] . tokenizer(&#39;one, two&#39;) . (#4) [&#39;xxbos&#39;,&#39;one&#39;,&#39;,&#39;,&#39;two&#39;] . Reading the train and validation files: . train - [1-8000] | valid = [8001-9999] | . train_txt = &#39;, &#39;.join(o.strip() for o in (path/&#39;train.txt&#39;).readlines()) valid_txt = &#39;, &#39;.join(o.strip() for o in (path/&#39;valid.txt&#39;).readlines()) . For this problem, I will create my own validation set. It will split close to the same as this, but by creating my own split, I don&#39;t have to do anything special when creating chunks around the train-&gt;validation split point . all_text = train_txt+valid_txt . all_text_tok = tokenizer(all_text) . all_text_tok . (#63094) [&#39;xxbos&#39;,&#39;one&#39;,&#39;,&#39;,&#39;two&#39;,&#39;,&#39;,&#39;three&#39;,&#39;,&#39;,&#39;four&#39;,&#39;,&#39;,&#39;five&#39;...] . Next, I take the tokenized text, count how many times each tokenizer occurs and create a vocab with that. . count=Counter(all_text_tok) vocab = make_vocab(count) . print(count) . Counter({&#39;,&#39;: 9996, &#39;hundred&#39;: 9000, &#39;thousand&#39;: 8999, &#39;one&#39;: 2900, &#39;two&#39;: 2900, &#39;three&#39;: 2900, &#39;four&#39;: 2900, &#39;five&#39;: 2900, &#39;six&#39;: 2900, &#39;seven&#39;: 2900, &#39;nine&#39;: 2899, &#39;eight&#39;: 2898, &#39;twenty&#39;: 1000, &#39;thirty&#39;: 1000, &#39;forty&#39;: 1000, &#39;fifty&#39;: 1000, &#39;sixty&#39;: 1000, &#39;seventy&#39;: 1000, &#39;eighty&#39;: 1000, &#39;ninety&#39;: 1000, &#39;ten&#39;: 100, &#39;eleven&#39;: 100, &#39;twelve&#39;: 100, &#39;thirteen&#39;: 100, &#39;fourteen&#39;: 100, &#39;fifteen&#39;: 100, &#39;sixteen&#39;: 100, &#39;seventeen&#39;: 100, &#39;eighteen&#39;: 100, &#39;nineteen&#39;: 100, &#39;xxbos&#39;: 1, &#39;nineeight&#39;: 1}) . print(vocab) . [&#39;xxunk&#39;, &#39;xxpad&#39;, &#39;xxbos&#39;, &#39;xxeos&#39;, &#39;xxfld&#39;, &#39;xxrep&#39;, &#39;xxwrep&#39;, &#39;xxup&#39;, &#39;xxmaj&#39;, &#39;,&#39;, &#39;hundred&#39;, &#39;thousand&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;, &#39;four&#39;, &#39;five&#39;, &#39;six&#39;, &#39;seven&#39;, &#39;nine&#39;, &#39;eight&#39;, &#39;twenty&#39;, &#39;thirty&#39;, &#39;forty&#39;, &#39;fifty&#39;, &#39;sixty&#39;, &#39;seventy&#39;, &#39;eighty&#39;, &#39;ninety&#39;, &#39;ten&#39;, &#39;eleven&#39;, &#39;twelve&#39;, &#39;thirteen&#39;, &#39;fourteen&#39;, &#39;fifteen&#39;, &#39;sixteen&#39;, &#39;seventeen&#39;, &#39;eighteen&#39;, &#39;nineteen&#39;, &#39;xxfake&#39;] . all_text_tok_chunked = list(chunked(all_text_tok, 11)) . all_text_tok_chunked = all_text_tok_chunked[:-1] . Next I create something that will get_x and get_y from the chunked data. . def get_x(o): return o[:10] def get_y(o): return [o[10]] if len(o) == 11 else [&#39;.&#39;] . print(f&quot;{get_x(all_text_tok_chunked[0])} -&gt; {get_y(all_text_tok_chunked[0])}&quot;) . [&#39;xxbos&#39;, &#39;one&#39;, &#39;,&#39;, &#39;two&#39;, &#39;,&#39;, &#39;three&#39;, &#39;,&#39;, &#39;four&#39;, &#39;,&#39;, &#39;five&#39;] -&gt; [&#39;,&#39;] . print(f&quot;{get_x(all_text_tok_chunked[-1])} -&gt; {get_y(all_text_tok_chunked[-1])}&quot;) . [&#39;nine&#39;, &#39;thousand&#39;, &#39;nine&#39;, &#39;hundred&#39;, &#39;ninety&#39;, &#39;seven&#39;, &#39;,&#39;, &#39;nine&#39;, &#39;thousand&#39;, &#39;nine&#39;] -&gt; [&#39;hundred&#39;] . TitledStringDecoder is a transform that only decodes and what it enables is the show_batch and show_results function to actually work properly. Without this, I had troubles getting those functions to work because TensorText doesn&#39;t have a proper show function or a truncate function. . class TitledStringDecoder(Transform): def decodes(self, o): return TitledStr(&#39; &#39;.join(o)) . All TitledStringDecoder really does is takes an array of text (&#39;one&#39;, &#39;two&#39;) and converts it into a space-concatenated string instead of type Titled str which knows how to display itself in a nice way. . TitledStr(&#39; &#39;.join([&#39;one&#39;, &#39;two&#39;])) . &#39;one two&#39; . tmp_ts = TitledStr(&#39; &#39;.join(all_text_tok[:10])) . tmp_ts . &#39;xxbos one , two , three , four , five&#39; . tmp_ts.truncate(3) . &#39;xxbos one ,&#39; . I create the splits based off the chunks. Putting 80% of the chunks into the training set and the last 20% in the validation set . splits = [L(range(int(len(all_text_tok_chunked)*0.8))), L(range(int(len(all_text_tok_chunked)*0.8),len(all_text_tok_chunked)))] . splits . [(#4588) [0,1,2,3,4,5,6,7,8,9...], (#1147) [4588,4589,4590,4591,4592,4593,4594,4595,4596,4597...]] . Now, let&#39;s test the transforms work properly . Numericalize(vocab)(TitledStringDecoder()(get_x(all_text_tok_chunked[0]))) . TensorText([ 2, 12, 9, 13, 9, 14, 9, 15, 9, 16]) . And confirm that they will work as a pipeline as well . pipeline = Pipeline([TitledStringDecoder, Numericalize(vocab)]) . get_x(pipeline(all_text_tok_chunked[0])) . TensorText([ 2, 12, 9, 13, 9, 14, 9, 15, 9, 16]) . pipeline_x = Pipeline([get_x, TitledStringDecoder, Numericalize(vocab)]) pipeline_y = Pipeline([get_y, TitledStringDecoder, Numericalize(vocab)]) . pipeline_y(all_text_tok_chunked[0]) . TensorText([9]) . Using Datasets + Dataloaders . dsets = Datasets(all_text_tok_chunked, tfms=[pipeline_x,pipeline_y], splits=splits) . dsets[0] . (TensorText([ 2, 12, 9, 13, 9, 14, 9, 15, 9, 16]), TensorText([9])) . dsets.show(dsets[0]) . xxbos one , two , three , four , five , . Next, we can create the dataloaders. This can be done with either DataLoaders.from_dsets(...) or dsets.dataloaders(...). Both methods are shown below. . dls = DataLoaders.from_dsets(dsets, shuffle=False, drop_last=True) . dls.show_batch() . text text_ . 0 xxbos one , two , three , four , five | , | . 1 six , seven , eight , nine , ten , | eleven | . 2 , twelve , thirteen , fourteen , fifteen , sixteen | , | . 3 seventeen , eighteen , nineteen , twenty , twenty one | , | . 4 twenty two , twenty three , twenty four , twenty | five | . 5 , twenty six , twenty seven , twenty eight , | twenty | . 6 nine , thirty , thirty one , thirty two , | thirty | . 7 three , thirty four , thirty five , thirty six | , | . 8 thirty seven , thirty eight , thirty nine , forty | , | . dls = dsets.dataloaders(bs=16, shuffle=False, drop_last=True) . dls.show_batch() . text text_ . 0 xxbos one , two , three , four , five | , | . 1 six , seven , eight , nine , ten , | eleven | . 2 , twelve , thirteen , fourteen , fifteen , sixteen | , | . 3 seventeen , eighteen , nineteen , twenty , twenty one | , | . 4 twenty two , twenty three , twenty four , twenty | five | . 5 , twenty six , twenty seven , twenty eight , | twenty | . 6 nine , thirty , thirty one , thirty two , | thirty | . 7 three , thirty four , thirty five , thirty six | , | . 8 thirty seven , thirty eight , thirty nine , forty | , | . Using Datasets -&gt; train TfmdDL + valid TfmdDL -&gt; dataloaders . Another way to get dls is to create TfmdDLs and pass those into DataLoaders. If you use DataLoader rather than TfmdDL, dls won&#39;t have a show_batch method available. . train_dl = TfmdDL(dsets.train, bs=16, drop_last=True) . valid_dl = TfmdDL(dsets.valid, bs=16, drop_last=True) . dls = DataLoaders(train_dl, valid_dl) . dls.show_batch() . text text_ . 0 xxbos one , two , three , four , five | , | . 1 six , seven , eight , nine , ten , | eleven | . 2 , twelve , thirteen , fourteen , fifteen , sixteen | , | . 3 seventeen , eighteen , nineteen , twenty , twenty one | , | . 4 twenty two , twenty three , twenty four , twenty | five | . 5 , twenty six , twenty seven , twenty eight , | twenty | . 6 nine , thirty , thirty one , thirty two , | thirty | . 7 three , thirty four , thirty five , thirty six | , | . 8 thirty seven , thirty eight , thirty nine , forty | , | . X,y = dls.one_batch() . Using DataBlock -&gt; datasets -&gt; dataloaders . Another way to get dataloaders is to use DataBlock. DataBlock wants to know what type of data will be passed which can be specified to blocks. It also wants a splitter and the functions to get_x and get_y . blocks = [TransformBlock(type_tfms=[TitledStringDecoder, Numericalize(vocab)]), # x piece TransformBlock(type_tfms=[TitledStringDecoder, Numericalize(vocab)])] # y piece . splits[-1] . (#1147) [4588,4589,4590,4591,4592,4593,4594,4595,4596,4597...] . IndexSplitter(splits[-1])(all_text_tok_chunked) . ((#4588) [0,1,2,3,4,5,6,7,8,9...], (#1147) [4588,4589,4590,4591,4592,4593,4594,4595,4596,4597...]) . dblock = DataBlock(blocks=blocks, splitter=IndexSplitter(splits[-1]), get_x=get_x, get_y=get_y) . With the dblock created, you can create a dset and then from the dset, you can create a dls similar to the one created above. . dsets_via_dblock = dblock.datasets(all_text_tok_chunked) . dsets_via_dblock . (#5735) [(TensorText([ 2, 12, 9, 13, 9, 14, 9, 15, 9, 16]), TensorText([9])),(TensorText([17, 9, 18, 9, 20, 9, 19, 9, 29, 9]), TensorText([30])),(TensorText([ 9, 31, 9, 32, 9, 33, 9, 34, 9, 35]), TensorText([9])),(TensorText([36, 9, 37, 9, 38, 9, 21, 9, 21, 12]), TensorText([9])),(TensorText([21, 13, 9, 21, 14, 9, 21, 15, 9, 21]), TensorText([16])),(TensorText([ 9, 21, 17, 9, 21, 18, 9, 21, 20, 9]), TensorText([21])),(TensorText([19, 9, 22, 9, 22, 12, 9, 22, 13, 9]), TensorText([22])),(TensorText([14, 9, 22, 15, 9, 22, 16, 9, 22, 17]), TensorText([9])),(TensorText([22, 18, 9, 22, 20, 9, 22, 19, 9, 23]), TensorText([9])),(TensorText([23, 12, 9, 23, 13, 9, 23, 14, 9, 23]), TensorText([15]))...] . dsets_via_dblock.show(dsets_via_dblock[0]) . xxbos one , two , three , four , five , . dls = dsets_via_dblock.dataloaders(bs=16,shuffle=False, drop_last=True) . dls.show_batch() . text text_ . 0 xxbos one , two , three , four , five | , | . 1 six , seven , eight , nine , ten , | eleven | . 2 , twelve , thirteen , fourteen , fifteen , sixteen | , | . 3 seventeen , eighteen , nineteen , twenty , twenty one | , | . 4 twenty two , twenty three , twenty four , twenty | five | . 5 , twenty six , twenty seven , twenty eight , | twenty | . 6 nine , thirty , thirty one , thirty two , | thirty | . 7 three , thirty four , thirty five , thirty six | , | . 8 thirty seven , thirty eight , thirty nine , forty | , | . Using DataBlock -&gt; dataloaders . Another option is to go directly from dblock to dls with dblock.dataloaders. Behind the scenes this is creating a dataset as well, but it can be a cleaner looking way to handle it if you always go from dblock -&gt; dls. . dls = dblock.dataloaders(all_text_tok_chunked, bs=16, shuffle=False, drop_last=True) . dls.show_batch() . text text_ . 0 xxbos one , two , three , four , five | , | . 1 six , seven , eight , nine , ten , | eleven | . 2 , twelve , thirteen , fourteen , fifteen , sixteen | , | . 3 seventeen , eighteen , nineteen , twenty , twenty one | , | . 4 twenty two , twenty three , twenty four , twenty | five | . 5 , twenty six , twenty seven , twenty eight , | twenty | . 6 nine , thirty , thirty one , thirty two , | thirty | . 7 three , thirty four , thirty five , thirty six | , | . 8 thirty seven , thirty eight , thirty nine , forty | , | . Conclusion . Creating dls is an extremely important capability when using fastai because that is what a learn expects to deal with all of the data. There are many different ways to get a dls object created so this isn&#39;t a comprehensive list, but at least shows a few ways to do the task. In a future blog post, I will be using this dls and exploring transformer models with it. Hopefully this will help others get their DLs working. . I&#39;d like to give a special thanks to Arto for helping me get things working properly and everybody in the fastai discord channel for dealing with my questions and for creating a great community to learn with every step of the way. . Useful Links . https://arampacha.github.io/thoughtsamples/fastai/pytorch/2021/01/02/transformer-lm-from-scratch.html . https://github.com/fastai/fastai/blob/ab154927696338741e59e0ffc4774777c4a9781c/nbs/39_tutorial.transformers.ipynb https://github.com/fastai/fastai/blob/ab154927696338741e59e0ffc4774777c4a9781c/dev_nbs/course/lesson7-human-numbers.ipynb .",
            "url": "https://blog.problemsolversguild.com/fastai/technical/exploration/2021/05/14/Text_DLs_Exploration.html",
            "relUrl": "/fastai/technical/exploration/2021/05/14/Text_DLs_Exploration.html",
            "date": " • May 14, 2021"
        }
        
    
  

  
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.problemsolversguild.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}