{
  
    
        "post0": {
            "title": "Normalizing Tabular Inputs",
            "content": "Normalizing data is an important aspect of data science that seemed like magic to me for the longest time. I could train a model for 10 epochs on un-normalized data and make little or no progress towards an effective model, then restart and run the same data through my model with normalization applied and have a perfect model. Why was this the case? What was happening that caused such a big difference? Let&#39;s explore that question. . import numpy as np import matplotlib.pyplot as plt . What is Normalization? . Normalization is the process of converting all of the numbers so the full set of numbers has a mean of 0 and a standard deviation of 1. My guess is that for most people reading this, those numbers and terms don&#39;t mean a lot. So let&#39;s dig in a bit deeper. Normalization is the process of putting all of our values on a common scale. By ensuring that the overall number range has a mean of 0, each side of zero will have half of the numbers. That means that all of the values are situated around 0. Let&#39;s get a real example. . Let&#39;s assume we have a set of numbers: [1, 5, 10, 25, 100, 1000]. . example_1 = np.array([1, 5, 10, 25, 100, 1000]) . plt.plot(example_1, [0]*len(example_1), marker=&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f8015e3b520&gt;] . How would we transform these numbers to have a mean of 0? Let&#39;s start by finding the current mean of these numbers. . example_1.mean() . 190.16666666666666 . 190.16666 is the mean of this set of numbers. In order to make this 0, we should be able to just subtract each number by that amount. . example_1_mean_0 = example_1-example_1.mean(); example_1_mean_0 . array([-189.16666667, -185.16666667, -180.16666667, -165.16666667, -90.16666667, 809.83333333]) . plt.plot(example_1_mean_0, [0]*len(example_1_mean_0), marker=&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f8013d27250&gt;] . f&quot;The mean is now: {example_1_mean_0.mean()} (AKA: 0)&quot; . &#39;The mean is now: 1.8947806286936004e-14 (AKA: 0)&#39; . We haven&#39;t really done anything at this point besides changing the balancing point of all of these numbers. Now instead of being distributed around the number 190.1666, the numbers are distributed around the number 0. You may have noticed that these numbers are still clumped up. The numbers -189 and -185 are very close to each other and might be hard to distinguish. Wouldn&#39;t it be nice if we could more evenly distribute those values no matter what the original scale was? That is exactly what the standard deviation of 1 requirement is designed to do. In order for a set of numbers to have a standard deviation of 1, they have to be evenly distributed. In order to do this, we need to divide by the current standard deviation since that will scale everything appropriately to have a standard deviation of 1. . example_1_mean_0_std_1 = example_1_mean_0/example_1_mean_0.std(); example_1_mean_0_std_1 . array([-0.52008301, -0.50908566, -0.49533897, -0.45409891, -0.2478986 , 2.22650516]) . plt.plot(example_1_mean_0_std_1, [0]*len(example_1_mean_0_std_1), marker=&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f8013d03c10&gt;] . example_1_mean_0_std_1.mean(), example_1_mean_0_std_1.std() . (7.401486830834377e-17, 1.0) . example_1_mean_0_std_1 . array([-0.52008301, -0.50908566, -0.49533897, -0.45409891, -0.2478986 , 2.22650516]) . Notice that in all three graphs, the distribution looks the same. Lots of numbers on the left-hand side and only the one number on the right. But to our neural network, there is a huge difference between taking numbers ranging from -0.52 and 2.23 and taking numbers ranging from 1 to 1000. The weights can make a lot finer adjustments on the normalized version and each incoming data point will be on the same scale. That&#39;s why when an image model has been pretrained on Imagenet, you have to keep the same standard deviation and mean. By using those numbers, you are taking all of the pixels in your image and converting them to the normal range for Imagenet. . Testing the Theory with fastai&#39;s Tabular Application . Now that we have an idea of what is happening, let&#39;s see if we can prove that it actually works how we anticipate. To do this, I will be creating a simple tabular learner and a sample dataloader. I will first train the model on an normalized input and then train the same model using the un-normalized input. My assumption is that the normalized inputs will be much better at converting the x value into y than the un-normalized version. . Experiment Setup . x1 + xN = y . from fastai.tabular.all import * x = torch.rand((200000,10)) scale_amt = torch.randint(0,100,size=(10,)) #x = x*scale_amt df = pd.DataFrame(x, columns=[f&quot;x{n}&quot; for n in range(1,11)]) #df[&#39;x1&#39;]=df[&#39;x1&#39;]*1000 #df.columns #df[&#39;y&#39;] = df[&#39;x1&#39;] + df[&#39;x2&#39;] + df[&#39;x3&#39;] + df[&#39;x4&#39;] + df[&#39;x5&#39;] + df[&#39;x6&#39;] + df[&#39;x7&#39;] + df[&#39;x8&#39;] + df[&#39;x9&#39;] + df[&#39;x10&#39;] df[&#39;y&#39;] = (df.values*scale_amt.tolist()).sum(axis=1) splits = RandomSplitter()(df) df.head() . x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 y . 0 0.545165 | 0.987303 | 0.005958 | 0.157278 | 0.059937 | 0.173226 | 0.372957 | 0.265160 | 0.057287 | 0.930829 | 136.481674 | . 1 0.066906 | 0.222810 | 0.789908 | 0.041755 | 0.124091 | 0.454513 | 0.161850 | 0.810107 | 0.233493 | 0.710355 | 122.886699 | . 2 0.189592 | 0.444550 | 0.631535 | 0.815730 | 0.626703 | 0.461263 | 0.548467 | 0.688737 | 0.461810 | 0.311470 | 221.644480 | . 3 0.583206 | 0.604546 | 0.995949 | 0.979822 | 0.044589 | 0.756607 | 0.989755 | 0.962176 | 0.376933 | 0.149482 | 275.019872 | . 4 0.068740 | 0.471974 | 0.623846 | 0.015703 | 0.350679 | 0.183968 | 0.637568 | 0.792408 | 0.305980 | 0.477072 | 142.014781 | . monitor_parameters is a hook that can be used to monitor values inside of a model. It is a little bit of a hack, but it works really well for getting a better idea what is happening inside the model. . def monitor_parameters(m, i, o): m.weight_track.append(list(m.parameters())[0].tolist()[0]) m.bias_track.append(list(m.parameters())[1].tolist()) . Normalized . to_normalized = TabularPandas(df, cont_names=[&#39;x1&#39;, &#39;x2&#39;, &#39;x3&#39;, &#39;x4&#39;, &#39;x5&#39;, &#39;x6&#39;, &#39;x7&#39;, &#39;x8&#39;, &#39;x9&#39;, &#39;x10&#39;], y_names=&#39;y&#39;, procs=[Normalize], splits=splits) dls_normalized = to_normalized.dataloaders(verbose=True, shuffle=False) learn_normalized = tabular_learner(dls_normalized, layers=[], config=tabular_config(use_bn=False, bn_cont=False)) learn_normalized.lr_find(start_lr=1e-3, end_lr=1000000) . Setting up after_item: Pipeline: Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: ReadTabBatch . SuggestedLRs(valley=2.630268096923828) . learn_normalized.model.layers[0][0].bias_track = [] learn_normalized.model.layers[0][0].weight_track = [] learn_normalized.model.layers[0][0].register_full_backward_hook(monitor_parameters) . &lt;torch.utils.hooks.RemovableHandle at 0x7f7e6d41a6d0&gt; . learn_normalized.fit_one_cycle(10, 10) learn_normalized.recorder.plot_loss(skip_start=1000) learn_normalized.show_results(ds_idx=0, shuffle=False) . epoch train_loss valid_loss time . 0 | 1.000931 | 0.999392 | 00:08 | . 1 | 17.346672 | 40.162426 | 00:08 | . 2 | 13.310960 | 12.002500 | 00:08 | . 3 | 3.002940 | 6.226650 | 00:08 | . 4 | 3.835130 | 0.752128 | 00:08 | . 5 | 1.692684 | 4.275723 | 00:08 | . 6 | 0.633194 | 1.079318 | 00:08 | . 7 | 0.151731 | 0.112498 | 00:08 | . 8 | 0.016996 | 0.030032 | 00:08 | . 9 | 0.000252 | 0.000233 | 00:08 | . x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 y y_pred . 0 -0.079419 | -0.871131 | -0.956761 | 1.266894 | -1.229273 | 1.506219 | 0.696179 | 1.544797 | 0.125643 | 1.584143 | 260.773682 | 260.739929 | . 1 -1.237262 | 0.797087 | 0.164796 | -0.461996 | 0.420892 | -0.344432 | -0.087012 | -0.839776 | -0.069317 | 0.854366 | 209.821198 | 209.823105 | . 2 -0.097476 | -0.795489 | 1.155189 | -0.233047 | -0.723798 | -1.342646 | -1.341711 | -0.292966 | -0.007374 | 0.161002 | 152.537537 | 152.539276 | . 3 -0.556748 | -1.395583 | 0.857968 | -0.076637 | -1.564265 | -0.687968 | -0.032200 | 0.543396 | -0.319755 | -0.043718 | 161.902405 | 161.896851 | . 4 -0.334514 | -0.166014 | -1.254841 | 0.564868 | -1.433059 | -0.966879 | -1.148502 | -1.522199 | -0.492140 | 1.214049 | 158.776001 | 158.781342 | . 5 -1.548363 | 1.060259 | -0.995473 | -0.589741 | 1.080149 | -0.116785 | -1.299394 | -1.726501 | -0.952094 | -0.265381 | 156.827560 | 156.836884 | . 6 -0.415335 | 0.909286 | -1.471615 | 1.124116 | 0.380935 | -0.990880 | 1.105329 | 0.754777 | -1.349851 | -0.822788 | 194.239136 | 194.247116 | . 7 -0.880697 | -1.574562 | 0.922269 | 1.318255 | -0.263991 | 0.003277 | -0.122180 | 1.590251 | 0.771138 | -1.460533 | 215.817535 | 215.799606 | . 8 1.410347 | -1.599703 | 0.632277 | -0.811613 | -1.719727 | -1.259379 | 0.514772 | -0.187357 | -1.441254 | 1.343041 | 137.570435 | 137.562714 | . plt.plot(learn_normalized.model.layers[0][0].bias_track) . [&lt;matplotlib.lines.Line2D at 0x7f7e6c6859a0&gt;] . plt.plot(learn_normalized.model.layers[0][0].weight_track) . [&lt;matplotlib.lines.Line2D at 0x7f7e6e59c940&gt;, &lt;matplotlib.lines.Line2D at 0x7f7e6d13f850&gt;, &lt;matplotlib.lines.Line2D at 0x7f7e6c68c460&gt;, &lt;matplotlib.lines.Line2D at 0x7f7e6c699f70&gt;, &lt;matplotlib.lines.Line2D at 0x7f7e4254d8e0&gt;, &lt;matplotlib.lines.Line2D at 0x7f7e4254d9a0&gt;, &lt;matplotlib.lines.Line2D at 0x7f7e4254da60&gt;, &lt;matplotlib.lines.Line2D at 0x7f7e4254db20&gt;, &lt;matplotlib.lines.Line2D at 0x7f7e4254dbe0&gt;, &lt;matplotlib.lines.Line2D at 0x7f7e4254dca0&gt;] . Not Normalized . to_not_normalized = TabularPandas(df, cont_names=[&#39;x1&#39;, &#39;x2&#39;, &#39;x3&#39;, &#39;x4&#39;, &#39;x5&#39;, &#39;x6&#39;, &#39;x7&#39;, &#39;x8&#39;, &#39;x9&#39;, &#39;x10&#39;], y_names=[&#39;y&#39;], splits=splits) dls_not_normalized = to_not_normalized.dataloaders(verbose=True, shuffle=False) dls_not_normalized.one_batch() learn_not_normalized = tabular_learner(dls_not_normalized, layers=[], config=tabular_config(use_bn=False, bn_cont=False), train_bn=False) learn_not_normalized.lr_find(start_lr=1e-3, end_lr=1000000) . Setting up after_item: Pipeline: Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: ReadTabBatch . SuggestedLRs(valley=2.1379621028900146) . learn_not_normalized.model.layers[0][0].bias_track = [] learn_not_normalized.model.layers[0][0].weight_track = [] learn_not_normalized.model.layers[0][0].register_full_backward_hook(monitor_parameters) . &lt;torch.utils.hooks.RemovableHandle at 0x7f7e426bbe20&gt; . learn_not_normalized.fit_one_cycle(10, 10) learn_not_normalized.recorder.plot_loss(skip_start=500) learn_not_normalized.show_results(ds_idx=0, shuffle=False) . epoch train_loss valid_loss time . 0 | 17.797153 | 16.464233 | 00:08 | . 1 | 94.062332 | 88.356964 | 00:08 | . 2 | 112.147125 | 101.556259 | 00:08 | . 3 | 101.421333 | 93.121895 | 00:08 | . 4 | 78.316704 | 78.493805 | 00:08 | . 5 | 47.606586 | 45.184231 | 00:08 | . 6 | 25.531321 | 23.139017 | 00:08 | . 7 | 10.122910 | 9.399118 | 00:08 | . 8 | 1.497447 | 1.419698 | 00:08 | . 9 | 0.040263 | 0.039970 | 00:08 | . x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 y y_pred . 0 0.477266 | 0.247732 | 0.224613 | 0.865618 | 0.145454 | 0.934797 | 0.700869 | 0.946973 | 0.535999 | 0.956671 | 260.773682 | 260.508392 | . 1 0.142747 | 0.729076 | 0.548405 | 0.365993 | 0.623227 | 0.400701 | 0.474965 | 0.257510 | 0.479615 | 0.746145 | 209.821198 | 209.837524 | . 2 0.472049 | 0.269558 | 0.834331 | 0.432157 | 0.291804 | 0.112618 | 0.113058 | 0.415612 | 0.497530 | 0.546123 | 152.537537 | 152.768845 | . 3 0.339358 | 0.096408 | 0.748523 | 0.477357 | 0.048463 | 0.301557 | 0.490775 | 0.657433 | 0.407187 | 0.487065 | 161.902405 | 162.085129 | . 4 0.403565 | 0.451185 | 0.138558 | 0.662743 | 0.086451 | 0.221064 | 0.168788 | 0.060198 | 0.357333 | 0.849907 | 158.776001 | 159.010284 | . 5 0.052865 | 0.805012 | 0.213437 | 0.329077 | 0.814103 | 0.466400 | 0.125264 | 0.001127 | 0.224312 | 0.423120 | 156.827560 | 157.115356 | . 6 0.380215 | 0.761450 | 0.075975 | 0.824358 | 0.611659 | 0.214137 | 0.818885 | 0.718551 | 0.109278 | 0.262319 | 194.239136 | 194.303299 | . 7 0.245764 | 0.044765 | 0.767087 | 0.880461 | 0.424933 | 0.501050 | 0.464821 | 0.960116 | 0.722679 | 0.078342 | 215.817535 | 215.709244 | . 8 0.907683 | 0.037511 | 0.683366 | 0.264959 | 0.003452 | 0.136649 | 0.648544 | 0.446147 | 0.082844 | 0.887118 | 137.570435 | 137.905518 | . plt.plot(learn_not_normalized.model.layers[0][0].bias_track) . [&lt;matplotlib.lines.Line2D at 0x7f7e6d4b7910&gt;] . plt.plot(learn_not_normalized.model.layers[0][0].weight_track) . [&lt;matplotlib.lines.Line2D at 0x7f7e6d4dfa00&gt;, &lt;matplotlib.lines.Line2D at 0x7f7e6d4df130&gt;, &lt;matplotlib.lines.Line2D at 0x7f7e6d4df820&gt;, &lt;matplotlib.lines.Line2D at 0x7f7e6d4df670&gt;, &lt;matplotlib.lines.Line2D at 0x7f7e6d4dfb20&gt;, &lt;matplotlib.lines.Line2D at 0x7f7e6d4dfbe0&gt;, &lt;matplotlib.lines.Line2D at 0x7f7e6d4df190&gt;, &lt;matplotlib.lines.Line2D at 0x7f7e6d4dfd90&gt;, &lt;matplotlib.lines.Line2D at 0x7f7e6d4dfee0&gt;, &lt;matplotlib.lines.Line2D at 0x7f7e6d4df730&gt;] . scale_amt . tensor([13, 48, 32, 68, 25, 62, 59, 0, 75, 35]) . list(learn_not_normalized.model.layers[0][0].parameters()) . [Parameter containing: tensor([[ 1.2934e+01, 4.7830e+01, 3.1887e+01, 6.7687e+01, 2.4900e+01, 6.1743e+01, 5.8763e+01, -3.1485e-02, 7.4586e+01, 3.4889e+01]], device=&#39;cuda:0&#39;, requires_grad=True), Parameter containing: tensor([0.9179], device=&#39;cuda:0&#39;, requires_grad=True)] . list(learn_normalized.model.layers[0][0].parameters()) . [Parameter containing: tensor([[ 3.7565e+00, 1.3848e+01, 9.2383e+00, 1.9648e+01, 7.2382e+00, 1.7898e+01, 1.7023e+01, -5.4254e-05, 2.1695e+01, 1.0098e+01]], device=&#39;cuda:0&#39;, requires_grad=True), Parameter containing: tensor([208.4588], device=&#39;cuda:0&#39;, requires_grad=True)] . Results . The results ended up matching our theory but in a slightly different way than I had anticipated. You can see that the non-normalized weights line up a lot closer to the scale_amt and after thinking about the reason for that, I think I understand. When the input values are normalized, all of the numbers are put onto the same scale so the easiest way for the gradients to be adjusted is to just move the bias. But in the non-normalized version when adjusting the bias, all of the individual weights would be moving at different rates so it is a lot less likely that the bias will be moved as the best step for the model. Instead the weights are adjusted more during the training process. .",
            "url": "https://blog.problemsolversguild.com/technical/2021/08/16/Tabular_Normalization_Exploration.html",
            "relUrl": "/technical/2021/08/16/Tabular_Normalization_Exploration.html",
            "date": " • Aug 16, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "5 Lessons from Richard Feynman",
            "content": "I recently listened to a biography about Richard Feynman and I had some takeaways that surprised me. I had always vaguely known that Richard Feynman was an interesting guy and a great professor, but this book went so much deeper and taught me some valuable lessons. . Lesson 1: Don’t Stay in Your Lane . Whether you are a writer that has decided to explore a topic that isn’t your forte or an athlete that has taken up a social justice campaign, expanding your boundaries and getting out of your comfort zone will have surprising results. Even if it doesn’t become a passion, it will still give you a perspective you wouldn’t have otherwise had. Being confined to a single topic just because you are good at it can also lead to burnout and won’t let you reach your full potential. . Lesson 2: Don’t Trust and Always Verify . Just because other people agree with the results, doesn’t mean that the results are correct. Being able to reproduce results and understanding the assumptions that have been made by previous researchers can at a minimum help you obtain a more accurate mental model and might even bring up some questions about why certain assumptions were made. Sometimes this might have to be done in stages. Maybe when you are first looking at a new paper, trust everything except for their results and then as you are able to reproduce the results at that layer, go back a layer and re-consider the assumptions that were made. Was X really the best choice for this task? What other options would have been available as alternatives for that task. . Lesson 3: Ask Questions (Even the “Obvious” ones) . If you don’t understand a concept that is being discussed, ask somebody that has more experience to explain it in another way. A lot of the time, these questions will lead to interesting discussions, unexplored knowledge gaps, and sometimes entirely new projects to work on. One great way to learn is to listen to people talk about things that they have learned and ask them questions. . Lesson 4: “I Don’t Know” . Not knowing an answer to a question is not something that should discourage you. On the contrary, knowing that you don’t know the answer to something is a really positive step in the right direction. This also gives you a great chance to improve your knowledge and learn something that you don’t currently know. . Lesson 5: Define the Problem . Whether you are working by yourself or with a group of people, focus on defining the problem before trying to come up with a solution. Use real-world examples when discussing what the problem actually is. Having different viewpoints is great when brainstorming, but make sure everybody is at least focusing on the same problem and not talking past each other. . Conclusion . These five lessons are just the start of what I learned from this book which is also full of entertaining stories and facts about Richard Feynman’s life. It’s well worth the read (or listen). .",
            "url": "https://blog.problemsolversguild.com/advice/2021/08/03/5_Lessons_From_Richard_Feynman.html",
            "relUrl": "/advice/2021/08/03/5_Lessons_From_Richard_Feynman.html",
            "date": " • Aug 3, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Python Native __slots__ Attribute Exploration",
            "content": "Introduction . While reading the python data model documentation, I came across something I hadn&#39;t seen before. __slots__ is an optional argument that allows users to &quot;explicitly declare data members&quot;. It is an interesting concept that I haven&#39;t seen utilized, but perhaps the reason is that not many people are aware it exists. I am going to explore this attribute that is available to see if it might provide value for my future projects. According to this blog post, __slots__ can significantly reduce the amount of ram required to create objects (40-50%!). Now let&#39;s dive in and figure out how it&#39;s used! . Python Version Check: 3.8.8 (default, Apr 13 2021, 19:58:26) [GCC 7.3.0] . Example #1: Typical Case . The first example we look at is the working example. we will have a class A1 with slots set to accept one var named var1. . class A1: __slots__ = [&#39;var1&#39;] def __init__(self, value_passed_through_here): self.var1 = value_passed_through_here . a1 = A1(1); a1.var1 . 1 . a has been created and everything is going well. Let&#39;s try adding another attribute. . a1.var2 = &quot;but can I set another var?&quot; . AttributeError Traceback (most recent call last) &lt;ipython-input-6-468e53fb51e0&gt; in &lt;module&gt; -&gt; 1 a1.var2 = &#34;but can I set another var?&#34; AttributeError: &#39;A1&#39; object has no attribute &#39;var2&#39; . a.var2 fails as expected because it isn&#39;t in the __slots__ list and __slots__ is read-only so it cannot be updated. . a1.__slots__ = [&#39;var2&#39;] . AttributeError Traceback (most recent call last) &lt;ipython-input-7-5213b9df3654&gt; in &lt;module&gt; -&gt; 1 a1.__slots__ = [&#39;var2&#39;] AttributeError: &#39;A1&#39; object attribute &#39;__slots__&#39; is read-only . a1.__slots__ = [&#39;var1&#39;, &#39;var2&#39;] . AttributeError Traceback (most recent call last) &lt;ipython-input-8-47cd4105f78c&gt; in &lt;module&gt; -&gt; 1 a1.__slots__ = [&#39;var1&#39;, &#39;var2&#39;] AttributeError: &#39;A1&#39; object attribute &#39;__slots__&#39; is read-only . When __slots__ is used, the __dict__ value is not set. Let&#39;s explore that a little further though. . a1.__dict__ . AttributeError Traceback (most recent call last) &lt;ipython-input-9-79636e85e82f&gt; in &lt;module&gt; -&gt; 1 a1.__dict__ AttributeError: &#39;A1&#39; object has no attribute &#39;__dict__&#39; . Example #2a: Exploring __dict__ Without Using __slots__ . class A2A: def __init__(self, value_passed_through_here): self.var1 = value_passed_through_here . a2a = A2A(1) . var1 shows up as expected when creating an object . a2a.__dict__ . {&#39;var1&#39;: 1} . a2a.var2 = &#39;adding a second thing&#39; . Adding a second variable adds it to the __dict__ as expected . a2a.__dict__ . {&#39;var1&#39;: 1, &#39;var2&#39;: &#39;adding a second thing&#39;} . Example #2b: Exploring __dict__ When Using __slots__ . class A2B: __slots__ = [&#39;var1&#39;, &#39;__dict__&#39;] def __init__(self, value_passed_through_here): self.var1 = value_passed_through_here . a2b = A2B(1) . a2b.__dict__ . {} . __dict__ exists now since we added it to __slots__, but it isn&#39;t populating the __dict__ like normal. We are still able to call the attribute var1 though. . a2b.var1 . 1 . a2b.var2 = &quot;test if we can add new variables now&quot; . Surprisingly, once we add __dict__ to the __slots__ list, adding a new var works. . a2b.__dict__ . {&#39;var2&#39;: &#39;test if we can add new variables now&#39;} . When we look at __dict__ after adding var2, there is an entry in __dict__ as well. . a2b.var2 . &#39;test if we can add new variables now&#39; . So if we enable __dict__ we are able to add new items to the __dict__, but __dict__ has to be explicitly defined to work. . Example 3: Inheritance . Now that we&#39;ve explored __slots__, let&#39;s see how it behaves when one class is inherited from another. . class A3: __slots__ = [&#39;a&#39;] def __init__(self, a): self.a = a . a3 = A3(1); a3.a . 1 . class B3A(A3): def __init__(self): self.a = 1 self.b = 2 . b3a = B3A() . b3a.__slots__ . [&#39;a&#39;] . b3a.__dict__ . {&#39;b&#39;: 2} . b3a.a . 1 . b3a.c = &quot;can I set c?&quot; . b3a.__dict__ . {&#39;b&#39;: 2, &#39;c&#39;: &#39;can I set c?&#39;} . So when B3A is inherited from A3, it uses the slots class, but it also reverts back in a lot of ways to a normal, non-slots, class again. The last thing I&#39;m going to try is actually setting a __slots__ in B3B just to see what happens . class B3B(A3): __slots__ = [&#39;a&#39;,&#39;b&#39;] def __init__(self): self.a = 1 self.b = 2 . b3b = B3B() . b3b.c = &quot;can I set this?&quot; . AttributeError Traceback (most recent call last) &lt;ipython-input-33-032b9cd26579&gt; in &lt;module&gt; -&gt; 1 b3b.c = &#34;can I set this?&#34; AttributeError: &#39;B3B&#39; object has no attribute &#39;c&#39; . So now that we have given B3B a __slots__ it is no longer behaves the same way that B3A is. . Here is what the official documentation says about this: . The action of a __slots__ declaration is not limited to the class where it is defined. __slots__ declared in parents are available in child classes. However, child subclasses will get a __dict__ and __weakref__ unless they also define __slots__ (which should only contain names of any additional slots). (https://docs.python.org/3.8/reference/datamodel.html#notes-on-using-slots 5th bullet) . Conclusion . __slots__ is an interesting concept that is built into Python that I hadn&#39;t heard of and wanted to explore. Hopefully this notebook is informative to other Python users as well. Things didn&#39;t always behave as I would have expected and that&#39;s part of the fun of actually testing out the code to see how things work in practice. .",
            "url": "https://blog.problemsolversguild.com/python/technical/exploration/2021/05/21/Python_slots_Exploration.html",
            "relUrl": "/python/technical/exploration/2021/05/21/Python_slots_Exploration.html",
            "date": " • May 21, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "5 Tips for a New CIO",
            "content": "Congratulations on your new role as CIO! As the CIO, your job is to remove roadblocks, provide a vision, and enable your business partners without spending more than is required to do so. Here are 5 tips that I have gathered as an employee watching multiple CIO transitions. . 1. Include Your Employees in your 5-year Plan . Every new CIO wants to create a vision, but one of the biggest mistakes that a new CIO can make is relying too much on external consultants or upper management to lay out this vision. Unfortunately, this type of top-down leadership strategy tends to lead to low adoption rates. While it is a great way to create a bold vision and great slide-deck, the resulting plan typically lacks adoption and leads to bland execution. . If you want buy-in to your vision, make sure you listen and absorb as you enter the new landscape and involve your employees when creating an execution plan. Finding a way to blend your 5-year plan with an execution roadmap that will get to that point is what will separate a great CIO from an average CIO. Give your staff a vision and operating rules and let them help you get to the end-state. Your job is to eliminate any roadblocks in the way of your vision and to make sure that everything that is being done today is in line with where you are trying to get to. . 2. Utilize Consultants Sparingly . Consultants are often used as scapegoats when a project or initiative isn’t on track, but consultants can be a great resource for an organization when utilized properly. The key with utilizing consultants is to make sure you are getting the experience that consultants can bring with them. Ask yourself and your team if the value being provided is worth the money being paid and also consider the technical debt that is being added by using a consultant. Always ensure that a full-time employee is paired with the consultant to learn from them and to take over the project before a consultant leaves. Any work done by consultants must include meaningful documentation and meaningful tests. Consultants can also be utilized to spin up an environment, but make sure an internal resource is involved as well because once the environment is up, you don’t want to rely on a consultant to handle ongoing support. . 3. Choose Your Advisors Carefully . Along with eliminating roadblocks, another role as a new CIO is to identify who you listen to around the organization. Identifying people you should take advice from can be daunting task but it is the only way to effectively guide the organization. One way to help identify who you should take advice from in an organization is to engage in brainstorming sessions. The idea behind these sessions is to help develop a vision that everybody buys into and to get everybody talking in the same conversations. Not everybody is comfortable with the same medium so it is a good idea to follow up on these brainstorming sessions with output items as well based on the conversations. This is a great way to figure out who is going to execute at a high level. Another important thing to keep in mind with this group of advisors is that they have to be willing to tell you their true opinion even if it’s not popular. You should be encouraging this to the entire staff, but really emphasize it with your advisor team. . Once these projects start to gain traction, go out of your way to recognize members of your team and solicit advice from them. Ask them specifically what is working well, what isn’t working well, and see if they have any ideas to help improve things going forward. Focus on the areas that they talk about and ask follow-up questions. Identify the strengths and weaknesses of yourself and your advisors. . 4. Learn From Outages Together . Issues can appear in many forms: missed SLAs, unresponsive servers, hanging jobs. Unplanned issues and outages should be viewed at as a huge opportunity to get your team a better understanding of how the overall technical landscape works at your company. Any time a member of your team is spending time fixing systems or chasing down issues, they aren’t working on the problems that are guiding the organization towards the 5-year vision. Understanding why system issues occur, how the issue was detected, how the solution was identified, and what can be done to reduce issues in the future either in severity or detection time are the key areas that should be focused on in this type of post-mortem. Be mindful that this doesn’t turn into a blame game. This is a collaborative team-building exercise and everybody should be growing from it. This is a great exercise that can help you understand how different pieces of the puzzle fit together and get everybody on the same page. . 5. Guide the Ship . As the CIO, you are responsible for crafting a draft of your organization’s roadmap and guiding your team. There will be times when you have to make changes to the 5-year vision, but it’s important to keep it in mind as you make decisions and enable your team to make decisions. Make yourself intentionally available to your organization. Keep an open door policy and advertise it. If you solicit advice and nobody responds, you probably need to re-consider how you are reaching out to your staff. Try meeting with small group, virtual suggestion boxes, or even one-on-one discussions until you find a medium that connects. Your actions will play a big part in setting the tone of the entire organization. .",
            "url": "https://blog.problemsolversguild.com/advice/2021/05/18/5_Tips_for_a_New_CIO.html",
            "relUrl": "/advice/2021/05/18/5_Tips_for_a_New_CIO.html",
            "date": " • May 18, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "fastai's Text DLs Exploration",
            "content": "Introduction . This post is an exploration into how to convert data into dls objects that can be used by fastai&#39;s learner. I was having issues creating a dls object that had the ability to show_batch and met my arbitrarily custom needs. So I set out to figure out how to create dls that worked well for my needs. . This blog post uses the Human Numbers dataset which is a dataset that counts sequentially from 1 to 9999 but in english text rather than numerical form. This is an interesting problem because there is quite a bit of repetition, but also new tokens and patterns being introduced regularly that a model will need to figure out. . My goal was to create a dls that would have X=1,2,3 and y=4. Over the course of this blog post, I will show ~4 ways to create dls that enable show_batch to work as expected. . from fastai.text.all import * . import fastai, fastcore . fastai.__version__,fastcore.__version__ . (&#39;2.3.1&#39;, &#39;1.3.20&#39;) . path = untar_data(URLs.HUMAN_NUMBERS) . First, I create a tokenizer, combine all of the text into a single string, and tokenize each word . tokenizer = Tokenizer(WordTokenizer()) . Testing the Tokenizer . tokenizer(&#39;one two three&#39;) . (#4) [&#39;xxbos&#39;,&#39;one&#39;,&#39;two&#39;,&#39;three&#39;] . tokenizer(&#39;one, two&#39;) . (#4) [&#39;xxbos&#39;,&#39;one&#39;,&#39;,&#39;,&#39;two&#39;] . Reading the train and validation files: . train - [1-8000] | valid = [8001-9999] | . train_txt = &#39;, &#39;.join(o.strip() for o in (path/&#39;train.txt&#39;).readlines()) valid_txt = &#39;, &#39;.join(o.strip() for o in (path/&#39;valid.txt&#39;).readlines()) . For this problem, I will create my own validation set. It will split close to the same as this, but by creating my own split, I don&#39;t have to do anything special when creating chunks around the train-&gt;validation split point . all_text = train_txt+valid_txt . all_text_tok = tokenizer(all_text) . all_text_tok . (#63094) [&#39;xxbos&#39;,&#39;one&#39;,&#39;,&#39;,&#39;two&#39;,&#39;,&#39;,&#39;three&#39;,&#39;,&#39;,&#39;four&#39;,&#39;,&#39;,&#39;five&#39;...] . Next, I take the tokenized text, count how many times each tokenizer occurs and create a vocab with that. . count=Counter(all_text_tok) vocab = make_vocab(count) . print(count) . Counter({&#39;,&#39;: 9996, &#39;hundred&#39;: 9000, &#39;thousand&#39;: 8999, &#39;one&#39;: 2900, &#39;two&#39;: 2900, &#39;three&#39;: 2900, &#39;four&#39;: 2900, &#39;five&#39;: 2900, &#39;six&#39;: 2900, &#39;seven&#39;: 2900, &#39;nine&#39;: 2899, &#39;eight&#39;: 2898, &#39;twenty&#39;: 1000, &#39;thirty&#39;: 1000, &#39;forty&#39;: 1000, &#39;fifty&#39;: 1000, &#39;sixty&#39;: 1000, &#39;seventy&#39;: 1000, &#39;eighty&#39;: 1000, &#39;ninety&#39;: 1000, &#39;ten&#39;: 100, &#39;eleven&#39;: 100, &#39;twelve&#39;: 100, &#39;thirteen&#39;: 100, &#39;fourteen&#39;: 100, &#39;fifteen&#39;: 100, &#39;sixteen&#39;: 100, &#39;seventeen&#39;: 100, &#39;eighteen&#39;: 100, &#39;nineteen&#39;: 100, &#39;xxbos&#39;: 1, &#39;nineeight&#39;: 1}) . print(vocab) . [&#39;xxunk&#39;, &#39;xxpad&#39;, &#39;xxbos&#39;, &#39;xxeos&#39;, &#39;xxfld&#39;, &#39;xxrep&#39;, &#39;xxwrep&#39;, &#39;xxup&#39;, &#39;xxmaj&#39;, &#39;,&#39;, &#39;hundred&#39;, &#39;thousand&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;, &#39;four&#39;, &#39;five&#39;, &#39;six&#39;, &#39;seven&#39;, &#39;nine&#39;, &#39;eight&#39;, &#39;twenty&#39;, &#39;thirty&#39;, &#39;forty&#39;, &#39;fifty&#39;, &#39;sixty&#39;, &#39;seventy&#39;, &#39;eighty&#39;, &#39;ninety&#39;, &#39;ten&#39;, &#39;eleven&#39;, &#39;twelve&#39;, &#39;thirteen&#39;, &#39;fourteen&#39;, &#39;fifteen&#39;, &#39;sixteen&#39;, &#39;seventeen&#39;, &#39;eighteen&#39;, &#39;nineteen&#39;, &#39;xxfake&#39;] . all_text_tok_chunked = list(chunked(all_text_tok, 11)) . all_text_tok_chunked = all_text_tok_chunked[:-1] . Next I create something that will get_x and get_y from the chunked data. . def get_x(o): return o[:10] def get_y(o): return [o[10]] if len(o) == 11 else [&#39;.&#39;] . print(f&quot;{get_x(all_text_tok_chunked[0])} -&gt; {get_y(all_text_tok_chunked[0])}&quot;) . [&#39;xxbos&#39;, &#39;one&#39;, &#39;,&#39;, &#39;two&#39;, &#39;,&#39;, &#39;three&#39;, &#39;,&#39;, &#39;four&#39;, &#39;,&#39;, &#39;five&#39;] -&gt; [&#39;,&#39;] . print(f&quot;{get_x(all_text_tok_chunked[-1])} -&gt; {get_y(all_text_tok_chunked[-1])}&quot;) . [&#39;nine&#39;, &#39;thousand&#39;, &#39;nine&#39;, &#39;hundred&#39;, &#39;ninety&#39;, &#39;seven&#39;, &#39;,&#39;, &#39;nine&#39;, &#39;thousand&#39;, &#39;nine&#39;] -&gt; [&#39;hundred&#39;] . TitledStringDecoder is a transform that only decodes and what it enables is the show_batch and show_results function to actually work properly. Without this, I had troubles getting those functions to work because TensorText doesn&#39;t have a proper show function or a truncate function. . class TitledStringDecoder(Transform): def decodes(self, o): return TitledStr(&#39; &#39;.join(o)) . All TitledStringDecoder really does is takes an array of text (&#39;one&#39;, &#39;two&#39;) and converts it into a space-concatenated string instead of type Titled str which knows how to display itself in a nice way. . TitledStr(&#39; &#39;.join([&#39;one&#39;, &#39;two&#39;])) . &#39;one two&#39; . tmp_ts = TitledStr(&#39; &#39;.join(all_text_tok[:10])) . tmp_ts . &#39;xxbos one , two , three , four , five&#39; . tmp_ts.truncate(3) . &#39;xxbos one ,&#39; . I create the splits based off the chunks. Putting 80% of the chunks into the training set and the last 20% in the validation set . splits = [L(range(int(len(all_text_tok_chunked)*0.8))), L(range(int(len(all_text_tok_chunked)*0.8),len(all_text_tok_chunked)))] . splits . [(#4588) [0,1,2,3,4,5,6,7,8,9...], (#1147) [4588,4589,4590,4591,4592,4593,4594,4595,4596,4597...]] . Now, let&#39;s test the transforms work properly . Numericalize(vocab)(TitledStringDecoder()(get_x(all_text_tok_chunked[0]))) . TensorText([ 2, 12, 9, 13, 9, 14, 9, 15, 9, 16]) . And confirm that they will work as a pipeline as well . pipeline = Pipeline([TitledStringDecoder, Numericalize(vocab)]) . get_x(pipeline(all_text_tok_chunked[0])) . TensorText([ 2, 12, 9, 13, 9, 14, 9, 15, 9, 16]) . pipeline_x = Pipeline([get_x, TitledStringDecoder, Numericalize(vocab)]) pipeline_y = Pipeline([get_y, TitledStringDecoder, Numericalize(vocab)]) . pipeline_y(all_text_tok_chunked[0]) . TensorText([9]) . Using Datasets + Dataloaders . dsets = Datasets(all_text_tok_chunked, tfms=[pipeline_x,pipeline_y], splits=splits) . dsets[0] . (TensorText([ 2, 12, 9, 13, 9, 14, 9, 15, 9, 16]), TensorText([9])) . dsets.show(dsets[0]) . xxbos one , two , three , four , five , . Next, we can create the dataloaders. This can be done with either DataLoaders.from_dsets(...) or dsets.dataloaders(...). Both methods are shown below. . dls = DataLoaders.from_dsets(dsets, shuffle=False, drop_last=True) . dls.show_batch() . text text_ . 0 xxbos one , two , three , four , five | , | . 1 six , seven , eight , nine , ten , | eleven | . 2 , twelve , thirteen , fourteen , fifteen , sixteen | , | . 3 seventeen , eighteen , nineteen , twenty , twenty one | , | . 4 twenty two , twenty three , twenty four , twenty | five | . 5 , twenty six , twenty seven , twenty eight , | twenty | . 6 nine , thirty , thirty one , thirty two , | thirty | . 7 three , thirty four , thirty five , thirty six | , | . 8 thirty seven , thirty eight , thirty nine , forty | , | . dls = dsets.dataloaders(bs=16, shuffle=False, drop_last=True) . dls.show_batch() . text text_ . 0 xxbos one , two , three , four , five | , | . 1 six , seven , eight , nine , ten , | eleven | . 2 , twelve , thirteen , fourteen , fifteen , sixteen | , | . 3 seventeen , eighteen , nineteen , twenty , twenty one | , | . 4 twenty two , twenty three , twenty four , twenty | five | . 5 , twenty six , twenty seven , twenty eight , | twenty | . 6 nine , thirty , thirty one , thirty two , | thirty | . 7 three , thirty four , thirty five , thirty six | , | . 8 thirty seven , thirty eight , thirty nine , forty | , | . Using Datasets -&gt; train TfmdDL + valid TfmdDL -&gt; dataloaders . Another way to get dls is to create TfmdDLs and pass those into DataLoaders. If you use DataLoader rather than TfmdDL, dls won&#39;t have a show_batch method available. . train_dl = TfmdDL(dsets.train, bs=16, drop_last=True) . valid_dl = TfmdDL(dsets.valid, bs=16, drop_last=True) . dls = DataLoaders(train_dl, valid_dl) . dls.show_batch() . text text_ . 0 xxbos one , two , three , four , five | , | . 1 six , seven , eight , nine , ten , | eleven | . 2 , twelve , thirteen , fourteen , fifteen , sixteen | , | . 3 seventeen , eighteen , nineteen , twenty , twenty one | , | . 4 twenty two , twenty three , twenty four , twenty | five | . 5 , twenty six , twenty seven , twenty eight , | twenty | . 6 nine , thirty , thirty one , thirty two , | thirty | . 7 three , thirty four , thirty five , thirty six | , | . 8 thirty seven , thirty eight , thirty nine , forty | , | . X,y = dls.one_batch() . Using DataBlock -&gt; datasets -&gt; dataloaders . Another way to get dataloaders is to use DataBlock. DataBlock wants to know what type of data will be passed which can be specified to blocks. It also wants a splitter and the functions to get_x and get_y . blocks = [TransformBlock(type_tfms=[TitledStringDecoder, Numericalize(vocab)]), # x piece TransformBlock(type_tfms=[TitledStringDecoder, Numericalize(vocab)])] # y piece . splits[-1] . (#1147) [4588,4589,4590,4591,4592,4593,4594,4595,4596,4597...] . IndexSplitter(splits[-1])(all_text_tok_chunked) . ((#4588) [0,1,2,3,4,5,6,7,8,9...], (#1147) [4588,4589,4590,4591,4592,4593,4594,4595,4596,4597...]) . dblock = DataBlock(blocks=blocks, splitter=IndexSplitter(splits[-1]), get_x=get_x, get_y=get_y) . With the dblock created, you can create a dset and then from the dset, you can create a dls similar to the one created above. . dsets_via_dblock = dblock.datasets(all_text_tok_chunked) . dsets_via_dblock . (#5735) [(TensorText([ 2, 12, 9, 13, 9, 14, 9, 15, 9, 16]), TensorText([9])),(TensorText([17, 9, 18, 9, 20, 9, 19, 9, 29, 9]), TensorText([30])),(TensorText([ 9, 31, 9, 32, 9, 33, 9, 34, 9, 35]), TensorText([9])),(TensorText([36, 9, 37, 9, 38, 9, 21, 9, 21, 12]), TensorText([9])),(TensorText([21, 13, 9, 21, 14, 9, 21, 15, 9, 21]), TensorText([16])),(TensorText([ 9, 21, 17, 9, 21, 18, 9, 21, 20, 9]), TensorText([21])),(TensorText([19, 9, 22, 9, 22, 12, 9, 22, 13, 9]), TensorText([22])),(TensorText([14, 9, 22, 15, 9, 22, 16, 9, 22, 17]), TensorText([9])),(TensorText([22, 18, 9, 22, 20, 9, 22, 19, 9, 23]), TensorText([9])),(TensorText([23, 12, 9, 23, 13, 9, 23, 14, 9, 23]), TensorText([15]))...] . dsets_via_dblock.show(dsets_via_dblock[0]) . xxbos one , two , three , four , five , . dls = dsets_via_dblock.dataloaders(bs=16,shuffle=False, drop_last=True) . dls.show_batch() . text text_ . 0 xxbos one , two , three , four , five | , | . 1 six , seven , eight , nine , ten , | eleven | . 2 , twelve , thirteen , fourteen , fifteen , sixteen | , | . 3 seventeen , eighteen , nineteen , twenty , twenty one | , | . 4 twenty two , twenty three , twenty four , twenty | five | . 5 , twenty six , twenty seven , twenty eight , | twenty | . 6 nine , thirty , thirty one , thirty two , | thirty | . 7 three , thirty four , thirty five , thirty six | , | . 8 thirty seven , thirty eight , thirty nine , forty | , | . Using DataBlock -&gt; dataloaders . Another option is to go directly from dblock to dls with dblock.dataloaders. Behind the scenes this is creating a dataset as well, but it can be a cleaner looking way to handle it if you always go from dblock -&gt; dls. . dls = dblock.dataloaders(all_text_tok_chunked, bs=16, shuffle=False, drop_last=True) . dls.show_batch() . text text_ . 0 xxbos one , two , three , four , five | , | . 1 six , seven , eight , nine , ten , | eleven | . 2 , twelve , thirteen , fourteen , fifteen , sixteen | , | . 3 seventeen , eighteen , nineteen , twenty , twenty one | , | . 4 twenty two , twenty three , twenty four , twenty | five | . 5 , twenty six , twenty seven , twenty eight , | twenty | . 6 nine , thirty , thirty one , thirty two , | thirty | . 7 three , thirty four , thirty five , thirty six | , | . 8 thirty seven , thirty eight , thirty nine , forty | , | . Conclusion . Creating dls is an extremely important capability when using fastai because that is what a learn expects to deal with all of the data. There are many different ways to get a dls object created so this isn&#39;t a comprehensive list, but at least shows a few ways to do the task. In a future blog post, I will be using this dls and exploring transformer models with it. Hopefully this will help others get their DLs working. . I&#39;d like to give a special thanks to Arto for helping me get things working properly and everybody in the fastai discord channel for dealing with my questions and for creating a great community to learn with every step of the way. . Useful Links . https://arampacha.github.io/thoughtsamples/fastai/pytorch/2021/01/02/transformer-lm-from-scratch.html . https://github.com/fastai/fastai/blob/ab154927696338741e59e0ffc4774777c4a9781c/nbs/39_tutorial.transformers.ipynb https://github.com/fastai/fastai/blob/ab154927696338741e59e0ffc4774777c4a9781c/dev_nbs/course/lesson7-human-numbers.ipynb .",
            "url": "https://blog.problemsolversguild.com/fastai/technical/exploration/2021/05/14/Text_DLs_Exploration.html",
            "relUrl": "/fastai/technical/exploration/2021/05/14/Text_DLs_Exploration.html",
            "date": " • May 14, 2021"
        }
        
    
  

  
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.problemsolversguild.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}