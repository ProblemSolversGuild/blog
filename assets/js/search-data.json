{
  
    
        "post0": {
            "title": "fastai's Text DLs Exploration",
            "content": "Introduction . This post is an exploration into how to convert data into dls objects that can be used by fastai&#39;s learner. I was having issues creating a dls object that had the ability to show_batch and met my arbitrarily custom needs. So I set out to figure out how to create dls that worked well for my needs. . This blog post uses the Human Numbers dataset which is a dataset that counts sequentially from 1 to 9999 but in english text rather than numerical form. This is an interesting problem because there is quite a bit of repetition, but also new tokens and patterns being introduced regularly that a model will need to figure out. . My goal was to create a dls that would have X=1,2,3 and y=4. Over the course of this blog post, I will show ~4 ways to create dls that enable show_batch to work as expected. . from fastai.text.all import * . path = untar_data(URLs.HUMAN_NUMBERS) . First, I create a tokenizer, combine all of the text into a single string, and tokenize each word . tokenizer = Tokenizer(WordTokenizer()) . Testing the Tokenizer . tokenizer(&#39;one two three&#39;) . (#4) [&#39;xxbos&#39;,&#39;one&#39;,&#39;two&#39;,&#39;three&#39;] . tokenizer(&#39;one, two&#39;) . (#4) [&#39;xxbos&#39;,&#39;one&#39;,&#39;,&#39;,&#39;two&#39;] . Reading the train and validation files: . train - [1-8000] | valid = [8001-9999] | . train_txt = &#39;, &#39;.join(o.strip() for o in (path/&#39;train.txt&#39;).readlines()) valid_txt = &#39;, &#39;.join(o.strip() for o in (path/&#39;valid.txt&#39;).readlines()) . For this problem, I will create my own validation set. It will split close to the same as this, but by creating my own split, I don&#39;t have to do anything special when creating chunks around the train-&gt;validation split point . all_text = train_txt+valid_txt . all_text_tok = tokenizer(all_text) . all_text_tok . (#63094) [&#39;xxbos&#39;,&#39;one&#39;,&#39;,&#39;,&#39;two&#39;,&#39;,&#39;,&#39;three&#39;,&#39;,&#39;,&#39;four&#39;,&#39;,&#39;,&#39;five&#39;...] . Next, I take the tokenized text, count how many times each tokenizer occurs and create a vocab with that. . count=Counter(all_text_tok) vocab = make_vocab(count) . print(count) . Counter({&#39;,&#39;: 9996, &#39;hundred&#39;: 9000, &#39;thousand&#39;: 8999, &#39;one&#39;: 2900, &#39;two&#39;: 2900, &#39;three&#39;: 2900, &#39;four&#39;: 2900, &#39;five&#39;: 2900, &#39;six&#39;: 2900, &#39;seven&#39;: 2900, &#39;nine&#39;: 2899, &#39;eight&#39;: 2898, &#39;twenty&#39;: 1000, &#39;thirty&#39;: 1000, &#39;forty&#39;: 1000, &#39;fifty&#39;: 1000, &#39;sixty&#39;: 1000, &#39;seventy&#39;: 1000, &#39;eighty&#39;: 1000, &#39;ninety&#39;: 1000, &#39;ten&#39;: 100, &#39;eleven&#39;: 100, &#39;twelve&#39;: 100, &#39;thirteen&#39;: 100, &#39;fourteen&#39;: 100, &#39;fifteen&#39;: 100, &#39;sixteen&#39;: 100, &#39;seventeen&#39;: 100, &#39;eighteen&#39;: 100, &#39;nineteen&#39;: 100, &#39;xxbos&#39;: 1, &#39;nineeight&#39;: 1}) . print(vocab) . [&#39;xxunk&#39;, &#39;xxpad&#39;, &#39;xxbos&#39;, &#39;xxeos&#39;, &#39;xxfld&#39;, &#39;xxrep&#39;, &#39;xxwrep&#39;, &#39;xxup&#39;, &#39;xxmaj&#39;, &#39;,&#39;, &#39;hundred&#39;, &#39;thousand&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;, &#39;four&#39;, &#39;five&#39;, &#39;six&#39;, &#39;seven&#39;, &#39;nine&#39;, &#39;eight&#39;, &#39;twenty&#39;, &#39;thirty&#39;, &#39;forty&#39;, &#39;fifty&#39;, &#39;sixty&#39;, &#39;seventy&#39;, &#39;eighty&#39;, &#39;ninety&#39;, &#39;ten&#39;, &#39;eleven&#39;, &#39;twelve&#39;, &#39;thirteen&#39;, &#39;fourteen&#39;, &#39;fifteen&#39;, &#39;sixteen&#39;, &#39;seventeen&#39;, &#39;eighteen&#39;, &#39;nineteen&#39;, &#39;xxfake&#39;] . all_text_tok_chunked = list(chunked(all_text_tok, 11)) . all_text_tok_chunked = all_text_tok_chunked[:-1] . Next I create something that will get_x and get_y from the chunked data. . def get_x(o): return o[:10] def get_y(o): return [o[10]] if len(o) == 11 else [&#39;.&#39;] . print(f&quot;{get_x(all_text_tok_chunked[0])} -&gt; {get_y(all_text_tok_chunked[0])}&quot;) . [&#39;xxbos&#39;, &#39;one&#39;, &#39;,&#39;, &#39;two&#39;, &#39;,&#39;, &#39;three&#39;, &#39;,&#39;, &#39;four&#39;, &#39;,&#39;, &#39;five&#39;] -&gt; [&#39;,&#39;] . print(f&quot;{get_x(all_text_tok_chunked[-1])} -&gt; {get_y(all_text_tok_chunked[-1])}&quot;) . [&#39;nine&#39;, &#39;thousand&#39;, &#39;nine&#39;, &#39;hundred&#39;, &#39;ninety&#39;, &#39;seven&#39;, &#39;,&#39;, &#39;nine&#39;, &#39;thousand&#39;, &#39;nine&#39;] -&gt; [&#39;hundred&#39;] . TitledStringDecoder is a transform that only decodes and what it enables is the show_batch and show_results function to actually work properly. Without this, I had troubles getting those functions to work because TensorText doesn&#39;t have a proper show function or a truncate function. . class TitledStringDecoder(Transform): def decodes(self, o): return TitledStr(&#39; &#39;.join(o)) . All TitledStringDecoder really does is takes an array of text (&#39;one&#39;, &#39;two&#39;) and converts it into a space-concatenated string instead of type Titled str which knows how to display itself in a nice way. . TitledStr(&#39; &#39;.join([&#39;one&#39;, &#39;two&#39;])) . &#39;one two&#39; . tmp_ts = TitledStr(&#39; &#39;.join(all_text_tok[:10])) . tmp_ts . &#39;xxbos one , two , three , four , five&#39; . tmp_ts.truncate(3) . &#39;xxbos one ,&#39; . I create the splits based off the chunks. Putting 80% of the chunks into the training set and the last 20% in the validation set . splits = [L(range(int(len(all_text_tok_chunked)*0.8))), L(range(int(len(all_text_tok_chunked)*0.8),len(all_text_tok_chunked)))] . splits . [(#4588) [0,1,2,3,4,5,6,7,8,9...], (#1147) [4588,4589,4590,4591,4592,4593,4594,4595,4596,4597...]] . Now, let&#39;s test the transforms work properly . Numericalize(vocab)(TitledStringDecoder()(get_x(all_text_tok_chunked[0]))) . TensorText([ 2, 12, 9, 13, 9, 14, 9, 15, 9, 16]) . And confirm that they will work as a pipeline as well . pipeline = Pipeline([TitledStringDecoder, Numericalize(vocab)]) . get_x(pipeline(all_text_tok_chunked[0])) . TensorText([ 2, 12, 9, 13, 9, 14, 9, 15, 9, 16]) . pipeline_x = Pipeline([get_x, TitledStringDecoder, Numericalize(vocab)]) pipeline_y = Pipeline([get_y, TitledStringDecoder, Numericalize(vocab)]) . pipeline_y(all_text_tok_chunked[0]) . TensorText([9]) . Using Datasets + Dataloaders . dsets = Datasets(all_text_tok_chunked, tfms=[pipeline_x,pipeline_y], splits=splits) . dsets[0] . (TensorText([ 2, 12, 9, 13, 9, 14, 9, 15, 9, 16]), TensorText([9])) . dsets.show(dsets[0]) . xxbos one , two , three , four , five , . Next, we can create the dataloaders. This can be done with either DataLoaders.from_dsets(...) or dsets.dataloaders(...). Both methods are shown below. . dls = DataLoaders.from_dsets(dsets, shuffle=False, drop_last=True) . dls.show_batch() . text text_ . 0 xxbos one , two , three , four , five | , | . 1 six , seven , eight , nine , ten , | eleven | . 2 , twelve , thirteen , fourteen , fifteen , sixteen | , | . 3 seventeen , eighteen , nineteen , twenty , twenty one | , | . 4 twenty two , twenty three , twenty four , twenty | five | . 5 , twenty six , twenty seven , twenty eight , | twenty | . 6 nine , thirty , thirty one , thirty two , | thirty | . 7 three , thirty four , thirty five , thirty six | , | . 8 thirty seven , thirty eight , thirty nine , forty | , | . dls = dsets.dataloaders(bs=16, shuffle=False, drop_last=True) . dls.show_batch() . text text_ . 0 xxbos one , two , three , four , five | , | . 1 six , seven , eight , nine , ten , | eleven | . 2 , twelve , thirteen , fourteen , fifteen , sixteen | , | . 3 seventeen , eighteen , nineteen , twenty , twenty one | , | . 4 twenty two , twenty three , twenty four , twenty | five | . 5 , twenty six , twenty seven , twenty eight , | twenty | . 6 nine , thirty , thirty one , thirty two , | thirty | . 7 three , thirty four , thirty five , thirty six | , | . 8 thirty seven , thirty eight , thirty nine , forty | , | . Using Datasets -&gt; train TfmdDL + valid TfmdDL -&gt; dataloaders . Another way to get dls is to create TfmdDLs and pass those into DataLoaders. If you use DataLoader rather than TfmdDL, dls won&#39;t have a show_batch method available. . train_dl = TfmdDL(dsets.train, bs=16, drop_last=True) . valid_dl = TfmdDL(dsets.valid, bs=16, drop_last=True) . dls = DataLoaders(train_dl, valid_dl) . dls.show_batch() . text text_ . 0 xxbos one , two , three , four , five | , | . 1 six , seven , eight , nine , ten , | eleven | . 2 , twelve , thirteen , fourteen , fifteen , sixteen | , | . 3 seventeen , eighteen , nineteen , twenty , twenty one | , | . 4 twenty two , twenty three , twenty four , twenty | five | . 5 , twenty six , twenty seven , twenty eight , | twenty | . 6 nine , thirty , thirty one , thirty two , | thirty | . 7 three , thirty four , thirty five , thirty six | , | . 8 thirty seven , thirty eight , thirty nine , forty | , | . X,y = dls.one_batch() . Using DataBlock -&gt; datasets -&gt; dataloaders . Another way to get dataloaders is to use DataBlock. DataBlock wants to know what type of data will be passed which can be specified to blocks. It also wants a splitter and the functions to get_x and get_y . blocks = [TransformBlock(type_tfms=[TitledStringDecoder, Numericalize(vocab)]), # x piece TransformBlock(type_tfms=[TitledStringDecoder, Numericalize(vocab)])] # y piece . splits[-1] . (#1147) [4588,4589,4590,4591,4592,4593,4594,4595,4596,4597...] . IndexSplitter(splits[-1])(all_text_tok_chunked) . ((#4588) [0,1,2,3,4,5,6,7,8,9...], (#1147) [4588,4589,4590,4591,4592,4593,4594,4595,4596,4597...]) . dblock = DataBlock(blocks=blocks, splitter=IndexSplitter(splits[-1]), get_x=get_x, get_y=get_y) . With the dblock created, you can create a dset and then from the dset, you can create a dls similar to the one created above. . dsets_via_dblock = dblock.datasets(all_text_tok_chunked) . dsets_via_dblock . (#5735) [(TensorText([ 2, 12, 9, 13, 9, 14, 9, 15, 9, 16]), TensorText([9])),(TensorText([17, 9, 18, 9, 20, 9, 19, 9, 29, 9]), TensorText([30])),(TensorText([ 9, 31, 9, 32, 9, 33, 9, 34, 9, 35]), TensorText([9])),(TensorText([36, 9, 37, 9, 38, 9, 21, 9, 21, 12]), TensorText([9])),(TensorText([21, 13, 9, 21, 14, 9, 21, 15, 9, 21]), TensorText([16])),(TensorText([ 9, 21, 17, 9, 21, 18, 9, 21, 20, 9]), TensorText([21])),(TensorText([19, 9, 22, 9, 22, 12, 9, 22, 13, 9]), TensorText([22])),(TensorText([14, 9, 22, 15, 9, 22, 16, 9, 22, 17]), TensorText([9])),(TensorText([22, 18, 9, 22, 20, 9, 22, 19, 9, 23]), TensorText([9])),(TensorText([23, 12, 9, 23, 13, 9, 23, 14, 9, 23]), TensorText([15]))...] . dsets_via_dblock.show(dsets_via_dblock[0]) . xxbos one , two , three , four , five , . dls = dsets_via_dblock.dataloaders(bs=16,shuffle=False, drop_last=True) . dls.show_batch() . text text_ . 0 xxbos one , two , three , four , five | , | . 1 six , seven , eight , nine , ten , | eleven | . 2 , twelve , thirteen , fourteen , fifteen , sixteen | , | . 3 seventeen , eighteen , nineteen , twenty , twenty one | , | . 4 twenty two , twenty three , twenty four , twenty | five | . 5 , twenty six , twenty seven , twenty eight , | twenty | . 6 nine , thirty , thirty one , thirty two , | thirty | . 7 three , thirty four , thirty five , thirty six | , | . 8 thirty seven , thirty eight , thirty nine , forty | , | . Using DataBlock -&gt; dataloaders . Another option is to go directly from dblock to dls with dblock.dataloaders. Behind the scenes this is creating a dataset as well, but it can be a cleaner looking way to handle it if you always go from dblock -&gt; dls. . dls = dblock.dataloaders(all_text_tok_chunked, bs=16, shuffle=False, drop_last=True) . dls.show_batch() . text text_ . 0 xxbos one , two , three , four , five | , | . 1 six , seven , eight , nine , ten , | eleven | . 2 , twelve , thirteen , fourteen , fifteen , sixteen | , | . 3 seventeen , eighteen , nineteen , twenty , twenty one | , | . 4 twenty two , twenty three , twenty four , twenty | five | . 5 , twenty six , twenty seven , twenty eight , | twenty | . 6 nine , thirty , thirty one , thirty two , | thirty | . 7 three , thirty four , thirty five , thirty six | , | . 8 thirty seven , thirty eight , thirty nine , forty | , | . Conclusion . Creating dls is an extremely important capability when using fastai because that is what a learn expects to deal with all of the data. There are many different ways to get a dls object created so this isn&#39;t a comprehensive list, but at least shows a few ways to do the task. In a future blog post, I will be using this dls and exploring transformer models with it. Hopefully this will help others get their DLs working. . I&#39;d like to give a special thanks to Arto for helping me get things working properly and everybody in the fastai discord channel for dealing with my questions and for creating a great community to learn with every step of the way. . Useful Links . https://arampacha.github.io/thoughtsamples/fastai/pytorch/2021/01/02/transformer-lm-from-scratch.html . https://github.com/fastai/fastai/blob/ab154927696338741e59e0ffc4774777c4a9781c/nbs/39_tutorial.transformers.ipynb https://github.com/fastai/fastai/blob/ab154927696338741e59e0ffc4774777c4a9781c/dev_nbs/course/lesson7-human-numbers.ipynb .",
            "url": "https://problemsolversguild.com/fastai/technical/exploration/2021/05/14/Text_DLs_Exploration.html",
            "relUrl": "/fastai/technical/exploration/2021/05/14/Text_DLs_Exploration.html",
            "date": " • May 14, 2021"
        }
        
    
  

  
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://problemsolversguild.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}