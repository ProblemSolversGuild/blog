[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Problem Solvers Guild Blog",
    "section": "",
    "text": "Genetic Algorithm Intro\n\n\n\n\n\n\n\ntechnical\n\n\nChatGPT\n\n\n\n\nA walkthrough of genetic algorithms generated using ChatGPT\n\n\n\n\n\n\nDec 21, 2022\n\n\nKevin Bird\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShould We Use ChatGPT?\n\n\n\n\n\n\n\nadvice\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2022\n\n\nKevin Bird\n\n\n\n\n\n\n  \n\n\n\n\nDiffEdit Paper Implementation\n\n\n\n\n\n\n\ntechnical\n\n\nresearch\n\n\n\n\nImplementing the DiffEdit Paper\n\n\n\n\n\n\nNov 2, 2022\n\n\nKevin Bird\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 Ways to Improve Your Quality Inspection Process\n\n\n\n\n\n\n\nManufacturing\n\n\nQuality\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2022\n\n\nKevin Bird\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Timm vs Torchvision Resnet18 Difference\n\n\n\n\n\n\n\nfastai\n\n\ntechnical\n\n\nexploration\n\n\n\n\nReviewing the impact batchnorm initialization has on non-pretrained model performance\n\n\n\n\n\n\nMay 2, 2022\n\n\nKevin Bird\n\n\n\n\n\n\n  \n\n\n\n\nPyTorch torch.lerp Exploration\n\n\n\n\n\n\n\npython\n\n\ntechnical\n\n\nexploration\n\n\n\n\nReviewing torch.lerp and its uses\n\n\n\n\n\n\nFeb 22, 2022\n\n\nKevin Bird\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA cron job that will run conda, run python script, and email when it fails\n\n\n\n\n\n\n\ntechnical\n\n\n\n\n\n\n\n\n\n\n\nFeb 17, 2022\n\n\nHiromi Suenaga\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate SSH key pair and using it\n\n\n\n\n\n\n\ntechnical\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2022\n\n\nHiromi Suenaga\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Function Default Argument Value\n\n\n\n\n\n\n\npython\n\n\ntechnical\n\n\nexploration\n\n\n\n\nExploring an unexpected Python behavior that could cause your program to behave inconsistently\n\n\n\n\n\n\nFeb 7, 2022\n\n\nKevin Bird\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Makefile to Create Git Hook\n\n\n\n\n\n\n\ntechnical\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2022\n\n\nHiromi Suenaga\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Git Hook to Clean Jupyter Notebook on Commit\n\n\n\n\n\n\n\ntechnical\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2022\n\n\nHiromi Suenaga\n\n\n\n\n\n\n  \n\n\n\n\nTexas Rate Inspector Overview\n\n\n\n\n\n\n\ntexas\n\n\nrate\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2021\n\n\nKevin Bird\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormalizing fastai Tabular Inputs\n\n\n\n\n\n\n\ntechnical\n\n\n\n\nExploring the effectiveness of normalization on tabular data\n\n\n\n\n\n\nAug 16, 2021\n\n\nKevin Bird\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 Lessons from Richard Feynman\n\n\n\n\n\n\n\nadvice\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2021\n\n\nKevin Bird\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Native slots Attribute Exploration\n\n\n\n\n\n\n\npython\n\n\ntechnical\n\n\nexploration\n\n\n\n\nDiving into Python to see what slots does and how it can be used\n\n\n\n\n\n\nMay 21, 2021\n\n\nKevin Bird\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 Tips for a New CIO\n\n\n\n\n\n\n\nadvice\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2021\n\n\nKevin Bird\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai’s Text DLs Exploration\n\n\n\n\n\n\n\nfastai\n\n\ntechnical\n\n\nexploration\n\n\n\n\nExploring different methods to create the dls object in fastai\n\n\n\n\n\n\nMay 14, 2021\n\n\nKevin Bird\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-12-08-Should_We_Use_ChatGPT.html",
    "href": "posts/2022-12-08-Should_We_Use_ChatGPT.html",
    "title": "Should We Use ChatGPT?",
    "section": "",
    "text": "Until a few days ago when I started trying out ChatGPT, I didn’t think this was a question I would have to contemplate for at least a few years, but, realistically, I thought this was 10+ years away from being a question for me. But recently, I have been contemplating the question of what types of problems should I allow a system like ChatGPT to solve? What do I feel comfortable using it for and what tasks should I do without assistance?\nI feel comfortable using ChatGPT to do any writing that I would otherwise be doing where the reason of the writing is not to show how well I am able to write, but is instead conveying information to an audience.\nHere is the logic I’m using to justify this stance:\n​ When I started learning math, I was not allowed to use a calculator. Every year, we were told that using a calculator was not allowed and this helped me learn the fundamentals of math at a good level. But at some point, the usage of a calculator became a convenience that unlocked higher-level math because we were no longer constrained by how long it took to multiply large values together or find a square root.\nMy thought is that the usage of ChatGPT is similar to the usage of a calculator and should therefore be considered a tool to unlock higher-levels of writing and ideation. It doesn’t remove my responsibility to verify claims and at the end of the day, anything that is published by me is my responsibility to validate and stand by.\nI do see why this type of tool is concerning from an educational perspective and I believe this will just require more in-person writing assessments to verify that a person is able to write at a certain level of competency. The other thing that I think will be interesting is to test students on the essay that they have written. For example, if a person hands in a 4 page essay on Van Gogh, are they able to answer questions about the essay after the fact? Each student’s test could be personalized based on what they handed in. At the end of the day, I believe the solution for this type of problem is going to vary depending on what exactly the reason for the essay is.\nI don’t believe that artificially throttling a system like ChatGPT is a net positive for society and I believe there are enough challenging problems to work on that we should be encouraging the utilization of this type of technology when possible to achieve results that would otherwise be hampered because we would be bogged down writing essays instead of thinking about the problems in the world that are waiting to be solved! I would love to hear your thoughts about this and am always open to changing my mind. Looking forward to hearing everybody’s perspective on this important topic!"
  },
  {
    "objectID": "posts/2022-02-17-Cron_job_failure_email.html",
    "href": "posts/2022-02-17-Cron_job_failure_email.html",
    "title": "A cron job that will run conda, run python script, and email when it fails",
    "section": "",
    "text": "I have searched high and low trying to create a scheduled task (cron job) that will: 1. Use a conda environment 2. Run a python script 3. Email if/when errors occur\nHere is my final script:\nIn the end, I realized it is much easier to write a bash script that handles emailing rather than configuring the cron to do so with MAILTO, output redirects, etc.\nIn the next post, we will show you how to automate the creation of this cron job and other required setup using Ansible."
  },
  {
    "objectID": "posts/2022-02-17-Cron_job_failure_email.html#my-web-searches-while-creating-the-above-script",
    "href": "posts/2022-02-17-Cron_job_failure_email.html#my-web-searches-while-creating-the-above-script",
    "title": "A cron job that will run conda, run python script, and email when it fails",
    "section": "My web searches while creating the above script",
    "text": "My web searches while creating the above script\n\nCron basics and MAILTO\nhttps://www.digitalocean.com/community/tutorials/how-to-use-cron-to-automate-tasks-ubuntu-1804\n\n\nHow to make cron send email only when script throws errors?\n20 6-10 * * 1-5 ~/job_failure_test.sh > ~/job_fail.log 2>&1 || mail -s \"Errors\" myemail@something.com < ~/job_fail.log\nhttps://unix.stackexchange.com/a/314647\n\n\nWhy can’t my cron job send emails?\nBefore you uninstall Postfix and install Sendmail (not because you prefer Sendmail, but because you are not receiving emails) check your spam/junk folder!. That would have saved me a couple hours of struggle.\nhttps://tecadmin.net/install-sendmail-on-ubuntu/\n\n\nHow to catch an exception thrown by a python script in shell script (so I can do something like sending an email)\n./script.py  || {\n    # Python script script.py failed. Do something\n} \nhttps://stackoverflow.com/a/24208293/6999874\n\n\nHow can I use conda in bash script? I get an error: CommandNotFoundError: Your shell has not been properly configured to use conda activate. To initialize your shell, run $ conda init <SHELL_NAME>\nConda puts something like this to your .bashrc or .zshrc when you install it. Just put this simplified version in the script before conda activate:\nsource /opt/conda/etc/profile.d/conda.sh\nhttps://unix.stackexchange.com/a/577347\nhttps://askubuntu.com/a/1218657"
  },
  {
    "objectID": "posts/2022-12-21-genetic_algorithm.html",
    "href": "posts/2022-12-21-genetic_algorithm.html",
    "title": "Genetic Algorithm Intro",
    "section": "",
    "text": "import random\nfrom typing import List, Callable\nfrom copy import deepcopy"
  },
  {
    "objectID": "posts/2022-12-21-genetic_algorithm.html#introduction",
    "href": "posts/2022-12-21-genetic_algorithm.html#introduction",
    "title": "Genetic Algorithm Intro",
    "section": "Introduction",
    "text": "Introduction\nHave you ever faced a problem that seemed impossible to solve? Perhaps you were trying to optimize a manufacturing process, find the shortest path between two points, or design a new product. These types of problems can be difficult to solve using traditional algorithms, especially when the solution space is large or the problem is unstructured. This is where genetic algorithms come in.\nA genetic algorithm is a heuristic optimization technique that is inspired by the process of natural evolution. It is a metaheuristic, which means that it is a high-level algorithm that is used to guide other algorithms.\nGenetic algorithms are used to solve a wide range of problems, from simple optimization problems to complex machine learning tasks. They are particularly useful for solving problems that have a large solution space, such as finding the shortest path in a graph or optimizing the parameters of a machine learning model.\nSo, how do genetic algorithms work? In a nutshell, they generate a population of candidate solutions, evaluate the fitness of each individual, select the fittest individuals for reproduction, generate offspring through crossover and mutation, and replace the weakest individuals. This process is repeated until a satisfactory solution is found or a predetermined number of iterations has been reached.\nIn this article, we will look at the key components of a genetic algorithm and see how they work together to solve complex problems. We will also explore some of the key considerations when designing a genetic algorithm, such as representation, fitness function, selection, crossover, and mutation."
  },
  {
    "objectID": "posts/2022-12-21-genetic_algorithm.html#initialize-the-population",
    "href": "posts/2022-12-21-genetic_algorithm.html#initialize-the-population",
    "title": "Genetic Algorithm Intro",
    "section": "Initialize the population",
    "text": "Initialize the population\nA population of candidate solutions (also known as individuals or chromosomes) is generated randomly. Each individual represents a potential solution to the problem. For example, if the problem is to find the maximum value of a mathematical function, each individual in the population could be a set of input values for the function.\nThe size of the population and the representation of the solutions depend on the problem being solved. For example, if the problem is a simple mathematical optimization problem, each individual could be a single floating-point number. If the problem is more complex, such as finding the shortest path in a graph, each individual could be a list of integers or a string of characters.\n\ninitialize_population in code\ninitialize_population takes two integer parameters: solution_size, which represents the size of a single solution (i.e., the number of genes), and population_size, which represents the size of the population (i.e., the number of solutions). It returns a list of lists containing random integers, representing the initial population.\n\ndef initialize_population(solution_size: int, population_size: int) -> List[List[int]]:\n    \"\"\"\n    Generates a random initial population.\n\n    Parameters:\n    - solution_size: the size of a single solution (number of genes)\n    - population_size: the size of the population (number of solutions)\n\n    Returns:\n    - A list of lists containing random integers, representing the initial population.\n    \"\"\"\n    return [[random.randint(0, 1) for _ in range(solution_size)] for _ in range(population_size)]\n\n\ndef test_initialize_population(solution_size, population_size):\n    population = initialize_population(solution_size, population_size)\n\n    # Check that the returned value is a list of lists\n    assert isinstance(population, list)\n    assert all(isinstance(x, list) for x in population)\n\n    # Check that the list contains 5 lists\n    assert len(population) == population_size\n\n    # Check that each list contains 10 integers\n    assert all(len(x) == solution_size for x in population)\n\n    # Check that each integer is either 0 or 1\n    assert all(all(x == 0 or x == 1 for x in solution) for solution in population)\n\n\ntest_initialize_population(5,10)\ntest_initialize_population(10,5)\ntest_initialize_population(0,5)\ntest_initialize_population(5,0)\n\n\nsolution_size = 5\npopulation_size = 10\n\npopulation = initialize_population(solution_size, population_size)"
  },
  {
    "objectID": "posts/2022-12-21-genetic_algorithm.html#evaluate-the-fitness-of-each-individual",
    "href": "posts/2022-12-21-genetic_algorithm.html#evaluate-the-fitness-of-each-individual",
    "title": "Genetic Algorithm Intro",
    "section": "Evaluate the fitness of each individual",
    "text": "Evaluate the fitness of each individual\nThe fitness of each individual is evaluated using a fitness function. This function takes an individual as input and returns a score that reflects the quality of the solution represented by that individual.\nThe fitness function should be designed to reflect the goals of the problem. For example, if the problem is to maximize a mathematical function, the fitness function could return the value of the function for a given set of input values. If the problem is to find the shortest path in a graph, the fitness function could return the length of the path.\n\nfitness in code\nfitness takes a single parameter, solution, which is a list of integers representing a solution. It returns an integer representing the fitness of the solution. In this particular implementation, the fitness is calculated by counting the number of 1s in the solution.\n\ndef fitness(solution: List[int]) -> int:\n    \"\"\"\n    Calculates the fitness of a solution.\n    \n    Parameters:\n    - solution: a list of integers representing a solution\n    \n    Returns:\n    - An integer representing the fitness of the solution.\n    \"\"\"\n    # Calculate the fitness of the solution\n    # For example, you could count the number of 1s in the solution\n    return solution.count(1)\n\n\n# Test with a solution containing all 0s\nsolution = [0, 0, 0, 0, 0]\nassert fitness(solution) == 0\n\n# Test with a solution containing all 1s\nsolution = [1, 1, 1, 1, 1]\nassert fitness(solution) == 5\n\n# Test with a solution containing a mix of 0s and 1s\nsolution = [0, 1, 1, 0, 0]\nassert fitness(solution) == 2\n\n# Test with a solution containing no 1s\nsolution = [0, 0, 0, 'blah', 0]\nassert fitness(solution) == 0"
  },
  {
    "objectID": "posts/2022-12-21-genetic_algorithm.html#select-the-fittest-individuals",
    "href": "posts/2022-12-21-genetic_algorithm.html#select-the-fittest-individuals",
    "title": "Genetic Algorithm Intro",
    "section": "Select the fittest individuals",
    "text": "Select the fittest individuals\nThe fittest individuals in the population are selected for reproduction based on their fitness scores. There are many ways to select the fittest individuals, such as using a ranking selection, tournament selection, or roulette wheel selection.\nIn ranking selection, the individuals are ranked based on their fitness and a probability of selection is assigned to each individual based on its rank. The fittest individuals have a higher probability of being selected.\nIn tournament selection, a group of individuals is selected at random and the fittest individual from the group is chosen for reproduction.\nIn roulette wheel selection, the probability of selection is proportional to the fitness of the individual. The higher the fitness, the higher the probability of selection.\n\nselection in code\nselection takes two parameters: population, which is a list of lists representing the population, and fitness_fn, which is a function that calculates the fitness of a solution. It returns a list of the fittest individuals in the population.\nIt does this by sorting the population by fitness using the fitness_fn function as the key, and then selecting the top half of the population (i.e., the individuals with the highest fitness).\n\ndef selection(population: List[List[int]], fitness_fn: Callable[[List[int]], int]) -> List[List[int]]:\n    \"\"\"\n    Selects the fittest individuals from the population.\n    \n    Parameters:\n    - population: a list of lists representing the population\n    - fitness_fn: a function that calculates the fitness of a solution\n    \n    Returns:\n    - A list of the fittest individuals in the population.\n    \"\"\"\n    # Sort the population by fitness\n    sorted_population = sorted(population, key=fitness_fn, reverse=True)\n    # Select the top individuals\n    return sorted_population[:int(len(sorted_population) / 2)]\n\n\n# Test with a population of size 10\npopulation = [[1, 1, 1, 1, 0], \n              [1, 1, 1, 0, 0], \n              [0, 1, 0, 0, 0], \n              [0, 1, 0, 0, 1], \n              [0, 0, 1, 0, 0], \n              [0, 0, 0, 0, 1], \n              [0, 0, 0, 0, 1],\n              [1, 1, 0, 1, 0], \n              [1, 0, 1, 0, 1], \n              [0, 1, 1, 1, 0]\n             ]\n\nfittest = selection(population, fitness)\n\n# Check that the returned value is a list of lists\nassert isinstance(fittest, list)\nassert all(isinstance(x, list) for x in fittest)\n\n# Check that the list contains 5 lists\nassert len(fittest) == 5\n\n# Check that the first list has 3 1s\nassert fitness(fittest[0]) == 4\n\n# Check that the last list has 2 1s\nassert fitness(fittest[-1]) == 3\n\n\nfittest\n\n[[1, 1, 1, 1, 0],\n [1, 1, 1, 0, 0],\n [1, 1, 0, 1, 0],\n [1, 0, 1, 0, 1],\n [0, 1, 1, 1, 0]]"
  },
  {
    "objectID": "posts/2022-12-21-genetic_algorithm.html#generate-offspring",
    "href": "posts/2022-12-21-genetic_algorithm.html#generate-offspring",
    "title": "Genetic Algorithm Intro",
    "section": "Generate offspring",
    "text": "Generate offspring\nOffspring are generated through crossover and mutation. Crossover involves combining the genetic material of two parents to produce offspring. There are many ways to perform crossover, such as single-point crossover, two-point crossover, and uniform crossover.\nIn single-point crossover, a random crossover point is chosen and the offspring inherit the genetic material from one parent before the crossover point and from the other parent after the crossover point\nIn two-point crossover, two crossover points are chosen and the offspring inherit the genetic material from one parent in the first segment, from the other parent in the second segment, and so on.\nIn uniform crossover, each gene in the offspring has an equal probability of inheriting the corresponding gene from either parent.\nMutation involves randomly flipping the value of one or more genes in the offspring. The mutation rate determines the probability that a gene will be mutated. A high mutation rate can introduce more diversity into the population, but it can also decrease the quality of the solutions.\n\ncrossover in code\ncrossover takes two parameters: parent1, which is a list of integers representing the first parent, and parent2, which is a list of integers representing the second parent. It returns a list of integers representing the offspring, generated through crossover.\nIt does this by choosing a random crossover point and combining the two parents by taking the first part of parent1 and the second part of parent2, or vice versa.\n\ndef crossover(parent1: List[int], parent2: List[int]) -> List[int]:\n    \"\"\"\n    Generates offspring through crossover.\n    \n    Parameters:\n    - parent1: a list of integers representing the first parent\n    - parent2: a list of integers representing the second parent\n    \n    Returns:\n    - A list of integers representing the offspring.\n    \"\"\"\n    # Choose a crossover point\n    crossover_point = random.randint(1, len(parent1) - 1)\n    # Generate offspring\n    offspring = parent1[:crossover_point] + parent2[crossover_point:]\n    return offspring\n\n\noffspring = crossover([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\n# Check that the returned value is a list\nassert isinstance(offspring, list)\n\n# Check that the list has length 5\nassert len(offspring) == 10\n# Check that the offspring contains elements from both parents\nassert 0 in offspring and 1 in offspring\n\nassert offspring[0] == 0\nassert offspring[-1] == 1\n\n\noffspring\n\n[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n\n\n\n\nmutate in code\nmutate takes two parameters: populations, which is a list of lists of integers representing the population, and mutation_rate, which is a float representing the probability of a mutation occurring. It returns the modified population.\nIt does this by iterating over each gene in each individual in the population and randomly flipping its value with a probability of mutation_rate.\n\ndef mutate(populations: List[List[int]], mutation_rate: float) -> List[List[int]]:\n    \"\"\"\n    Introduces mutations into the population.\n    \n    Parameters:\n    - populations: a list of lists of integers representing the population\n    - mutation_rate: a float representing the probability of a mutation occurring\n    \n    Returns:\n    - The modified population.\n    \"\"\"\n    populations = deepcopy(populations)\n    for population in populations:\n        for i in range(len(population)):\n            if random.random() < mutation_rate:\n                # Flip the value of a random gene\n                population[i] = 1 - population[i]\n    return populations\n\n\n# Test with population = [[0, 0, 0, 0, 0], [1, 1, 1, 1, 1]] and mutation_rate = 0.5\npopulations = [[0, 0, 0, 0, 0], [1, 1, 1, 1, 1]]\nmutation_rate = 0.5\nresult = mutate(populations, mutation_rate)\n\n# Check that the returned value is a list of lists\nassert isinstance(result, list)\nassert all(isinstance(i, list) for i in result)\n\n# Check that the length of each inner list is 5\nassert all(len(i) == 5 for i in result)\n\n# Check that the result contains at least one mutation\nassert any(i != j for i, j in zip(result[0], populations[0])) or any(i != j for i, j in zip(result[1], populations[1]))\n\n\n# Test with population = [[0, 0, 0, 0, 0], [1, 1, 1, 1, 1]] and mutation_rate = 0.0\npopulations = [[0, 0, 0, 0, 0], [1, 1, 1, 1, 1]]\nmutation_rate = 0\nresult = mutate(populations, mutation_rate)\n\n# Check that the returned value is a list of lists\nassert isinstance(result, list)\nassert all(isinstance(i, list) for i in result)\n\n# Check that the length of each inner list is 5\nassert all(len(i) == 5 for i in result)\n\n# Check that the result is unchanged\nassert result == populations"
  },
  {
    "objectID": "posts/2022-12-21-genetic_algorithm.html#replace-the-weakest-individuals",
    "href": "posts/2022-12-21-genetic_algorithm.html#replace-the-weakest-individuals",
    "title": "Genetic Algorithm Intro",
    "section": "Replace the weakest individuals",
    "text": "Replace the weakest individuals\nThe weakest individuals in the population are replaced with the offspring. This step is known as survival of the fittest. The size of the population remains constant and the fittest individuals are preserved while the weakest are replaced."
  },
  {
    "objectID": "posts/2022-12-21-genetic_algorithm.html#repeat-the-process",
    "href": "posts/2022-12-21-genetic_algorithm.html#repeat-the-process",
    "title": "Genetic Algorithm Intro",
    "section": "Repeat the process",
    "text": "Repeat the process\nThe process is repeated until a satisfactory solution is found or a predetermined number of iterations has been reached. The genetic algorithm continues to evolve the population until a satisfactory solution is found or the maximum number of iterations is reached.\n\ngenetic_algorithm in code\n\n# Run the genetic algorithm\ndef genetic_algorithm(solution_size, population_size, fitness_fn, mutation_rate):\n    # Initialize the population\n    population = initialize_population(solution_size, population_size)\n    print(selection(population, fitness_fn)[0])\n    # Iterate until a solution is found\n    while True:\n        # Select the fittest individuals\n        fittest = selection(population, fitness_fn)\n        # Generate offspring through crossover\n        offspring = crossover(fittest[0], fittest[1])\n        # Introduce mutations\n        population = mutate(population, mutation_rate)\n        # Add the offspring to the population\n        population.append(offspring)\n        # Check if a solution has been found\n        if fitness_fn(offspring) == len(offspring):\n            return offspring\n\n\n# Test the genetic algorithm\nresult = genetic_algorithm(100, 100, fitness, 0.001)\nprint(result)\n\n[0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
  },
  {
    "objectID": "posts/2022-12-21-genetic_algorithm.html#choosing-an-appropriate-representation-in-a-genetic-algorithm",
    "href": "posts/2022-12-21-genetic_algorithm.html#choosing-an-appropriate-representation-in-a-genetic-algorithm",
    "title": "Genetic Algorithm Intro",
    "section": "Choosing an appropriate representation in a genetic algorithm",
    "text": "Choosing an appropriate representation in a genetic algorithm\nRepresentation is an important aspect of genetic algorithms because it determines how the solutions are encoded and how they can be manipulated. In order to design a genetic algorithm, it is necessary to choose an appropriate representation that is well-suited to the problem being solved.\nThere are many different ways to represent solutions in a genetic algorithm. For example, solutions can be represented as binary strings, floating-point numbers, permutations, or even images. The choice of representation depends on the problem being solved and the available resources.\nOne common representation in genetic algorithms is the binary string, which is a sequence of 0s and 1s. This representation is well-suited to problems that have a discrete set of solutions, such as finding the shortest path in a graph.\nAnother common representation is the floating-point number, which is a real number represented in binary format. This representation is well-suited to continuous optimization problems, such as finding the maximum value of a mathematical function.\nPermutations are also a common representation in genetic algorithms. They are used to solve problems that involve rearranging a set of items, such as scheduling tasks or arranging objects in a warehouse.\nFinally, images can also be used as a representation in genetic algorithms. This is particularly useful for tasks such as image generation or image recognition.\nWhen choosing a representation, it is important to consider the size of the solution space, the complexity of the problem, and the available resources. The representation should be able to capture the key features of the problem and allow for efficient manipulation of the solutions."
  },
  {
    "objectID": "posts/2022-12-21-genetic_algorithm.html#conclusion",
    "href": "posts/2022-12-21-genetic_algorithm.html#conclusion",
    "title": "Genetic Algorithm Intro",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we have explored a simple genetic algorithm that demonstrates the key components of this optimization technique. We have seen how the genetic algorithm generates a population of candidate solutions, evaluates the fitness of each individual, selects the fittest individuals for reproduction, generates offspring through crossover and mutation, and replaces the weakest individuals. This process is repeated until a satisfactory solution is found or a predetermined number of iterations has been reached.\nWe have also looked at some of the considerations when designing a genetic algorithm, such as the representation of the solutions, the fitness function, the selection function, the crossover function, and the mutation function. These components are important for guiding the search for solutions and improving the quality of the solutions found by the genetic algorithm.\nOverall, genetic algorithms are a powerful optimization technique that can be used to solve a wide range of problems, particularly those with large solution spaces or unstructured problems. By understanding the key components and how they work together, you can design and implement a genetic algorithm that can find high-quality solutions to complex problems."
  },
  {
    "objectID": "posts/2022-05-02-zero_init_last_resnetperformance.html",
    "href": "posts/2022-05-02-zero_init_last_resnetperformance.html",
    "title": "Exploring Timm vs Torchvision Resnet18 Difference",
    "section": "",
    "text": "I was recently doing some experiments on imagenette testing out the new PolyLoss paper and I noticed that when I was running my baseline model using resnet18 from torchvision, I was consistently getting ~78% after 5 epochs, but that same baseline model was around 72% consistently when I used resnet18 from timm instead. For this post, I’m going to stick to one run per model, but this really should use at least 5 runs to make sure the issue isn’t a poorly seeded run.\nThe first thing I am going to do is import fastai’s vision module and download imagenette which is a dataset to test techniques that is lighter than imagenet."
  },
  {
    "objectID": "posts/2022-05-02-zero_init_last_resnetperformance.html#timm-vanilla",
    "href": "posts/2022-05-02-zero_init_last_resnetperformance.html#timm-vanilla",
    "title": "Exploring Timm vs Torchvision Resnet18 Difference",
    "section": "Timm Vanilla",
    "text": "Timm Vanilla\nNow, let’s train a model using timm’s resnet18 architecture. The newest version of fastai makes this really slick by allowing a user to pass resnet18 in as a string. This is a signal to fastai’s vision_learner to look for this in the timm model library.\n\ndls = ImageDataLoaders.from_folder(data_path, valid='val', item_tfms=Resize(256))\nlearn_timm = vision_learner(dls, 'resnet18', pretrained=False, metrics=accuracy)\nlearn_timm.fit_one_cycle(5)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      2.412845\n      2.793077\n      0.292484\n      00:25\n    \n    \n      1\n      1.847957\n      2.888684\n      0.314904\n      00:24\n    \n    \n      2\n      1.413061\n      1.247371\n      0.608662\n      00:24\n    \n    \n      3\n      1.124863\n      1.019361\n      0.675414\n      00:24\n    \n    \n      4\n      0.963823\n      0.848421\n      0.726879\n      00:24\n    \n  \n\n\n\n72.7% accuracy when we run timm’s resnet18 for 5 epochs."
  },
  {
    "objectID": "posts/2022-05-02-zero_init_last_resnetperformance.html#torchvision-vanilla",
    "href": "posts/2022-05-02-zero_init_last_resnetperformance.html#torchvision-vanilla",
    "title": "Exploring Timm vs Torchvision Resnet18 Difference",
    "section": "TorchVision Vanilla",
    "text": "TorchVision Vanilla\nNow let’s do the same training but instead, let’s use torchvision’s resnet18 which can be called by passing resnet18 to the vision_learner (not the string version).\n\ndls = ImageDataLoaders.from_folder(data_path, valid='val', item_tfms=Resize(256))\nlearn_torchvision = vision_learner(dls, resnet18, pretrained=False, metrics=accuracy)\nlearn_torchvision.fit_one_cycle(5)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      2.369731\n      3.082903\n      0.196178\n      00:24\n    \n    \n      1\n      1.618597\n      2.213618\n      0.449936\n      00:24\n    \n    \n      2\n      1.195862\n      2.348811\n      0.519236\n      00:24\n    \n    \n      3\n      0.949387\n      0.815663\n      0.735541\n      00:25\n    \n    \n      4\n      0.721939\n      0.674278\n      0.781656\n      00:24\n    \n  \n\n\n\n78.2% accuracy when we run torchvision’s resnet18 for 5 epochs."
  },
  {
    "objectID": "posts/2022-05-02-zero_init_last_resnetperformance.html#what-is-causing-this-difference",
    "href": "posts/2022-05-02-zero_init_last_resnetperformance.html#what-is-causing-this-difference",
    "title": "Exploring Timm vs Torchvision Resnet18 Difference",
    "section": "What is causing this difference?",
    "text": "What is causing this difference?\nThis difference was pointed out in the fastai discord channel and Ross Wightman, the creator of timm had some ideas. The first was to try running this experiment multiple times due to variance. This was easy enough to test so I went ahead and saw a similar pattern for the next 5 runs. The next thing he mentioned was something called zero_init which I hadn’t heard of before. The argument may be referred to as zero_init_residual or zero_init_last_bn. The timm library defaults this variable to True and torchvision defaults this to False. First, let’s confirm that this difference fixes our discrepancy between timm and torchvision, then I’ll explain what it is doing, and lastly I will explain which is the better option."
  },
  {
    "objectID": "posts/2022-05-02-zero_init_last_resnetperformance.html#timm-zero_init_last_bnfalse",
    "href": "posts/2022-05-02-zero_init_last_resnetperformance.html#timm-zero_init_last_bnfalse",
    "title": "Exploring Timm vs Torchvision Resnet18 Difference",
    "section": "Timm zero_init_last_bn=False",
    "text": "Timm zero_init_last_bn=False\n\ndls = ImageDataLoaders.from_folder(data_path, valid='val', item_tfms=Resize(256))\nlearn_timm_no_zero = vision_learner(dls, 'resnet18', pretrained=False, metrics=accuracy, zero_init_last_bn=False)\n\nlearn_timm_no_zero.fit_one_cycle(5)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      2.349849\n      2.469171\n      0.282803\n      00:24\n    \n    \n      1\n      1.621092\n      3.614518\n      0.263949\n      00:24\n    \n    \n      2\n      1.222227\n      1.627461\n      0.538599\n      00:24\n    \n    \n      3\n      0.960997\n      0.816477\n      0.741401\n      00:24\n    \n    \n      4\n      0.718745\n      0.682136\n      0.787516\n      00:24"
  },
  {
    "objectID": "posts/2022-05-02-zero_init_last_resnetperformance.html#torchvision-zero_init_residualtrue",
    "href": "posts/2022-05-02-zero_init_last_resnetperformance.html#torchvision-zero_init_residualtrue",
    "title": "Exploring Timm vs Torchvision Resnet18 Difference",
    "section": "TorchVision zero_init_residual=True",
    "text": "TorchVision zero_init_residual=True\n\ndls = ImageDataLoaders.from_folder(data_path, valid='val', item_tfms=Resize(256))\nlearn_tv_zero_bn = vision_learner(dls, partial(resnet18, zero_init_residual=True), pretrained=False, metrics=accuracy)\nlearn_tv_zero_bn.fit_one_cycle(5)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      2.453036\n      3.386052\n      0.208408\n      00:24\n    \n    \n      1\n      1.851731\n      2.367089\n      0.373758\n      00:24\n    \n    \n      2\n      1.414447\n      1.292271\n      0.581401\n      00:24\n    \n    \n      3\n      1.109155\n      0.982513\n      0.684331\n      00:24\n    \n    \n      4\n      0.927408\n      0.845072\n      0.729682\n      00:24\n    \n  \n\n\n\nSo what is this option that is swinging our accuracy by 5%? It is an option that says whether we should start the second batchnorm layer (bn2) of our resnet model at 0 or at 1.\n\ndls = ImageDataLoaders.from_folder(data_path, valid='val', item_tfms=Resize(256))\nlearn_timm = vision_learner(dls, 'resnet18', pretrained=False, metrics=accuracy)\n\n\nlearn_timm.model[0].model.layer1\n\nSequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act2): ReLU(inplace=True)\n  )\n  (1): BasicBlock(\n    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act2): ReLU(inplace=True)\n  )\n)\n\n\n\nlearn_timm.model[0].model.layer1[0].bn2.weight\n\nParameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       requires_grad=True)\n\n\n\ndls = ImageDataLoaders.from_folder(data_path, valid='val', item_tfms=Resize(256))\nlearn_torchvision = vision_learner(dls, resnet18, pretrained=False, metrics=accuracy)\n\n\nlearn_torchvision.model[0][4]\n\nSequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (1): BasicBlock(\n    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n)\n\n\n\nlearn_torchvision.model[0][4][0].bn2.weight\n\nParameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n\n\nThe other thing that may catch your eye here is that timm has a second relu, but this is actually just a difference in torchvision using the same relu twice in the forward function so it doesn’t quite show the full picture."
  },
  {
    "objectID": "posts/2022-05-02-zero_init_last_resnetperformance.html#conclusion",
    "href": "posts/2022-05-02-zero_init_last_resnetperformance.html#conclusion",
    "title": "Exploring Timm vs Torchvision Resnet18 Difference",
    "section": "Conclusion",
    "text": "Conclusion\nAfter changing these two defaults, I am now able to see similar performance where TorchVision performs at a lower accuracy level and timm performs at a higher accuracy level. So clearly, setting this option to False is best right? Not so fast. Ross says that while this option will perform better on short epoch runs when this option is set to False, it is not the case on a longer training run and actually will outperform the non-zero out version."
  },
  {
    "objectID": "posts/2022-05-02-zero_init_last_resnetperformance.html#next-steps",
    "href": "posts/2022-05-02-zero_init_last_resnetperformance.html#next-steps",
    "title": "Exploring Timm vs Torchvision Resnet18 Difference",
    "section": "Next Steps",
    "text": "Next Steps\nThe next thing to do is to test the claim that the zero’d version performs better and also to try other initializations as well. This is also not an issue if using pretrained weights since the bn2 weights will be specified by the pretrained weights already so this is only something that will occur if new performance metrics are being compared as was the case when this question arose."
  },
  {
    "objectID": "posts/2022-02-06-Makefile_to_create_githook.html",
    "href": "posts/2022-02-06-Makefile_to_create_githook.html",
    "title": "Using Makefile to Create Git Hook",
    "section": "",
    "text": "In our previous post, we created a Git Hook that will strip all the superfluous metadata automatically when you commit notebooks. We wanted to make the installation of this hook easier by creating a Makefile target.\nHere is what we came up with:\n[Download]\nRun make hook and Voila! You have your Git Hook that will reduce merge conflicts headaches."
  },
  {
    "objectID": "posts/2022-02-06-Makefile_to_create_githook.html#my-web-searches-while-creating-this-makefile",
    "href": "posts/2022-02-06-Makefile_to_create_githook.html#my-web-searches-while-creating-this-makefile",
    "title": "Using Makefile to Create Git Hook",
    "section": "My web searches while creating this Makefile",
    "text": "My web searches while creating this Makefile\n\nHow to convert multiline file into a string with newline \\n characters:\nawk '$1=$1' ORS='\\\\n' filename\n\nThis is how I converted an existing hook file (pre-commit) into a single string. I had to add tabs (\\t) manually, but this awk got me close enough.\n\nawk '$1=$1' ORS='\\\\n' pre-commit\nwill return:\n#!/bin/sh\\nfor file in $(git diff --diff-filter=d --cached --name-only | grep -E '\\.ipynb$')\\ndo\\njupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace \"$file\"\\ngit add \"$file\"\\ndone\\n\n\n\n\nHow to write a multiline file with echo:\necho -e \"Line 1\\r\\nLine2\" >> readme.txt\n\n\n\nHow to escape single quotes in single quoted strings:\n'\"'\"' is interpreted as '\n\n\n\nHow to escape dollar signs $ in Makefile:\nEscape $ by adding another $ (i.e. $$)\n\n\n\nReferences\n\nhttps://stackoverflow.com/a/26451573\nhttps://unix.stackexchange.com/a/219270\nhttps://stackoverflow.com/a/1250279\n\nhttps://til.hashrocket.com/posts/k3kjqxtppx-escape-dollar-sign-on-makefiles"
  },
  {
    "objectID": "posts/2021-05-18-5_Tips_for_a_New_CIO.html",
    "href": "posts/2021-05-18-5_Tips_for_a_New_CIO.html",
    "title": "5 Tips for a New CIO",
    "section": "",
    "text": "Congratulations on your new role as CIO! As the CIO, your job is to remove roadblocks, provide a vision, and enable your business partners without spending more than is required to do so. Here are 5 tips that I have gathered as an employee watching multiple CIO transitions."
  },
  {
    "objectID": "posts/2021-05-18-5_Tips_for_a_New_CIO.html#include-your-employees-in-your-5-year-plan",
    "href": "posts/2021-05-18-5_Tips_for_a_New_CIO.html#include-your-employees-in-your-5-year-plan",
    "title": "5 Tips for a New CIO",
    "section": "1. Include Your Employees in your 5-year Plan",
    "text": "1. Include Your Employees in your 5-year Plan\nEvery new CIO wants to create a vision, but one of the biggest mistakes that a new CIO can make is relying too much on external consultants or upper management to lay out this vision. Unfortunately, this type of top-down leadership strategy tends to lead to low adoption rates. While it is a great way to create a bold vision and great slide-deck, the resulting plan typically lacks adoption and leads to bland execution.\nIf you want buy-in to your vision, make sure you listen and absorb as you enter the new landscape and involve your employees when creating an execution plan. Finding a way to blend your 5-year plan with an execution roadmap that will get to that point is what will separate a great CIO from an average CIO. Give your staff a vision and operating rules and let them help you get to the end-state. Your job is to eliminate any roadblocks in the way of your vision and to make sure that everything that is being done today is in line with where you are trying to get to."
  },
  {
    "objectID": "posts/2021-05-18-5_Tips_for_a_New_CIO.html#utilize-consultants-sparingly",
    "href": "posts/2021-05-18-5_Tips_for_a_New_CIO.html#utilize-consultants-sparingly",
    "title": "5 Tips for a New CIO",
    "section": "2. Utilize Consultants Sparingly",
    "text": "2. Utilize Consultants Sparingly\nConsultants are often used as scapegoats when a project or initiative isn’t on track, but consultants can be a great resource for an organization when utilized properly. The key with utilizing consultants is to make sure you are getting the experience that consultants can bring with them. Ask yourself and your team if the value being provided is worth the money being paid and also consider the technical debt that is being added by using a consultant. Always ensure that a full-time employee is paired with the consultant to learn from them and to take over the project before a consultant leaves. Any work done by consultants must include meaningful documentation and meaningful tests. Consultants can also be utilized to spin up an environment, but make sure an internal resource is involved as well because once the environment is up, you don’t want to rely on a consultant to handle ongoing support."
  },
  {
    "objectID": "posts/2021-05-18-5_Tips_for_a_New_CIO.html#choose-your-advisors-carefully",
    "href": "posts/2021-05-18-5_Tips_for_a_New_CIO.html#choose-your-advisors-carefully",
    "title": "5 Tips for a New CIO",
    "section": "3. Choose Your Advisors Carefully",
    "text": "3. Choose Your Advisors Carefully\nAlong with eliminating roadblocks, another role as a new CIO is to identify who you listen to around the organization. Identifying people you should take advice from can be daunting task but it is the only way to effectively guide the organization. One way to help identify who you should take advice from in an organization is to engage in brainstorming sessions. The idea behind these sessions is to help develop a vision that everybody buys into and to get everybody talking in the same conversations. Not everybody is comfortable with the same medium so it is a good idea to follow up on these brainstorming sessions with output items as well based on the conversations. This is a great way to figure out who is going to execute at a high level. Another important thing to keep in mind with this group of advisors is that they have to be willing to tell you their true opinion even if it’s not popular. You should be encouraging this to the entire staff, but really emphasize it with your advisor team.\nOnce these projects start to gain traction, go out of your way to recognize members of your team and solicit advice from them. Ask them specifically what is working well, what isn’t working well, and see if they have any ideas to help improve things going forward. Focus on the areas that they talk about and ask follow-up questions. Identify the strengths and weaknesses of yourself and your advisors."
  },
  {
    "objectID": "posts/2021-05-18-5_Tips_for_a_New_CIO.html#learn-from-outages-together",
    "href": "posts/2021-05-18-5_Tips_for_a_New_CIO.html#learn-from-outages-together",
    "title": "5 Tips for a New CIO",
    "section": "4. Learn From Outages Together",
    "text": "4. Learn From Outages Together\nIssues can appear in many forms: missed SLAs, unresponsive servers, hanging jobs. Unplanned issues and outages should be viewed at as a huge opportunity to get your team a better understanding of how the overall technical landscape works at your company. Any time a member of your team is spending time fixing systems or chasing down issues, they aren’t working on the problems that are guiding the organization towards the 5-year vision. Understanding why system issues occur, how the issue was detected, how the solution was identified, and what can be done to reduce issues in the future either in severity or detection time are the key areas that should be focused on in this type of post-mortem. Be mindful that this doesn’t turn into a blame game. This is a collaborative team-building exercise and everybody should be growing from it. This is a great exercise that can help you understand how different pieces of the puzzle fit together and get everybody on the same page."
  },
  {
    "objectID": "posts/2021-05-18-5_Tips_for_a_New_CIO.html#guide-the-ship",
    "href": "posts/2021-05-18-5_Tips_for_a_New_CIO.html#guide-the-ship",
    "title": "5 Tips for a New CIO",
    "section": "5. Guide the Ship",
    "text": "5. Guide the Ship\nAs the CIO, you are responsible for crafting a draft of your organization’s roadmap and guiding your team. There will be times when you have to make changes to the 5-year vision, but it’s important to keep it in mind as you make decisions and enable your team to make decisions. Make yourself intentionally available to your organization. Keep an open door policy and advertise it. If you solicit advice and nobody responds, you probably need to re-consider how you are reaching out to your staff. Try meeting with small group, virtual suggestion boxes, or even one-on-one discussions until you find a medium that connects. Your actions will play a big part in setting the tone of the entire organization."
  },
  {
    "objectID": "posts/2021-08-03-5_Lessons_From_Richard_Feynman.html",
    "href": "posts/2021-08-03-5_Lessons_From_Richard_Feynman.html",
    "title": "5 Lessons from Richard Feynman",
    "section": "",
    "text": "I recently listened to a biography about Richard Feynman and I had some takeaways that surprised me. I had always vaguely known that Richard Feynman was an interesting guy and a great professor, but this book went so much deeper and taught me some valuable lessons.\n\nLesson 1: Don’t Stay in Your Lane\nWhether you are a writer that has decided to explore a topic that isn’t your forte or an athlete that has taken up a social justice campaign, expanding your boundaries and getting out of your comfort zone will have surprising results. Even if it doesn’t become a passion, it will still give you a perspective you wouldn’t have otherwise had. Being confined to a single topic just because you are good at it can also lead to burnout and won’t let you reach your full potential.\n\n\nLesson 2: Don’t Trust and Always Verify\nJust because other people agree with the results, doesn’t mean that the results are correct. Being able to reproduce results and understanding the assumptions that have been made by previous researchers can at a minimum help you obtain a more accurate mental model and might even bring up some questions about why certain assumptions were made. Sometimes this might have to be done in stages. Maybe when you are first looking at a new paper, trust everything except for their results and then as you are able to reproduce the results at that layer, go back a layer and re-consider the assumptions that were made. Was X really the best choice for this task? What other options would have been available as alternatives for that task.\n\n\nLesson 3: Ask Questions (Even the “Obvious” ones)\nIf you don’t understand a concept that is being discussed, ask somebody that has more experience to explain it in another way. A lot of the time, these questions will lead to interesting discussions, unexplored knowledge gaps, and sometimes entirely new projects to work on. One great way to learn is to listen to people talk about things that they have learned and ask them questions.\n\n\nLesson 4: “I Don’t Know”\nNot knowing an answer to a question is not something that should discourage you. On the contrary, knowing that you don’t know the answer to something is a really positive step in the right direction. This also gives you a great chance to improve your knowledge and learn something that you don’t currently know.\n\n\nLesson 5: Define the Problem\nWhether you are working by yourself or with a group of people, focus on defining the problem before trying to come up with a solution. Use real-world examples when discussing what the problem actually is. Having different viewpoints is great when brainstorming, but make sure everybody is at least focusing on the same problem and not talking past each other.\n\n\nConclusion\nThese five lessons are just the start of what I learned from this book which is also full of entertaining stories and facts about Richard Feynman’s life. It’s well worth the read (or listen)."
  },
  {
    "objectID": "posts/2021-05-21-python_slots_exploration.html",
    "href": "posts/2021-05-21-python_slots_exploration.html",
    "title": "Python Native slots Attribute Exploration",
    "section": "",
    "text": "While reading the python data model documentation, I came across something I hadn’t seen before. __slots__ is an optional argument that allows users to “explicitly declare data members”. It is an interesting concept that I haven’t seen utilized, but perhaps the reason is that not many people are aware it exists. I am going to explore this attribute that is available to see if it might provide value for my future projects. According to this blog post, __slots__ can significantly reduce the amount of ram required to create objects (40-50%!). Now let’s dive in and figure out how it’s used!\n\n\nPython Version Check:\n3.8.8 (default, Apr 13 2021, 19:58:26) \n[GCC 7.3.0]"
  },
  {
    "objectID": "posts/2021-05-21-python_slots_exploration.html#example-1-typical-case",
    "href": "posts/2021-05-21-python_slots_exploration.html#example-1-typical-case",
    "title": "Python Native slots Attribute Exploration",
    "section": "Example #1: Typical Case",
    "text": "Example #1: Typical Case\nThe first example we look at is the working example. we will have a class A1 with slots set to accept one var named var1.\n\nclass A1:\n    __slots__ = ['var1']\n    def __init__(self, value_passed_through_here):\n        self.var1 = value_passed_through_here\n\n\na1 = A1(1); a1.var1\n\n1\n\n\na has been created and everything is going well. Let’s try adding another attribute.\n\na1.var2 = \"but can I set another var?\"\n\nAttributeError: 'A1' object has no attribute 'var2'\n\n\na.var2 fails as expected because it isn’t in the __slots__ list and __slots__ is read-only so it cannot be updated.\n\na1.__slots__ = ['var2']\n\nAttributeError: 'A1' object attribute '__slots__' is read-only\n\n\n\na1.__slots__ = ['var1', 'var2']\n\nAttributeError: 'A1' object attribute '__slots__' is read-only\n\n\nWhen __slots__ is used, the __dict__ value is not set. Let’s explore that a little further though.\n\na1.__dict__\n\nAttributeError: 'A1' object has no attribute '__dict__'"
  },
  {
    "objectID": "posts/2021-05-21-python_slots_exploration.html#example-2a-exploring-__dict__-without-using-__slots__",
    "href": "posts/2021-05-21-python_slots_exploration.html#example-2a-exploring-__dict__-without-using-__slots__",
    "title": "Python Native slots Attribute Exploration",
    "section": "Example #2a: Exploring __dict__ Without Using __slots__",
    "text": "Example #2a: Exploring __dict__ Without Using __slots__\n\nclass A2A:\n    def __init__(self, value_passed_through_here):\n        self.var1 = value_passed_through_here\n\n\na2a = A2A(1)\n\nvar1 shows up as expected when creating an object\n\na2a.__dict__\n\n{'var1': 1}\n\n\n\na2a.var2 = 'adding a second thing'\n\nAdding a second variable adds it to the __dict__ as expected\n\na2a.__dict__\n\n{'var1': 1, 'var2': 'adding a second thing'}"
  },
  {
    "objectID": "posts/2021-05-21-python_slots_exploration.html#example-2b-exploring-__dict__-when-using-__slots__",
    "href": "posts/2021-05-21-python_slots_exploration.html#example-2b-exploring-__dict__-when-using-__slots__",
    "title": "Python Native slots Attribute Exploration",
    "section": "Example #2b: Exploring __dict__ When Using __slots__",
    "text": "Example #2b: Exploring __dict__ When Using __slots__\n\nclass A2B:\n    __slots__ = ['var1', '__dict__']\n    def __init__(self, value_passed_through_here):\n        self.var1 = value_passed_through_here\n\n\na2b = A2B(1)\n\n\na2b.__dict__\n\n{}\n\n\n__dict__ exists now since we added it to __slots__, but it isn’t populating the __dict__ like normal. We are still able to call the attribute var1 though.\n\na2b.var1\n\n1\n\n\n\na2b.var2 = \"test if we can add new variables now\"\n\nSurprisingly, once we add __dict__ to the __slots__ list, adding a new var works.\n\na2b.__dict__\n\n{'var2': 'test if we can add new variables now'}\n\n\nWhen we look at __dict__ after adding var2, there is an entry in __dict__ as well.\n\na2b.var2\n\n'test if we can add new variables now'\n\n\nSo if we enable __dict__ we are able to add new items to the __dict__, but __dict__ has to be explicitly defined to work."
  },
  {
    "objectID": "posts/2021-05-21-python_slots_exploration.html#example-3-inheritance",
    "href": "posts/2021-05-21-python_slots_exploration.html#example-3-inheritance",
    "title": "Python Native slots Attribute Exploration",
    "section": "Example 3: Inheritance",
    "text": "Example 3: Inheritance\nNow that we’ve explored __slots__, let’s see how it behaves when one class is inherited from another.\n\nclass A3:\n    __slots__ = ['a']\n    def __init__(self, a):\n        self.a = a\n\n\na3 = A3(1); a3.a\n\n1\n\n\n\nclass B3A(A3):\n    def __init__(self):\n        self.a = 1\n        self.b = 2\n\n\nb3a = B3A()\n\n\nb3a.__slots__\n\n['a']\n\n\n\nb3a.__dict__\n\n{'b': 2}\n\n\n\nb3a.a\n\n1\n\n\n\nb3a.c = \"can I set c?\"\n\n\nb3a.__dict__\n\n{'b': 2, 'c': 'can I set c?'}\n\n\nSo when B3A is inherited from A3, it uses the slots class, but it also reverts back in a lot of ways to a normal, non-slots, class again. The last thing I’m going to try is actually setting a __slots__ in B3B just to see what happens\n\nclass B3B(A3):\n    __slots__ = ['a','b']\n    def __init__(self):\n        self.a = 1\n        self.b = 2\n\n\nb3b = B3B()\n\n\nb3b.c = \"can I set this?\"\n\nAttributeError: 'B3B' object has no attribute 'c'\n\n\nSo now that we have given B3B a __slots__ it is no longer behaves the same way that B3A is.\nHere is what the official documentation says about this: >The action of a __slots__ declaration is not limited to the class where it is defined. __slots__ declared in parents are available in child classes. However, child subclasses will get a __dict__ and __weakref__ unless they also define __slots__ (which should only contain names of any additional slots). (https://docs.python.org/3.8/reference/datamodel.html#notes-on-using-slots 5th bullet)"
  },
  {
    "objectID": "posts/2021-05-21-python_slots_exploration.html#conclusion",
    "href": "posts/2021-05-21-python_slots_exploration.html#conclusion",
    "title": "Python Native slots Attribute Exploration",
    "section": "Conclusion",
    "text": "Conclusion\n__slots__ is an interesting concept that is built into Python that I hadn’t heard of and wanted to explore. Hopefully this notebook is informative to other Python users as well. Things didn’t always behave as I would have expected and that’s part of the fun of actually testing out the code to see how things work in practice."
  },
  {
    "objectID": "posts/2021-09-19-texas-rate-inspector.html",
    "href": "posts/2021-09-19-texas-rate-inspector.html",
    "title": "Texas Rate Inspector Overview",
    "section": "",
    "text": "Currently Texas’ rate selection process is heavily driven by the lowest cost for a typical user. However, this limits the imagination of Retail Electricity Providers (REP) and ends up hurting residential customers by creating a single variable rate plan that every REP optimizes for. We at the Problem Solvers Guild have a different way of measuring plans that utilizes a customers’ true historical usage. Now the question is, which of the 159 plans currently available in almost any zip code in Texas is right for an individual?"
  },
  {
    "objectID": "posts/2021-09-19-texas-rate-inspector.html#whats-the-solution",
    "href": "posts/2021-09-19-texas-rate-inspector.html#whats-the-solution",
    "title": "Texas Rate Inspector Overview",
    "section": "What’s the Solution?",
    "text": "What’s the Solution?\nThe Problem Solvers Guild has developed a method to take any electric rate plan and apply an individual’s usage to calculate a more accurate price-per-kilowatt than what is typically provided by REPs. With this solution, we help residential customers identify plans they would have previously overlooked.\nWe grab usage by working with Smart Meter Texas. A site that was created to allow residential customers to access their 15-minute usage data. This site also allows customers to share this access with companies with just a few pieces of information.\nAll of the rate plans that we currently use come from Power to Choose which is a site where REPs are able to advertise their available rates to residential customers. Each plan has an electricity facts label (EFL) that describes the plan in detail. We use these EFLs to model different rate plans that are offered.\nOnce we have all of this information, we combine the data and find the rate that would have given an individual the lowest price-per-kilowatt over the past year. If this is something you are interested in knowing for yourself, fill out our form and we will run an analysis for you. If you are a company that would like to use our library to create better rate plans or to drive your rate engine, feel free to contact us and we can find a solution based on your needs as well."
  },
  {
    "objectID": "posts/2021-09-19-texas-rate-inspector.html#how-does-it-work",
    "href": "posts/2021-09-19-texas-rate-inspector.html#how-does-it-work",
    "title": "Texas Rate Inspector Overview",
    "section": "How does it work?",
    "text": "How does it work?\nThe Texas Rate Inspector was developed by The Problem Solvers Guild as a tool that allows an individual to quickly simulate any rate plan with their historical usage. Let me take you on a quick tour of how this tool works.\nHere are three rate plans to choose from.\n1. Plan 1 (Flat Rate) charges \\(</span>0.119 per kWh plus a <span>\\)9.95 monthly charge if your usage for that month is less than 1000 kWh. 1. Plan 2 (Free Weekends) charges \\(</span>0.200 per kWh between 12:00 AM Monday to 7:00 PM Friday plus a <span>\\)4.95 monthly charge. 1. Plan 3 (Tiered Rate) charges \\(</span>0.089 per kWh between 0 and 1200 kWh of usage and <span>\\)0.145 per kWh past that. This plan also has a \\(</span>9.95 monthly charge and a <span>\\)30 credit if you use more than 800 kWh.\nNow that I’ve given you all the information you need, which of these plans is the best option for a residential customer in Texas?\nYou can close your Excel sheet. I will walk you through each of these scenarios and at the end, I will tell you which of these is the best plan for the user."
  },
  {
    "objectID": "posts/2021-09-19-texas-rate-inspector.html#grabbing-historical-usage",
    "href": "posts/2021-09-19-texas-rate-inspector.html#grabbing-historical-usage",
    "title": "Texas Rate Inspector Overview",
    "section": "Grabbing Historical Usage",
    "text": "Grabbing Historical Usage\nFirst, we grab a user’s usage from Smart Meter Texas.\nnote: If you are interested in providing your usage information to us, fill out our form and we will request access to your usage.\n\nfrom rate_inspector.usage import *\nstart_date = pd.to_datetime(datetime.date(2021, 7, 1))\nend_date = pd.to_datetime(datetime.date(2021, 7,31))\ndata = pd.read_csv('../assets/example_usage.csv', parse_dates=['USAGE_DATE'])\ndata = get_data(data, start_date, end_date)\njuly_usage_data = UsageComponent.from_historical_df(data)\n\nHere is a single day of historical usage (96 15-minute intervals).\n\njuly_usage_data.plot_usage(0,96)\n\n\n\n\nNow that we have historical usage, let’s model each of the three plans."
  },
  {
    "objectID": "posts/2021-09-19-texas-rate-inspector.html#plan-1-flat-rate",
    "href": "posts/2021-09-19-texas-rate-inspector.html#plan-1-flat-rate",
    "title": "Texas Rate Inspector Overview",
    "section": "Plan 1: Flat Rate",
    "text": "Plan 1: Flat Rate\n\nPlan 1 (Flat Rate) charges \\(</span>0.119 per kWh plus a <span>\\)9.95 monthly charge if your usage for that month is less than 1000 kWh\n\n\nfrom rate_inspector.rate import * \nfrom rate_inspector.plan import *\n\nFirst, we identify the rate components. This first plan has a FlatThreshold component that has a flat charge up to a threshold.\n\nbase_w_peak = FlatThreshold('Flat Charge', 9.95, max_thresh=1000)\nkwh_charge = TimeOfUseRate('Energy Charge', 0.119)\n\nOnce all of the rate components have been defined, we create a rate plan that contains all of the rate components.\n\nplan1 = RatePlan('Flat Rate', [base_w_peak, kwh_charge])\n\nNext, we create a statement which combines the plan and the usage.\n\nstatement1 = Statement(plan1, july_usage_data)\n\nSince total_usage was under 1000 kWh in this case, the $9.95 base charge is applied.\n\nstatement1.print_line_item()\n\nFlat Charge: 9.9500\nEnergy Charge: 96.2753\n\n\n\nstatement1.total_usage\n\n809.0359999999996\n\n\nSo our total cost per kwh is $0.131/kWh for Plan 1\n\nstatement1.price_per_kwh\n\n0.1312985874546989"
  },
  {
    "objectID": "posts/2021-09-19-texas-rate-inspector.html#plan-2-free-weekends",
    "href": "posts/2021-09-19-texas-rate-inspector.html#plan-2-free-weekends",
    "title": "Texas Rate Inspector Overview",
    "section": "Plan 2: Free Weekends",
    "text": "Plan 2: Free Weekends\n\nPlan 2 (Free Weekends) charges \\(</span>0.200 per kWh between 12:00 AM Monday to 7:00 PM Friday plus a <span>\\)4.95 monthly charge\n\n\nenergy_kwh_rest_of_week = TimeOfUseRate('RoW Energy Charge', 0.20, days_of_week=[calendar.MONDAY, calendar.TUESDAY, calendar.WEDNESDAY, calendar.THURSDAY])\nenergy_kwh_friday = TimeOfUseRate('Friday Energy Charge', 0.20, days_of_week=[calendar.FRIDAY], end_time=datetime.time(7))\nbase = Flat('Base Charge', 4.95)\n\n\nplan2 = RatePlan('Free Weekends', [energy_kwh_rest_of_week, energy_kwh_friday, base])\n\n\nstatement2 = Statement(plan2, july_usage_data)\n\n\nstatement2.print_line_item()\n\nRoW Energy Charge: 82.3296\nFriday Energy Charge: 1.7952\nBase Charge: 4.9500\n\n\nSo our total cost per kwh is \\(</span>0.11/kWh for Plan 2 even though the advertised price on PowerToChoose shows the minimum price per kWh to be <span>\\)0.132 (@ 2000 kWh)\n\nstatement2.price_per_kwh\n\n0.11009992138792346"
  },
  {
    "objectID": "posts/2021-09-19-texas-rate-inspector.html#plan-3-tiered-rate",
    "href": "posts/2021-09-19-texas-rate-inspector.html#plan-3-tiered-rate",
    "title": "Texas Rate Inspector Overview",
    "section": "Plan 3: Tiered Rate",
    "text": "Plan 3: Tiered Rate\n\nPlan 3 (Tiered Rate) charges \\(</span>0.089 per kWh between 0 and 1200 kWh of usage and <span>\\)0.145 per kWh past that. This plan also has a \\(</span>9.95 monthly charge and a <span>\\)30 credit if you use more than 800 kWh.\n\n\nbase = Flat('Base Charge', 9.95)\ntier1 = Tier('0-1200 kWh', 0.089, max_kwh=1200)\ntier2 = Tier('> 1200 kWh', 0.145, min_kwh=1200)\ncredit = FlatThreshold('800 kwh credit', -30, min_thresh=800)\n\n\nplan3 = RatePlan('Tiered Rate', [base, tier1, tier2, credit])\n\n\nstatement3 = Statement(plan3, july_usage_data)\n\n\nstatement3.print_line_item()\n\nBase Charge: 9.9500\n0-1200 kWh: 72.0042\n> 1200 kWh: 0.0000\n800 kwh credit: -30.0000\n\n\nPlan 3 ends up being a great option for the usage. Our user would have paid just $0.064/kWh for plan number three. One thing to keep in mind is that we are only simulating a single month at this point.\n\nstatement3.price_per_kwh\n\n0.06421741924957602"
  },
  {
    "objectID": "posts/2021-09-19-texas-rate-inspector.html#conclusion",
    "href": "posts/2021-09-19-texas-rate-inspector.html#conclusion",
    "title": "Texas Rate Inspector Overview",
    "section": "Conclusion",
    "text": "Conclusion\nThis blog post shows an example of how the Texas Rate Inspector works. If you are interested in getting your electric rates modeled or would like to hear more about what we do, please reach out. We would love to take your usage and help you find your next rate plan."
  },
  {
    "objectID": "posts/2021-08-16-tabular_normalization_exploration.html",
    "href": "posts/2021-08-16-tabular_normalization_exploration.html",
    "title": "Normalizing fastai Tabular Inputs",
    "section": "",
    "text": "Normalizing data is an important aspect of data science that seemed like magic to me for the longest time. I could train a model for 10 epochs on un-normalized data and make little or no progress towards an effective model, then restart and run the same data through my model with normalization applied and have a perfect model. Why was this the case? What was happening that caused such a big difference? Let’s explore that question."
  },
  {
    "objectID": "posts/2021-08-16-tabular_normalization_exploration.html#what-is-normalization",
    "href": "posts/2021-08-16-tabular_normalization_exploration.html#what-is-normalization",
    "title": "Normalizing fastai Tabular Inputs",
    "section": "What is Normalization?",
    "text": "What is Normalization?\nNormalization is the process of converting all of the numbers so the full set of numbers has a mean of 0 and a standard deviation of 1. My guess is that for most people reading this, those numbers and terms don’t mean a lot. So let’s dig in a bit deeper. Normalization is the process of putting all of our values on a common scale. By ensuring that the overall number range has a mean of 0, each side of zero will have half of the weight of the numbers. That means that all of the values are balanced around 0. Let’s get a real example.\nLet’s assume we have a set of numbers: [1, 5, 10, 25, 100, 1000].\n\nexample_1 = np.array([1, 5, 10, 25, 100, 1000]); example_1\n\narray([   1,    5,   10,   25,  100, 1000])\n\n\n\n\n\n\n\nHow would we transform these numbers to have a mean of 0? Let’s start by finding the current mean of these numbers.\n\nexample_1.mean()\n\n190.16666666666666\n\n\n190.16666 is the mean of this set of numbers. In order to make this 0, we should be able to just subtract each number by that amount.\n\nexample_1_mean_0 = example_1-example_1.mean(); example_1_mean_0\n\narray([-189.16666667, -185.16666667, -180.16666667, -165.16666667,\n        -90.16666667,  809.83333333])\n\n\n\n\n\n\n\n\nf\"The mean is now: {example_1_mean_0.mean()} (AKA: 0)\"\n\n'The mean is now: 1.8947806286936004e-14 (AKA: 0)'\n\n\nInstead of being distributed around the number 190.1666, the numbers are now distributed around the number 0. With the numbers now balanced correctly, we want to remove scale and units from our dataset. statista.com has the following example to explain standard deviation: > 1,000 people were questioned about their monthly phone bill. The mean value is \\$40 and the standard deviation 27. Which means that the average distance of all answers (=values) to the mean is \\$27.\nLet’s consider what it means then to divide by the standard deviation. For this example, we would divide by 27 so that the standard deviation is 1 and that is going put us into the same scale whether we started with a huge scale like monthly mortgage amount or monthly phone bill. It strips the original scaling away from the dataset and puts onto a scale where 1 is the distance from the center point. This is helpful because it means that a step in any direction will change the loss the same amount. Another way of putting this is that it smooths the loss landscape so all attributes have the same amount of pull.\n\nexample_1_mean_0_std_1 = example_1_mean_0/example_1_mean_0.std(); example_1_mean_0_std_1\n\narray([-0.52008301, -0.50908566, -0.49533897, -0.45409891, -0.2478986 ,\n        2.22650516])\n\n\n\n\n\n\n\n\nexample_1_mean_0_std_1.mean(), example_1_mean_0_std_1.std()\n\n(7.401486830834377e-17, 1.0)\n\n\nNotice that in all three graphs, the distribution looks the same. Lots of numbers on the left-hand side and only the one number on the right. But to our neural network, there is a huge difference between taking numbers ranging from -0.52 and 2.23 and taking numbers ranging from 1 to 1000. The weights can make a lot finer adjustments on the normalized version and each incoming data point will be on the same scale. That’s why when an image model has been pretrained on Imagenet, you have to keep the same standard deviation and mean. By using those numbers, you are taking all of the pixels in your image and converting them to the normal range for Imagenet."
  },
  {
    "objectID": "posts/2021-08-16-tabular_normalization_exploration.html#testing-the-theory-with-fastais-tabular-application",
    "href": "posts/2021-08-16-tabular_normalization_exploration.html#testing-the-theory-with-fastais-tabular-application",
    "title": "Normalizing fastai Tabular Inputs",
    "section": "Testing the Theory with fastai’s Tabular Application",
    "text": "Testing the Theory with fastai’s Tabular Application\nNow that we have an idea of what is happening, let’s see if we can prove that it actually works how we anticipate. To do this, I will be creating a simple tabular learner and a sample dataloader. I will first train the model on an normalized input and then train the same model using the un-normalized input. My assumption is that the normalized inputs will be much better at converting the x value into y than the un-normalized version.\n\nExperiment Setup\nx1 + xN = y\n\nfrom fastai.tabular.all import *\nx = torch.rand((200000,10))\nscale_amt = torch.randint(0,100,size=(10,))\n#x = x*scale_amt\ndf = pd.DataFrame(x, columns=[f\"x{n}\" for n in range(1,11)])\n#df['x1']=df['x1']*1000\n#df.columns\n#df['y'] = df['x1'] + df['x2'] + df['x3'] + df['x4'] + df['x5'] + df['x6'] + df['x7'] + df['x8'] + df['x9'] + df['x10']\ndf['y'] = (df.values*scale_amt.tolist()).sum(axis=1)\nsplits = RandomSplitter()(df)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      x1\n      x2\n      x3\n      x4\n      x5\n      x6\n      x7\n      x8\n      x9\n      x10\n      y\n    \n  \n  \n    \n      0\n      0.075054\n      0.456406\n      0.745085\n      0.930838\n      0.109559\n      0.498177\n      0.565746\n      0.743462\n      0.893542\n      0.796160\n      343.622546\n    \n    \n      1\n      0.823718\n      0.925904\n      0.418757\n      0.579081\n      0.645427\n      0.502683\n      0.689346\n      0.439689\n      0.281477\n      0.984495\n      380.632963\n    \n    \n      2\n      0.212132\n      0.928440\n      0.068775\n      0.974991\n      0.274955\n      0.107026\n      0.274451\n      0.439982\n      0.731169\n      0.026224\n      295.966817\n    \n    \n      3\n      0.874167\n      0.900319\n      0.221668\n      0.755159\n      0.525875\n      0.861770\n      0.347837\n      0.993839\n      0.466206\n      0.554722\n      442.779244\n    \n    \n      4\n      0.836071\n      0.795962\n      0.009992\n      0.531759\n      0.619634\n      0.196202\n      0.648332\n      0.899659\n      0.893503\n      0.094362\n      408.136464\n    \n  \n\n\n\n\nmonitor_parameters is a hook that can be used to monitor values inside of a model. It is a little bit of a hack, but it works really well for getting a better idea what is happening inside the model.\n\ndef monitor_parameters(m, i, o):\n    m.weight_track.append(list(m.parameters())[0].tolist()[0])\n    m.bias_track.append(list(m.parameters())[1].tolist())\n\n\n\nNormalized\n\nto_normalized = TabularPandas(df, cont_names=['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10'], y_names='y', procs=[Normalize], splits=splits)\ndls_normalized = to_normalized.dataloaders(verbose=True, shuffle=False)\nlearn_normalized = tabular_learner(dls_normalized, layers=[], config=tabular_config(use_bn=False, bn_cont=False))\nlearn_normalized.lr_find(start_lr=1e-3, end_lr=1000000)\n\nSetting up after_item: Pipeline: \nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: ReadTabBatch\n\n\n\n\n\nSuggestedLRs(valley=3.981071710586548)\n\n\n\n\n\n\nlearn_normalized.model.layers[0][0].bias_track = []\nlearn_normalized.model.layers[0][0].weight_track = []\nlearn_normalized.model.layers[0][0].register_full_backward_hook(monitor_parameters)\n\n<torch.utils.hooks.RemovableHandle at 0x7f44a05ccf70>\n\n\n\nlearn_normalized.fit_one_cycle(10, 10)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.142768\n      0.722641\n      00:08\n    \n    \n      1\n      6.045738\n      0.614625\n      00:08\n    \n    \n      2\n      16.018417\n      7.314993\n      00:08\n    \n    \n      3\n      8.353395\n      5.835221\n      00:08\n    \n    \n      4\n      3.468160\n      0.493669\n      00:08\n    \n    \n      5\n      2.660133\n      1.251976\n      00:08\n    \n    \n      6\n      0.722458\n      0.373421\n      00:08\n    \n    \n      7\n      0.367568\n      0.272525\n      00:08\n    \n    \n      8\n      0.025171\n      0.010871\n      00:08\n    \n    \n      9\n      0.000455\n      0.000518\n      00:08\n    \n  \n\n\n\n\nlearn_normalized.recorder.plot_loss(skip_start=1000)\n\n\n\n\n\nlearn_normalized.show_results(ds_idx=0, shuffle=False)\n\n\n\n\n\n\n  \n    \n      \n      x1\n      x2\n      x3\n      x4\n      x5\n      x6\n      x7\n      x8\n      x9\n      x10\n      y\n      y_pred\n    \n  \n  \n    \n      0\n      1.562167\n      -1.569042\n      -1.114254\n      0.678972\n      -0.447562\n      -0.721848\n      0.950705\n      -0.506051\n      1.427110\n      1.305289\n      318.861755\n      318.871887\n    \n    \n      1\n      -0.558561\n      1.409515\n      -1.694918\n      0.051202\n      -1.187221\n      0.865891\n      1.003036\n      -0.675074\n      1.489744\n      -1.504115\n      319.647339\n      319.605621\n    \n    \n      2\n      0.580085\n      1.621262\n      1.555035\n      -1.474297\n      0.485046\n      1.653549\n      -1.121346\n      0.184075\n      1.324292\n      -0.353307\n      442.348450\n      442.299805\n    \n    \n      3\n      1.634533\n      1.501764\n      0.694681\n      -0.052227\n      0.998929\n      -0.802163\n      0.824090\n      1.452214\n      0.217072\n      -0.867735\n      456.612335\n      456.587646\n    \n    \n      4\n      0.305931\n      0.446660\n      0.606511\n      0.404355\n      -0.881533\n      -0.470554\n      1.398422\n      0.411920\n      0.697609\n      1.205599\n      364.134827\n      364.130890\n    \n    \n      5\n      -0.953506\n      -1.129230\n      1.685477\n      0.200103\n      1.446620\n      1.669933\n      0.654975\n      -0.218070\n      0.763460\n      1.622134\n      378.366058\n      378.378632\n    \n    \n      6\n      -1.729204\n      1.402740\n      1.129965\n      -1.550283\n      -0.174763\n      -1.702276\n      0.349754\n      -1.626720\n      -0.736905\n      0.554198\n      219.651688\n      219.670380\n    \n    \n      7\n      1.051801\n      0.651880\n      -0.606515\n      -0.211603\n      1.065033\n      1.140438\n      1.395582\n      -1.302371\n      0.548107\n      0.569542\n      379.511841\n      379.522766\n    \n    \n      8\n      0.845884\n      -0.547829\n      -1.420988\n      0.976506\n      -1.266368\n      -0.449748\n      -0.053614\n      -0.578055\n      0.252759\n      -0.031057\n      266.948334\n      266.938416\n    \n  \n\n\n\n\nplt.plot(learn_normalized.model.layers[0][0].bias_track)\nplt.show()\n\n\n\n\n\nplt.plot(learn_normalized.model.layers[0][0].weight_track)\nplt.show()\n\n\n\n\n\n\nNot Normalized\n\nto_not_normalized = TabularPandas(df, cont_names=['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10'], y_names=['y'], splits=splits)\ndls_not_normalized = to_not_normalized.dataloaders(verbose=True, shuffle=False)\ndls_not_normalized.one_batch()\nlearn_not_normalized = tabular_learner(dls_not_normalized, layers=[], config=tabular_config(use_bn=False, bn_cont=False), train_bn=False)\nlearn_not_normalized.lr_find(start_lr=1e-3, end_lr=1000000)\n\nSetting up after_item: Pipeline: \nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: ReadTabBatch\n\n\n\n\n\nSuggestedLRs(valley=3.2359366416931152)\n\n\n\n\n\n\nlearn_not_normalized.model.layers[0][0].bias_track = []\nlearn_not_normalized.model.layers[0][0].weight_track = []\nlearn_not_normalized.model.layers[0][0].register_full_backward_hook(monitor_parameters)\n\n<torch.utils.hooks.RemovableHandle at 0x7f4473678e80>\n\n\n\nlearn_not_normalized.fit_one_cycle(10, 10)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      154.066040\n      153.006943\n      00:08\n    \n    \n      1\n      453.208710\n      498.198853\n      00:08\n    \n    \n      2\n      450.821014\n      405.338318\n      00:08\n    \n    \n      3\n      462.034576\n      507.816345\n      00:08\n    \n    \n      4\n      402.913025\n      391.320465\n      00:08\n    \n    \n      5\n      299.577545\n      273.074860\n      00:08\n    \n    \n      6\n      187.224609\n      182.667740\n      00:08\n    \n    \n      7\n      95.384758\n      89.960159\n      00:08\n    \n    \n      8\n      57.199078\n      54.958164\n      00:08\n    \n    \n      9\n      27.854624\n      28.252810\n      00:08\n    \n  \n\n\n\n\nlearn_not_normalized.recorder.plot_loss(skip_start=500)\n\n\n\n\n\nlearn_not_normalized.show_results(ds_idx=0, shuffle=False)\n\n\n\n\n\n\n  \n    \n      \n      x1\n      x2\n      x3\n      x4\n      x5\n      x6\n      x7\n      x8\n      x9\n      x10\n      y\n      y_pred\n    \n  \n  \n    \n      0\n      0.950224\n      0.048134\n      0.178727\n      0.696215\n      0.370312\n      0.293320\n      0.776244\n      0.353630\n      0.912490\n      0.877288\n      318.861755\n      318.913208\n    \n    \n      1\n      0.339225\n      0.906561\n      0.010991\n      0.514821\n      0.156830\n      0.751503\n      0.791369\n      0.304868\n      0.930580\n      0.066351\n      319.647339\n      318.007935\n    \n    \n      2\n      0.667278\n      0.967587\n      0.949804\n      0.074028\n      0.639484\n      0.978802\n      0.177347\n      0.552729\n      0.882794\n      0.398533\n      442.348450\n      431.576630\n    \n    \n      3\n      0.971073\n      0.933147\n      0.701273\n      0.484935\n      0.787802\n      0.270143\n      0.739647\n      0.918583\n      0.563008\n      0.250043\n      456.612335\n      444.280334\n    \n    \n      4\n      0.588292\n      0.629064\n      0.675804\n      0.616864\n      0.245058\n      0.365837\n      0.905650\n      0.618462\n      0.701796\n      0.848512\n      364.134827\n      360.096039\n    \n    \n      5\n      0.225438\n      0.174889\n      0.987485\n      0.557846\n      0.917016\n      0.983530\n      0.690767\n      0.436712\n      0.720815\n      0.968745\n      378.366058\n      374.773651\n    \n    \n      6\n      0.001953\n      0.904608\n      0.827014\n      0.052072\n      0.449048\n      0.010392\n      0.602547\n      0.030322\n      0.287482\n      0.660485\n      219.651688\n      226.905151\n    \n    \n      7\n      0.803183\n      0.688209\n      0.325397\n      0.438884\n      0.806881\n      0.830731\n      0.904829\n      0.123895\n      0.658617\n      0.664914\n      379.511841\n      374.775940\n    \n    \n      8\n      0.743857\n      0.342450\n      0.090121\n      0.782187\n      0.133986\n      0.371841\n      0.485960\n      0.332858\n      0.573315\n      0.491550\n      266.948334\n      270.995636\n    \n  \n\n\n\n\nplt.plot(learn_not_normalized.model.layers[0][0].bias_track)\nplt.show()\n\n\n\n\n\nplt.plot(learn_not_normalized.model.layers[0][0].weight_track)\nplt.show()\n\n\n\n\n\n#actual values the parameter value should have\nscale_amt\n\ntensor([79, 93, 48, 48, 84, 53, 18, 92, 93, 22])\n\n\n\nlist(learn_not_normalized.model.layers[0][0].parameters())\n\n[Parameter containing:\n tensor([[73.1407, 84.1651, 45.1576, 45.1585, 77.2888, 49.8361, 16.7273, 83.5148,\n          84.1355, 20.6937]], device='cuda:0', requires_grad=True),\n Parameter containing:\n tensor([25.1835], device='cuda:0', requires_grad=True)]\n\n\n\nlist(learn_normalized.model.layers[0][0].parameters())\n\n[Parameter containing:\n tensor([[22.7578, 26.8047, 13.8633, 13.8707, 24.2422, 15.2929,  5.2018, 26.5390,\n          26.8526,  6.3496]], device='cuda:0', requires_grad=True),\n Parameter containing:\n tensor([315.1483], device='cuda:0', requires_grad=True)]"
  },
  {
    "objectID": "posts/2021-08-16-tabular_normalization_exploration.html#results",
    "href": "posts/2021-08-16-tabular_normalization_exploration.html#results",
    "title": "Normalizing fastai Tabular Inputs",
    "section": "Results",
    "text": "Results\nThe results ended up matching our theory but in a slightly different way than I had anticipated. You can see that the non-normalized weights line up a lot closer to the scale_amt and after thinking about the reason for that, I think I understand. When the input values are normalized, all of the numbers are put onto the same scale so the easiest way for the gradients to be adjusted is to just move the bias. But in the non-normalized version when adjusting the bias, all of the individual weights would be moving at different rates so it is a lot less likely that the bias will be moved as the best step for the model. Instead the weights are adjusted more during the training process."
  },
  {
    "objectID": "posts/2022-11-02-diffedit-implementation.html",
    "href": "posts/2022-11-02-diffedit-implementation.html",
    "title": "DiffEdit Paper Implementation",
    "section": "",
    "text": "The goal of this blog post is to implement the DiffEdit paper to the best of my understanding. While this task is primarily to help my own understanding of the process, I also want to help the reader of my post understand the process better as well.\nBefore I start, I want to give a big thanks to the DiffEdit authors (Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord) for publishing this paper. Without the openness and willingness to share, this type of implementation would not be possible. I also want to thank the fast.ai community for helping me solve problems when I was unclear about how to move forward. Finally, I want to thank Jonathan Whitaker, the auther of the Stable Diffusion Deep Dive notebook. This was the notebook that I started with in my DiffEdit implementation.\nIf anybody reading this blog post works in manufacturing and cares about improving product quality (or knows somebody that does), please reach out to me at kevin@problemsolversguild.com.\n#hide ## Loading the models\nThis code (and that in the next section) comes from the Huggingface example notebook.\nThis will download and set up the relevant models and components we’ll be using. Let’s just run this for now and move on to the next section to check that it all works before diving deeper.\nIf you’ve loaded a pipeline, you can also access these components using pipe.unet, pipe.vae and so on.\n\n# Load the autoencoder model which will be used to decode the latents into image space. \nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n\n# Load the tokenizer and text encoder to tokenize and encode the text. \ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n\n# The UNet model for generating the latents.\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n\n\n# The noise scheduler\n# scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\nscheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n\n\nscheduler.timesteps\n\ntensor([999, 998, 997, 996, 995, 994, 993, 992, 991, 990, 989, 988, 987, 986,\n        985, 984, 983, 982, 981, 980, 979, 978, 977, 976, 975, 974, 973, 972,\n        971, 970, 969, 968, 967, 966, 965, 964, 963, 962, 961, 960, 959, 958,\n        957, 956, 955, 954, 953, 952, 951, 950, 949, 948, 947, 946, 945, 944,\n        943, 942, 941, 940, 939, 938, 937, 936, 935, 934, 933, 932, 931, 930,\n        929, 928, 927, 926, 925, 924, 923, 922, 921, 920, 919, 918, 917, 916,\n        915, 914, 913, 912, 911, 910, 909, 908, 907, 906, 905, 904, 903, 902,\n        901, 900, 899, 898, 897, 896, 895, 894, 893, 892, 891, 890, 889, 888,\n        887, 886, 885, 884, 883, 882, 881, 880, 879, 878, 877, 876, 875, 874,\n        873, 872, 871, 870, 869, 868, 867, 866, 865, 864, 863, 862, 861, 860,\n        859, 858, 857, 856, 855, 854, 853, 852, 851, 850, 849, 848, 847, 846,\n        845, 844, 843, 842, 841, 840, 839, 838, 837, 836, 835, 834, 833, 832,\n        831, 830, 829, 828, 827, 826, 825, 824, 823, 822, 821, 820, 819, 818,\n        817, 816, 815, 814, 813, 812, 811, 810, 809, 808, 807, 806, 805, 804,\n        803, 802, 801, 800, 799, 798, 797, 796, 795, 794, 793, 792, 791, 790,\n        789, 788, 787, 786, 785, 784, 783, 782, 781, 780, 779, 778, 777, 776,\n        775, 774, 773, 772, 771, 770, 769, 768, 767, 766, 765, 764, 763, 762,\n        761, 760, 759, 758, 757, 756, 755, 754, 753, 752, 751, 750, 749, 748,\n        747, 746, 745, 744, 743, 742, 741, 740, 739, 738, 737, 736, 735, 734,\n        733, 732, 731, 730, 729, 728, 727, 726, 725, 724, 723, 722, 721, 720,\n        719, 718, 717, 716, 715, 714, 713, 712, 711, 710, 709, 708, 707, 706,\n        705, 704, 703, 702, 701, 700, 699, 698, 697, 696, 695, 694, 693, 692,\n        691, 690, 689, 688, 687, 686, 685, 684, 683, 682, 681, 680, 679, 678,\n        677, 676, 675, 674, 673, 672, 671, 670, 669, 668, 667, 666, 665, 664,\n        663, 662, 661, 660, 659, 658, 657, 656, 655, 654, 653, 652, 651, 650,\n        649, 648, 647, 646, 645, 644, 643, 642, 641, 640, 639, 638, 637, 636,\n        635, 634, 633, 632, 631, 630, 629, 628, 627, 626, 625, 624, 623, 622,\n        621, 620, 619, 618, 617, 616, 615, 614, 613, 612, 611, 610, 609, 608,\n        607, 606, 605, 604, 603, 602, 601, 600, 599, 598, 597, 596, 595, 594,\n        593, 592, 591, 590, 589, 588, 587, 586, 585, 584, 583, 582, 581, 580,\n        579, 578, 577, 576, 575, 574, 573, 572, 571, 570, 569, 568, 567, 566,\n        565, 564, 563, 562, 561, 560, 559, 558, 557, 556, 555, 554, 553, 552,\n        551, 550, 549, 548, 547, 546, 545, 544, 543, 542, 541, 540, 539, 538,\n        537, 536, 535, 534, 533, 532, 531, 530, 529, 528, 527, 526, 525, 524,\n        523, 522, 521, 520, 519, 518, 517, 516, 515, 514, 513, 512, 511, 510,\n        509, 508, 507, 506, 505, 504, 503, 502, 501, 500, 499, 498, 497, 496,\n        495, 494, 493, 492, 491, 490, 489, 488, 487, 486, 485, 484, 483, 482,\n        481, 480, 479, 478, 477, 476, 475, 474, 473, 472, 471, 470, 469, 468,\n        467, 466, 465, 464, 463, 462, 461, 460, 459, 458, 457, 456, 455, 454,\n        453, 452, 451, 450, 449, 448, 447, 446, 445, 444, 443, 442, 441, 440,\n        439, 438, 437, 436, 435, 434, 433, 432, 431, 430, 429, 428, 427, 426,\n        425, 424, 423, 422, 421, 420, 419, 418, 417, 416, 415, 414, 413, 412,\n        411, 410, 409, 408, 407, 406, 405, 404, 403, 402, 401, 400, 399, 398,\n        397, 396, 395, 394, 393, 392, 391, 390, 389, 388, 387, 386, 385, 384,\n        383, 382, 381, 380, 379, 378, 377, 376, 375, 374, 373, 372, 371, 370,\n        369, 368, 367, 366, 365, 364, 363, 362, 361, 360, 359, 358, 357, 356,\n        355, 354, 353, 352, 351, 350, 349, 348, 347, 346, 345, 344, 343, 342,\n        341, 340, 339, 338, 337, 336, 335, 334, 333, 332, 331, 330, 329, 328,\n        327, 326, 325, 324, 323, 322, 321, 320, 319, 318, 317, 316, 315, 314,\n        313, 312, 311, 310, 309, 308, 307, 306, 305, 304, 303, 302, 301, 300,\n        299, 298, 297, 296, 295, 294, 293, 292, 291, 290, 289, 288, 287, 286,\n        285, 284, 283, 282, 281, 280, 279, 278, 277, 276, 275, 274, 273, 272,\n        271, 270, 269, 268, 267, 266, 265, 264, 263, 262, 261, 260, 259, 258,\n        257, 256, 255, 254, 253, 252, 251, 250, 249, 248, 247, 246, 245, 244,\n        243, 242, 241, 240, 239, 238, 237, 236, 235, 234, 233, 232, 231, 230,\n        229, 228, 227, 226, 225, 224, 223, 222, 221, 220, 219, 218, 217, 216,\n        215, 214, 213, 212, 211, 210, 209, 208, 207, 206, 205, 204, 203, 202,\n        201, 200, 199, 198, 197, 196, 195, 194, 193, 192, 191, 190, 189, 188,\n        187, 186, 185, 184, 183, 182, 181, 180, 179, 178, 177, 176, 175, 174,\n        173, 172, 171, 170, 169, 168, 167, 166, 165, 164, 163, 162, 161, 160,\n        159, 158, 157, 156, 155, 154, 153, 152, 151, 150, 149, 148, 147, 146,\n        145, 144, 143, 142, 141, 140, 139, 138, 137, 136, 135, 134, 133, 132,\n        131, 130, 129, 128, 127, 126, 125, 124, 123, 122, 121, 120, 119, 118,\n        117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104,\n        103, 102, 101, 100,  99,  98,  97,  96,  95,  94,  93,  92,  91,  90,\n         89,  88,  87,  86,  85,  84,  83,  82,  81,  80,  79,  78,  77,  76,\n         75,  74,  73,  72,  71,  70,  69,  68,  67,  66,  65,  64,  63,  62,\n         61,  60,  59,  58,  57,  56,  55,  54,  53,  52,  51,  50,  49,  48,\n         47,  46,  45,  44,  43,  42,  41,  40,  39,  38,  37,  36,  35,  34,\n         33,  32,  31,  30,  29,  28,  27,  26,  25,  24,  23,  22,  21,  20,\n         19,  18,  17,  16,  15,  14,  13,  12,  11,  10,   9,   8,   7,   6,\n          5,   4,   3,   2,   1,   0])\n\n\nThe first thing we need to do is choose an image that we want to use as a starting point. I chose to go with a picture that was similar to one of the images used in the paper, but maybe a little harder.\n\np = FastDownload().download('https://negativespace.co/wp-content/uploads/2020/11/negative-space-horses-in-field-with-trees-1062x705.jpg')\ninit_image = Image.open(p).convert(\"RGB\")\n# init_image = init_image.resize((init_image.size[0]//2, init_image.size[1]//2))\ninit_image = init_image.resize((512,512))\ninit_image\n\n\n\n\n\n\n    \n      \n      -819200.00% [8192/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -1638400.00% [16384/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -2457600.00% [24576/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -3276800.00% [32768/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -4096000.00% [40960/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -4915200.00% [49152/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -5734400.00% [57344/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -6553600.00% [65536/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -7372800.00% [73728/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -8192000.00% [81920/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -9011200.00% [90112/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -9830400.00% [98304/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -10649600.00% [106496/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -11468800.00% [114688/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -12288000.00% [122880/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -13107200.00% [131072/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -13926400.00% [139264/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -14745600.00% [147456/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -15564800.00% [155648/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -16384000.00% [163840/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -17203200.00% [172032/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -18022400.00% [180224/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -18841600.00% [188416/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -19660800.00% [196608/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -20480000.00% [204800/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -21299200.00% [212992/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -22118400.00% [221184/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -22937600.00% [229376/-1 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      -23756800.00% [237568/-1 00:00<00:00]\n    \n    \n\n\n\n\n\nNow that I have found an image, let’s define the reference_text and the query_text. These are defined in the paper as R and Q. Let’s follow the paper here and keep Q and R simple.\n\nreference_text = \"Two horses\"\nquery_text = \"Two zebras\"\n\nA good amount of the code in the next few cells is coming from the StableDiffusionImg2ImgPipeline function in the diffusers library. This was very helpful in creating the implementation I ended up with\n\ndef preprocess(image):\n    w, h = image.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    image = image.resize((w, h), resample=PIL.Image.Resampling.LANCZOS)\n    image = np.array(image).astype(np.float32) / 255.0\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image)\n    return 2.0 * image - 1.0\n\n\ndef get_text_embeddings(prompt, negative_prompt, tokenizer, text_encoder, do_classifier_free_guidance, device): #outputs text_embeddings\n    # get prompt text embeddings\n    text_inputs = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, \n                            return_tensors=\"pt\", truncation=True)\n    text_input_ids = text_inputs.input_ids\n    text_embeddings = text_encoder(text_input_ids.to(device))[0]\n    # text_embeddings = text_embeddings.repeat_interleave(num_images_per_prompt, dim=0)\n    \n    if negative_prompt is None:\n        uncond_tokens = [\"\"]\n    else:\n        uncond_tokens = negative_prompt\n    max_length = text_input_ids.shape[-1]\n    uncond_input = tokenizer(uncond_tokens, padding=\"max_length\", max_length=max_length, \n                             return_tensors=\"pt\", truncation=True)\n    with torch.no_grad():\n        uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]\n\n    # duplicate unconditional embeddings for each generation per prompt\n    # uncond_embeddings = uncond_embeddings.repeat_interleave(batch_size * num_images_per_prompt, dim=0)\n    \n    # For classifier free guidance, we need to do two forward passes.\n    # Here we concatenate the unconditional and text embeddings into a single batch\n    # to avoid doing two forward passes\n    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n    return text_embeddings\n\n\ndef get_timestamps(scheduler, num_inference_steps, strength, device):\n    scheduler.set_timesteps(num_inference_steps)\n\n    # get the original timestep using init_timestep\n    offset = scheduler.config.get(\"steps_offset\", 0)\n    init_timestep = int(num_inference_steps * strength) + offset\n    init_timestep = min(init_timestep, num_inference_steps)\n\n    timesteps = scheduler.timesteps[-init_timestep]\n    timesteps = torch.tensor([timesteps], device=device)\n    t_start = max(num_inference_steps - init_timestep + offset, 0)\n    return timesteps, t_start\n\n\ndef encode_image(init_image, latents_dtype, device):\n    # encode the init image into latents and scale the latents\n    init_image = preprocess(init_image)\n    init_image = init_image.to(device=device, dtype=latents_dtype)\n    with torch.no_grad(): init_latent_dist = vae.encode(init_image).latent_dist\n    init_latents = init_latent_dist.sample(generator=generator)\n    init_latents = 0.18215 * init_latents\n    return init_latents\n\n\ndef img2noise(init_image, \n              prompt,\n              mask=None,\n              strength = 0.5,\n              num_inference_steps = 50,\n              guidance_scale = 5,\n              negative_prompt=None,\n              generator = None, \n              output_type = \"pil\", \n              return_dict = True, \n              callback = None, \n              callback_steps = 1, \n              device='cuda'\n             ):\n    do_classifier_free_guidance = guidance_scale > 1.0\n    text_embeddings = get_text_embeddings(prompt, negative_prompt, tokenizer, text_encoder, do_classifier_free_guidance, device)\n    latents_dtype=text_embeddings.dtype\n    timesteps, t_start = get_timestamps(scheduler, num_inference_steps, strength, device)\n    \n    # encode the init image into latents and scale the latents\n    init_latents = encode_image(init_image, latents_dtype, device)\n\n    # add noise to latents using the timesteps\n    noise = torch.randn(init_latents.shape, generator=generator, device=device, dtype=latents_dtype)\n    noisy_latents = scheduler.add_noise(init_latents, noise, timesteps)\n\n    latents = noisy_latents\n\n    # Some schedulers like PNDM have timesteps as arrays\n    # It's more optimized to move all timesteps to correct device beforehand\n    timesteps = scheduler.timesteps[t_start:].to(device)\n    noise_preds = torch.tensor([], device='cuda')\n    print(timesteps)\n    for i, t in enumerate(timesteps):\n        # expand the latents if we are doing classifier free guidance\n        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n        # predict the noise residual\n        with torch.no_grad():\n            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\n        # perform guidance\n        if do_classifier_free_guidance:\n            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n        \n        noise_preds = torch.concat((noise_preds, noise_pred)) #This performs much worse when outside the for-loop\n        # compute the previous noisy sample x_t -> x_t-1\n        latent_step = scheduler.step(noise_pred, t, latents)\n        latents = latent_step.prev_sample \n        if mask is not None: \n            latents = mask*latents+(1-mask)*init_latents\n            \n    latents = 1 / 0.18215 * latents\n    with torch.no_grad(): image = vae.decode(latents).sample\n\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images, noise_preds\n\n\n\nEstimate noise conditioned to Reference Text R\n\ngenerator = torch.cuda.manual_seed(32)\nreference_noises = torch.tensor([], device='cuda')\nfor _ in range(10):\n    reference_pil, reference_noise = img2noise(init_image, strength=0.5, prompt=reference_text, generator=generator)\n    reference_noises = torch.concat((reference_noises, reference_noise))\n\ntensor([489.3061, 468.9184, 448.5306, 428.1429, 407.7551, 387.3673, 366.9796,\n        346.5918, 326.2041, 305.8163, 285.4286, 265.0408, 244.6531, 224.2653,\n        203.8776, 183.4898, 163.1020, 142.7143, 122.3265, 101.9388,  81.5510,\n         61.1633,  40.7755,  20.3878,   0.0000], device='cuda:0',\n       dtype=torch.float64)\ntensor([489.3061, 468.9184, 448.5306, 428.1429, 407.7551, 387.3673, 366.9796,\n        346.5918, 326.2041, 305.8163, 285.4286, 265.0408, 244.6531, 224.2653,\n        203.8776, 183.4898, 163.1020, 142.7143, 122.3265, 101.9388,  81.5510,\n         61.1633,  40.7755,  20.3878,   0.0000], device='cuda:0',\n       dtype=torch.float64)\ntensor([489.3061, 468.9184, 448.5306, 428.1429, 407.7551, 387.3673, 366.9796,\n        346.5918, 326.2041, 305.8163, 285.4286, 265.0408, 244.6531, 224.2653,\n        203.8776, 183.4898, 163.1020, 142.7143, 122.3265, 101.9388,  81.5510,\n         61.1633,  40.7755,  20.3878,   0.0000], device='cuda:0',\n       dtype=torch.float64)\ntensor([489.3061, 468.9184, 448.5306, 428.1429, 407.7551, 387.3673, 366.9796,\n        346.5918, 326.2041, 305.8163, 285.4286, 265.0408, 244.6531, 224.2653,\n        203.8776, 183.4898, 163.1020, 142.7143, 122.3265, 101.9388,  81.5510,\n         61.1633,  40.7755,  20.3878,   0.0000], device='cuda:0',\n       dtype=torch.float64)\ntensor([489.3061, 468.9184, 448.5306, 428.1429, 407.7551, 387.3673, 366.9796,\n        346.5918, 326.2041, 305.8163, 285.4286, 265.0408, 244.6531, 224.2653,\n        203.8776, 183.4898, 163.1020, 142.7143, 122.3265, 101.9388,  81.5510,\n         61.1633,  40.7755,  20.3878,   0.0000], device='cuda:0',\n       dtype=torch.float64)\ntensor([489.3061, 468.9184, 448.5306, 428.1429, 407.7551, 387.3673, 366.9796,\n        346.5918, 326.2041, 305.8163, 285.4286, 265.0408, 244.6531, 224.2653,\n        203.8776, 183.4898, 163.1020, 142.7143, 122.3265, 101.9388,  81.5510,\n         61.1633,  40.7755,  20.3878,   0.0000], device='cuda:0',\n       dtype=torch.float64)\ntensor([489.3061, 468.9184, 448.5306, 428.1429, 407.7551, 387.3673, 366.9796,\n        346.5918, 326.2041, 305.8163, 285.4286, 265.0408, 244.6531, 224.2653,\n        203.8776, 183.4898, 163.1020, 142.7143, 122.3265, 101.9388,  81.5510,\n         61.1633,  40.7755,  20.3878,   0.0000], device='cuda:0',\n       dtype=torch.float64)\ntensor([489.3061, 468.9184, 448.5306, 428.1429, 407.7551, 387.3673, 366.9796,\n        346.5918, 326.2041, 305.8163, 285.4286, 265.0408, 244.6531, 224.2653,\n        203.8776, 183.4898, 163.1020, 142.7143, 122.3265, 101.9388,  81.5510,\n         61.1633,  40.7755,  20.3878,   0.0000], device='cuda:0',\n       dtype=torch.float64)\ntensor([489.3061, 468.9184, 448.5306, 428.1429, 407.7551, 387.3673, 366.9796,\n        346.5918, 326.2041, 305.8163, 285.4286, 265.0408, 244.6531, 224.2653,\n        203.8776, 183.4898, 163.1020, 142.7143, 122.3265, 101.9388,  81.5510,\n         61.1633,  40.7755,  20.3878,   0.0000], device='cuda:0',\n       dtype=torch.float64)\ntensor([489.3061, 468.9184, 448.5306, 428.1429, 407.7551, 387.3673, 366.9796,\n        346.5918, 326.2041, 305.8163, 285.4286, 265.0408, 244.6531, 224.2653,\n        203.8776, 183.4898, 163.1020, 142.7143, 122.3265, 101.9388,  81.5510,\n         61.1633,  40.7755,  20.3878,   0.0000], device='cuda:0',\n       dtype=torch.float64)\n\n\n\nscheduler.step??\n\n\n\nEstimate noise conditioned to Query Q\n\ngenerator = torch.cuda.manual_seed(32)\nquery_noises = torch.tensor([], device='cuda')\nfor _ in range(10):\n    query_pil, query_noise = img2noise(init_image, strength=0.5, prompt=query_text, generator=generator)\n    query_noises = torch.concat((query_noises, query_noise))\n\n\n\nView Latent Noise Channels\n\nfig, axs = plt.subplots(1, 4, figsize=(16, 4))\nfor c in range(4):\n    axs[c].imshow(reference_noises.mean(0, keepdim=True)[0][c].cpu())\n\n\n\n\n\nfig, axs = plt.subplots(1, 4, figsize=(16, 4))\nfor c in range(4):\n    axs[c].imshow(query_noises.mean(0, keepdim=True)[0][c].cpu())\n\n\n\n\nWhile there isn’t much that looks interesting when looking at the reference_noises or query_noises individually, let’s look at the difference between the two.\n\ndiff_noises = (reference_noises.mean(0, keepdim=True) - query_noises.mean(0, keepdim=True))\n\n\nfig, axs = plt.subplots(1, 4, figsize=(16, 4))\nfor c in range(4):\n    axs[c].imshow(diff_noises[0][c].cpu())\n\n\n\n\nNow, we are seeing some signs that the noise that is being removed is quite different over the horse area of the picture and pretty similar outside of that area. One thing to note on these channels is that some of them are darker surrounding the horses and some are lighter.\n\ndiff_noises.min(), diff_noises.max()\n\n(tensor(-0.8503, device='cuda:0'), tensor(0.6133, device='cuda:0'))\n\n\nOne thing I’ve found improves this is to determine the distance away from the midpoint, so I take the absolute value to make sure the intensity, not the direction, is being taken into consideration. The thought here is that whether the zebra query or the horse reference is activating that noise a lot, it is probably a pixel we should include in the mask.\n\ndiff_noises_abs = diff_noises.abs()\n\n\nfig, axs = plt.subplots(1, 4, figsize=(16, 4))\nfor c in range(4):\n    axs[c].imshow(diff_noises_abs[0][c].cpu())#, cmap='Greys')\n\n\n\n\nNow, let’s scale all of our values to be between 0 and 1 by subtracting the smallest value and then dividing everything by the largest value. I am not removing any values when doing this although I will note here that the paper does say to “remove extreme values in noise predictions”. I wasn’t sure how to go about doing this.\n\ndiff_noise_normed = (diff_noises_abs - diff_noises_abs.min())/(diff_noises_abs - diff_noises_abs.min()).max()\n\n\ndiff_noise_normed.min(), diff_noise_normed.max()\n\n(tensor(0., device='cuda:0'), tensor(1., device='cuda:0'))\n\n\n\nmask = diff_noise_normed.mean(dim=1).squeeze()\n\nThe next step is to binarize the mask so that everything is either a 0 or a 1. To acheive this, I used Otsu thresholding which I believe deviates from the version that is used in the paper which seems more straightforward: “binarized with a threshold, which we set to 0.5 by default”. I was never able to get a mask that I was happy with when using that strategy.\n\ndef extract_channel_mask(img, do_inverse=False):\n    kernel = np.ones((3,3),np.uint8)\n    img = (img*255).squeeze().cpu().to(torch.uint8).numpy()\n    if do_inverse:\n        ret2,img2 = cv2.threshold(img,0,1,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n    else:\n        ret2,img2 = cv2.threshold(img,0,1,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n    dilate = cv2.dilate(img2, kernel)\n    return dilate\n\n\nfinal_mask = extract_channel_mask(mask)\n\n\nplt.imshow(final_mask)\n\n<matplotlib.image.AxesImage at 0x7f3d61067e20>\n\n\n\n\n\nSince our mask is generated in Latent Space, we have to find a way to go from latent space back to the original image space. For this, I decided to use F.interpolate.\n\nhorse_sized_mask = F.interpolate(torch.from_numpy(final_mask).unsqueeze(0).unsqueeze(0), mode='nearest', size=(init_image.size[1],init_image.size[0])).squeeze()\n\nThe mask came out looking pretty good for this easy prompt. The horses are definitely well masked and seem to match what would intuitively make sense. I have done some other prompts that didn’t look good including trying to change the grass into sand and changing the forest into a mushroom forest. Both of these included the horses in the mask still.\n\nplt.imshow(init_image)\nplt.imshow(horse_sized_mask.squeeze().cpu(), alpha=0.5)\n\n<matplotlib.image.AxesImage at 0x7f3d60fadd90>\n\n\n\n\n\nMost of the interesting results from this paper are coming from step 1. The other two steps are hard to really build up to, but I will share them here.\nStep 2: Encoding is replicating the step that was done before where we add noise back into the latent. This is already being done so there was no new code that was needed to accomplish this. This is the code where that step occurs:\n    # add noise to latents using the timesteps\n    noise = torch.randn(init_latents.shape, generator=generator, device=device, dtype=latents_dtype)\n    noisy_latents = scheduler.add_noise(init_latents, noise, timesteps)\nStep 3: Decode with mask-wise correction is interesting, but also a step that doesn’t add much code. The idea of step 3 is that anything outside of the mask that was generated from step 1 will be reverted to the original pixel value. So only the pixels that are inside of the masked areas will be changed which helps accomplish the goal of keeping most of the original image the same as before the prompt.\nHere is the equation of the concept I described: \\(\\tilde{y}_t = M*y_t + (1−M)*x_t\\) where \\(y_t\\) is the new image and \\(x_t\\) is the original image (both encoded in latent space).\nLet’s look at the results:\n\npipe = StableDiffusionInpaintPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\",\n    revision=\"fp16\",\n    torch_dtype=torch.float16,\n).to(torch_device)\n\n\n\n\n\nplt.imshow(torch.from_numpy((final_mask)).cuda().float().cpu())\n\n<matplotlib.image.AxesImage at 0x7f3d80ee4610>\n\n\n\n\n\n\ndef image_grid(imgs, rows, cols):\n    assert len(imgs) == rows*cols\n\n    w, h = imgs[0].size\n    grid = PIL.Image.new('RGB', size=(cols*w, rows*h))\n    grid_w, grid_h = grid.size\n    \n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\n\nfor i in range(0,20):\n    generator = torch.cuda.manual_seed(i)\n#     final_pil, _= img2noise(init_image, prompt=query_text, mask=torch.from_numpy((final_mask)).cuda(), generator=generator, strength=0.5)\n    final_pil = pipe(prompt=query_text, image=init_image, mask_image=Image.fromarray(255*horse_sized_mask.numpy()), num_images_per_prompt=1).images\n    print(i)\n    display(final_pil[0])\n\n\n\n\n0\n\n\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\n\n2\n\n\n\n\n\n\n\n\n3\n\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n\n\n5\n\n\n\n\n\n\n\n\n6\n\n\n\n\n\n\n\n\n7\n\n\n\n\n\n\n\n\n8\n\n\n\n\n\n\n\n\n9\n\n\n\n\n\n\n\n\n10\n\n\n\n\n\n\n\n\n11\n\n\n\n\n\n\n\n\n12\n\n\n\n\n\n\n\n\n13\n\n\n\n\n\n\n\n\n14\n\n\n\n\n\n\n\n\n15\n\n\n\n\n\n\n\n\n16\n\n\n\n\n\n\n\n\n17\n\n\n\n\n\n\n\n\n18\n\n\n\n\n\n\n\n\n19\n\n\n\n\n\nOverall, it seems that the process is doing what we would expect. I am seeing some decent results, but there is definitely still room for innovation in this space. If you noticed any typos or have any ideas for improving this blog post, please reach out to kevin@problemsolversguild.com and let me know! Thank you for reading and I hope this has been helpful in your learning process."
  },
  {
    "objectID": "posts/2022-02-15-SSH-key.html",
    "href": "posts/2022-02-15-SSH-key.html",
    "title": "Create SSH key pair and using it",
    "section": "",
    "text": "It is fairly simple to create a SSH key pair as ssh-keygen will guide you through the whole process.\nssh-keygen -t ed25519 -C \"sunny@example.com\"\n-C is a comment that will be added at the end of the public key.\nThe series of questions after you run the above code looks something like:\nGenerating public/private ed25519 key pair.\nEnter file in which to save the key (/home/sunny/.ssh/id_ed25519):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYou can choose to use all default values or enter your own.\nOnce it is complete: - The public key is now located in /home/sunny/.ssh/id_ed25519.pub - The private key is now located in /home/sunny/.ssh/id_ed25519"
  },
  {
    "objectID": "posts/2022-02-15-SSH-key.html#copy-public-key-to-the-remote-server",
    "href": "posts/2022-02-15-SSH-key.html#copy-public-key-to-the-remote-server",
    "title": "Create SSH key pair and using it",
    "section": "2. Copy public key to the remote server:",
    "text": "2. Copy public key to the remote server:\nLet’s say, the remote server is running SSH on a non-standard port. If your remote server is using the standard port, you can omit the port settings:\nssh-copy-id -i ~/.ssh/id_ed25519 -p 123 sunny@remote_server_address \n.pub is automatically added if you specify a filename that does not end with .pub.\nThis will add your public key to the remote server’s ~/.ssh/authorized_keys file. If you open it up, you will see the comment you added when generating via -C flag in step 1."
  },
  {
    "objectID": "posts/2022-02-15-SSH-key.html#add-private-key-to-the-ssh-agent",
    "href": "posts/2022-02-15-SSH-key.html#add-private-key-to-the-ssh-agent",
    "title": "Create SSH key pair and using it",
    "section": "3. Add private key to the SSH agent:",
    "text": "3. Add private key to the SSH agent:\n[Optional] Start the ssh-agent if it isn’t already. If you are not sure, skip it for now and come back if the next step fails.\neval \"$(ssh-agent -s)\"\nAdd your SSH private key to the ssh-agent.\nssh-add ~/.ssh/id_ed25519\nIf you are curious which keys ssh-agent knows about, run:\nssh-add -L"
  },
  {
    "objectID": "posts/2022-02-15-SSH-key.html#add-the-remote-host-to-ssh-config-just-for-convenience",
    "href": "posts/2022-02-15-SSH-key.html#add-the-remote-host-to-ssh-config-just-for-convenience",
    "title": "Create SSH key pair and using it",
    "section": "4. Add the remote host to SSH config (just for convenience):",
    "text": "4. Add the remote host to SSH config (just for convenience):\nFor simplicity, you can add the remote host to your SSH config file (~/.ssh/config). If this file does not exist, you can create it manually (touch ~/.ssh/config).\nAdd a block like this:\nHost remote_server_name\n        HostName 123.45.678.90\n        User sunny\n        IdentityFile ~/.ssh/id_ed25519\n        Port 123\nNow you can SSH to the server by:\nssh remote_server_name\nInstead of:\nssh -i ~/.ssh/id_ed25519 -p 123 sunny@123.45.678.90"
  },
  {
    "objectID": "posts/2022-02-15-SSH-key.html#references",
    "href": "posts/2022-02-15-SSH-key.html#references",
    "title": "Create SSH key pair and using it",
    "section": "References",
    "text": "References\n\nhttps://www.ssh.com/academy/ssh/copy-id\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\nhttps://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys-2\nhttps://linuxize.com/post/using-the-ssh-config-file/"
  },
  {
    "objectID": "posts/2022-02-22-exploring-torch.lerp.html",
    "href": "posts/2022-02-22-exploring-torch.lerp.html",
    "title": "PyTorch torch.lerp Exploration",
    "section": "",
    "text": "torch.lerp stands for linear interpolation is a handy function that combines two tensors using a provided weight. Let’s explore how it can be used!\n\nimport torch\n\nLet’s start off with something easy. We have two items and we want to combine them by taking 75% of item1 and 25% of item2. Mathematically, this could be represented as \\(output = 0.75*item1+0.25*item2\\). A more general form of this can be represented as \\(output = pct*item1+(1-pct)*item2\\). This is a very common piece of code in machine learning papers. That’s why pytorch has the handy torch.lerp function!\n\nitem1 = torch.tensor(2.)\nitem2 = torch.tensor(6.)\nweight = 1/4 # This means that we will use 3/4 of item1 and 1/4 of item2\n\n\noutput1 = (1-weight)*item1+(weight)*item2\n\n\noutput2 = torch.lerp(item1, item2, weight)\n\n\noutput1\n\ntensor(3.)\n\n\n\noutput2\n\ntensor(3.)\n\n\nHere is an example in the mixup paper of lerp being used in practice:\n\n\nimport matplotlib.pyplot as plt\n\n\nnp_april = plt.imread('notebook_images/pets/april.jpg')\napril = torch.from_numpy(np_april)\n\n\napril_smaller = april[600:600+1224,1100:1100+1124,:]/255.\n\n\nx_i = april_smaller #simulated image #1\nx_j = torch.rand_like(x_i) #simulated image #2\nlam=0.1 # Let's set lam to 0.5 which will blend equal parts of xi and xj.\n\nNow, let’s blend these two ‘images’\n\nfig, axs = plt.subplots(ncols=5, sharey=True, figsize=(18,3))\nfor i,lam in enumerate([0, 0.25, 0.5, 0.75, 1]):\n    x_hat = torch.lerp(x_j,x_i,lam)\n    axs[i].set_title(f'{i}:λ={lam}')\n    axs[i].imshow(x_hat)\n\n\n\n\n\nfrom fastcore.all import test_close, test_eq\n\n\nx_hat = torch.lerp(x_j,x_i,weight=0.5)\n\n\ntest_close((x_j + x_i)/2, x_hat, eps=1e-6)\n\nAs we expected, these two are equal (within a small amount of error due to float math)\nLinear interpolation is also often used in exponentially weighted decay which allows us to not entirely discard previous weight results while only keeping track of the most recent value.\nHere is what exponential weighted decay looks like in the Adam Optimizer formula:\n)\nThis algorithm actually contains two linear interpolations:\n\\(m_t = \\beta_1*m_{t-1}+(1-\\beta_1)*g_t\\)\n\\(v_t = \\beta_2*v_{t-1}+(1-\\beta_2)*g_t^2\\)\nand here is what they look like in code:\n\nm_tm1=torch.tensor(0.)\nv_tm1=torch.tensor(0.)\ng_t = torch.tensor(0.5)\nbeta1=torch.tensor(0.99)\nbeta2=torch.tensor(0.999)\n\n\nm_t = torch.lerp(m_tm1, g_t, beta1)\nv_t = torch.lerp(v_tm1, g_t**2, beta2)\n\n\nm_t\n\ntensor(0.4950)\n\n\n\nv_t\n\ntensor(0.2498)\n\n\nHope this was helpful and gave a better understanding of what torch.lerp is and where it is used. If you have any suggestions or questions, please feel free to reach out and I would be happy to address them!"
  },
  {
    "objectID": "posts/2021-05-14-text_dls_exploration.html",
    "href": "posts/2021-05-14-text_dls_exploration.html",
    "title": "fastai’s Text DLs Exploration",
    "section": "",
    "text": "This post is an exploration into how to convert data into dls objects that can be used by fastai’s learner. I was having issues creating a dls object that had the ability to show_batch and met my arbitrarily custom needs. So I set out to figure out how to create dls that worked well for my needs.\nThis blog post uses the Human Numbers dataset which is a dataset that counts sequentially from 1 to 9999 but in english text rather than numerical form. This is an interesting problem because there is quite a bit of repetition, but also new tokens and patterns being introduced regularly that a model will need to figure out.\nMy goal was to create a dls that would have X=1,2,3 and y=4. Over the course of this blog post, I will show ~4 ways to create dls that enable show_batch to work as expected.\n\nfrom fastai.text.all import *\n\n\nimport fastai, fastcore\n\n\nfastai.__version__,fastcore.__version__\n\n('2.3.1', '1.3.20')\n\n\n\npath = untar_data(URLs.HUMAN_NUMBERS)\n\nFirst, I create a tokenizer, combine all of the text into a single string, and tokenize each word\n\ntokenizer = Tokenizer(WordTokenizer())\n\nTesting the Tokenizer\n\ntokenizer('one two three')\n\n(#4) ['xxbos','one','two','three']\n\n\n\ntokenizer('one, two')\n\n(#4) ['xxbos','one',',','two']\n\n\nReading the train and validation files:\n* train - [1-8000] * valid = [8001-9999]\n\ntrain_txt = ', '.join(o.strip() for o in (path/'train.txt').readlines())\nvalid_txt = ', '.join(o.strip() for o in (path/'valid.txt').readlines())\n\nFor this problem, I will create my own validation set. It will split close to the same as this, but by creating my own split, I don’t have to do anything special when creating chunks around the train->validation split point\n\nall_text = train_txt+valid_txt\n\n\nall_text_tok = tokenizer(all_text)\n\n\nall_text_tok\n\n(#63094) ['xxbos','one',',','two',',','three',',','four',',','five'...]\n\n\nNext, I take the tokenized text, count how many times each tokenizer occurs and create a vocab with that.\n\ncount=Counter(all_text_tok)\nvocab = make_vocab(count)\n\n\nprint(count)\n\nCounter({',': 9996, 'hundred': 9000, 'thousand': 8999, 'one': 2900, 'two': 2900, 'three': 2900, 'four': 2900, 'five': 2900, 'six': 2900, 'seven': 2900, 'nine': 2899, 'eight': 2898, 'twenty': 1000, 'thirty': 1000, 'forty': 1000, 'fifty': 1000, 'sixty': 1000, 'seventy': 1000, 'eighty': 1000, 'ninety': 1000, 'ten': 100, 'eleven': 100, 'twelve': 100, 'thirteen': 100, 'fourteen': 100, 'fifteen': 100, 'sixteen': 100, 'seventeen': 100, 'eighteen': 100, 'nineteen': 100, 'xxbos': 1, 'nineeight': 1})\n\n\n\nprint(vocab)\n\n['xxunk', 'xxpad', 'xxbos', 'xxeos', 'xxfld', 'xxrep', 'xxwrep', 'xxup', 'xxmaj', ',', 'hundred', 'thousand', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'nine', 'eight', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen', 'seventeen', 'eighteen', 'nineteen', 'xxfake']\n\n\n\nall_text_tok_chunked = list(chunked(all_text_tok, 11))\n\n\n#drop last non-full row\nall_text_tok_chunked = all_text_tok_chunked[:-1]\n\nNext I create something that will get_x and get_y from the chunked data.\n\ndef get_x(o):\n    return o[:10]\n\ndef get_y(o):\n    return [o[10]] if len(o) == 11 else ['.']\n\n\nprint(f\"{get_x(all_text_tok_chunked[0])} -> {get_y(all_text_tok_chunked[0])}\")\n\n['xxbos', 'one', ',', 'two', ',', 'three', ',', 'four', ',', 'five'] -> [',']\n\n\n\nprint(f\"{get_x(all_text_tok_chunked[-1])} -> {get_y(all_text_tok_chunked[-1])}\")\n\n['nine', 'thousand', 'nine', 'hundred', 'ninety', 'seven', ',', 'nine', 'thousand', 'nine'] -> ['hundred']\n\n\nTitledStringDecoder is a transform that only decodes and what it enables is the show_batch and show_results function to actually work properly. Without this, I had troubles getting those functions to work because TensorText doesn’t have a proper show function or a truncate function.\n\nclass TitledStringDecoder(Transform):\n    def decodes(self, o):\n        return TitledStr(' '.join(o))\n\nAll TitledStringDecoder really does is takes an array of text (‘one’, ‘two’) and converts it into a space-concatenated string instead of type Titled str which knows how to display itself in a nice way.\n\nTitledStr(' '.join(['one', 'two']))\n\n'one two'\n\n\n\ntmp_ts = TitledStr(' '.join(all_text_tok[:10]))\n\n\ntmp_ts\n\n'xxbos one , two , three , four , five'\n\n\n\ntmp_ts.truncate(3)\n\n'xxbos one ,'\n\n\nI create the splits based off the chunks. Putting 80% of the chunks into the training set and the last 20% in the validation set\n\nsplits = [L(range(int(len(all_text_tok_chunked)*0.8))), L(range(int(len(all_text_tok_chunked)*0.8),len(all_text_tok_chunked)))]\n\n\nsplits\n\n[(#4588) [0,1,2,3,4,5,6,7,8,9...],\n (#1147) [4588,4589,4590,4591,4592,4593,4594,4595,4596,4597...]]\n\n\nNow, let’s test the transforms work properly\n\nNumericalize(vocab)(TitledStringDecoder()(get_x(all_text_tok_chunked[0])))\n\nTensorText([ 2, 12,  9, 13,  9, 14,  9, 15,  9, 16])\n\n\nAnd confirm that they will work as a pipeline as well\n\npipeline = Pipeline([TitledStringDecoder, Numericalize(vocab)])\n\n\nget_x(pipeline(all_text_tok_chunked[0]))\n\nTensorText([ 2, 12,  9, 13,  9, 14,  9, 15,  9, 16])\n\n\n\npipeline_x = Pipeline([get_x, TitledStringDecoder, Numericalize(vocab)])\npipeline_y = Pipeline([get_y, TitledStringDecoder, Numericalize(vocab)])\n\n\npipeline_y(all_text_tok_chunked[0])\n\nTensorText([9])"
  },
  {
    "objectID": "posts/2021-05-14-text_dls_exploration.html#using-datasets-dataloaders",
    "href": "posts/2021-05-14-text_dls_exploration.html#using-datasets-dataloaders",
    "title": "fastai’s Text DLs Exploration",
    "section": "Using Datasets + Dataloaders",
    "text": "Using Datasets + Dataloaders\n\ndsets = Datasets(all_text_tok_chunked, tfms=[pipeline_x,pipeline_y], splits=splits)\n\n\ndsets[0]\n\n(TensorText([ 2, 12,  9, 13,  9, 14,  9, 15,  9, 16]), TensorText([9]))\n\n\n\ndsets.show(dsets[0])\n\nxxbos one , two , three , four , five\n,\n\n\nNext, we can create the dataloaders. This can be done with either DataLoaders.from_dsets(...) or dsets.dataloaders(...). Both methods are shown below.\n\ndls = DataLoaders.from_dsets(dsets, shuffle=False, drop_last=True)\n\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos one , two , three , four , five\n      ,\n    \n    \n      1\n      six , seven , eight , nine , ten ,\n      eleven\n    \n    \n      2\n      , twelve , thirteen , fourteen , fifteen , sixteen\n      ,\n    \n    \n      3\n      seventeen , eighteen , nineteen , twenty , twenty one\n      ,\n    \n    \n      4\n      twenty two , twenty three , twenty four , twenty\n      five\n    \n    \n      5\n      , twenty six , twenty seven , twenty eight ,\n      twenty\n    \n    \n      6\n      nine , thirty , thirty one , thirty two ,\n      thirty\n    \n    \n      7\n      three , thirty four , thirty five , thirty six\n      ,\n    \n    \n      8\n      thirty seven , thirty eight , thirty nine , forty\n      ,\n    \n  \n\n\n\n\ndls = dsets.dataloaders(bs=16, shuffle=False, drop_last=True)\n\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos one , two , three , four , five\n      ,\n    \n    \n      1\n      six , seven , eight , nine , ten ,\n      eleven\n    \n    \n      2\n      , twelve , thirteen , fourteen , fifteen , sixteen\n      ,\n    \n    \n      3\n      seventeen , eighteen , nineteen , twenty , twenty one\n      ,\n    \n    \n      4\n      twenty two , twenty three , twenty four , twenty\n      five\n    \n    \n      5\n      , twenty six , twenty seven , twenty eight ,\n      twenty\n    \n    \n      6\n      nine , thirty , thirty one , thirty two ,\n      thirty\n    \n    \n      7\n      three , thirty four , thirty five , thirty six\n      ,\n    \n    \n      8\n      thirty seven , thirty eight , thirty nine , forty\n      ,"
  },
  {
    "objectID": "posts/2021-05-14-text_dls_exploration.html#using-datasets---train-tfmddl-valid-tfmddl---dataloaders",
    "href": "posts/2021-05-14-text_dls_exploration.html#using-datasets---train-tfmddl-valid-tfmddl---dataloaders",
    "title": "fastai’s Text DLs Exploration",
    "section": "Using Datasets -> train TfmdDL + valid TfmdDL -> dataloaders",
    "text": "Using Datasets -> train TfmdDL + valid TfmdDL -> dataloaders\nAnother way to get dls is to create TfmdDLs and pass those into DataLoaders. If you use DataLoader rather than TfmdDL, dls won’t have a show_batch method available.\n\ntrain_dl = TfmdDL(dsets.train, bs=16, drop_last=True)\n\n\nvalid_dl = TfmdDL(dsets.valid, bs=16, drop_last=True)\n\n\ndls = DataLoaders(train_dl, valid_dl)\n\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos one , two , three , four , five\n      ,\n    \n    \n      1\n      six , seven , eight , nine , ten ,\n      eleven\n    \n    \n      2\n      , twelve , thirteen , fourteen , fifteen , sixteen\n      ,\n    \n    \n      3\n      seventeen , eighteen , nineteen , twenty , twenty one\n      ,\n    \n    \n      4\n      twenty two , twenty three , twenty four , twenty\n      five\n    \n    \n      5\n      , twenty six , twenty seven , twenty eight ,\n      twenty\n    \n    \n      6\n      nine , thirty , thirty one , thirty two ,\n      thirty\n    \n    \n      7\n      three , thirty four , thirty five , thirty six\n      ,\n    \n    \n      8\n      thirty seven , thirty eight , thirty nine , forty\n      ,\n    \n  \n\n\n\n\nX,y = dls.one_batch()"
  },
  {
    "objectID": "posts/2021-05-14-text_dls_exploration.html#using-datablock---datasets---dataloaders",
    "href": "posts/2021-05-14-text_dls_exploration.html#using-datablock---datasets---dataloaders",
    "title": "fastai’s Text DLs Exploration",
    "section": "Using DataBlock -> datasets -> dataloaders",
    "text": "Using DataBlock -> datasets -> dataloaders\nAnother way to get dataloaders is to use DataBlock. DataBlock wants to know what type of data will be passed which can be specified to blocks. It also wants a splitter and the functions to get_x and get_y\n\nblocks = [TransformBlock(type_tfms=[TitledStringDecoder, Numericalize(vocab)]), # x piece\n          TransformBlock(type_tfms=[TitledStringDecoder, Numericalize(vocab)])] # y piece\n\n\nsplits[-1]\n\n(#1147) [4588,4589,4590,4591,4592,4593,4594,4595,4596,4597...]\n\n\n\nIndexSplitter(splits[-1])(all_text_tok_chunked)\n\n((#4588) [0,1,2,3,4,5,6,7,8,9...],\n (#1147) [4588,4589,4590,4591,4592,4593,4594,4595,4596,4597...])\n\n\n\ndblock = DataBlock(blocks=blocks,\n                   splitter=IndexSplitter(splits[-1]),\n                   get_x=get_x,\n                   get_y=get_y)\n\nWith the dblock created, you can create a dset and then from the dset, you can create a dls similar to the one created above.\n\ndsets_via_dblock = dblock.datasets(all_text_tok_chunked)\n\n\ndsets_via_dblock\n\n(#5735) [(TensorText([ 2, 12,  9, 13,  9, 14,  9, 15,  9, 16]), TensorText([9])),(TensorText([17,  9, 18,  9, 20,  9, 19,  9, 29,  9]), TensorText([30])),(TensorText([ 9, 31,  9, 32,  9, 33,  9, 34,  9, 35]), TensorText([9])),(TensorText([36,  9, 37,  9, 38,  9, 21,  9, 21, 12]), TensorText([9])),(TensorText([21, 13,  9, 21, 14,  9, 21, 15,  9, 21]), TensorText([16])),(TensorText([ 9, 21, 17,  9, 21, 18,  9, 21, 20,  9]), TensorText([21])),(TensorText([19,  9, 22,  9, 22, 12,  9, 22, 13,  9]), TensorText([22])),(TensorText([14,  9, 22, 15,  9, 22, 16,  9, 22, 17]), TensorText([9])),(TensorText([22, 18,  9, 22, 20,  9, 22, 19,  9, 23]), TensorText([9])),(TensorText([23, 12,  9, 23, 13,  9, 23, 14,  9, 23]), TensorText([15]))...]\n\n\n\ndsets_via_dblock.show(dsets_via_dblock[0])\n\nxxbos one , two , three , four , five\n,\n\n\n\ndls = dsets_via_dblock.dataloaders(bs=16,shuffle=False, drop_last=True)\n\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos one , two , three , four , five\n      ,\n    \n    \n      1\n      six , seven , eight , nine , ten ,\n      eleven\n    \n    \n      2\n      , twelve , thirteen , fourteen , fifteen , sixteen\n      ,\n    \n    \n      3\n      seventeen , eighteen , nineteen , twenty , twenty one\n      ,\n    \n    \n      4\n      twenty two , twenty three , twenty four , twenty\n      five\n    \n    \n      5\n      , twenty six , twenty seven , twenty eight ,\n      twenty\n    \n    \n      6\n      nine , thirty , thirty one , thirty two ,\n      thirty\n    \n    \n      7\n      three , thirty four , thirty five , thirty six\n      ,\n    \n    \n      8\n      thirty seven , thirty eight , thirty nine , forty\n      ,"
  },
  {
    "objectID": "posts/2021-05-14-text_dls_exploration.html#using-datablock---dataloaders",
    "href": "posts/2021-05-14-text_dls_exploration.html#using-datablock---dataloaders",
    "title": "fastai’s Text DLs Exploration",
    "section": "Using DataBlock -> dataloaders",
    "text": "Using DataBlock -> dataloaders\nAnother option is to go directly from dblock to dls with dblock.dataloaders. Behind the scenes this is creating a dataset as well, but it can be a cleaner looking way to handle it if you always go from dblock -> dls.\n\ndls = dblock.dataloaders(all_text_tok_chunked, bs=16, shuffle=False, drop_last=True)\n\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos one , two , three , four , five\n      ,\n    \n    \n      1\n      six , seven , eight , nine , ten ,\n      eleven\n    \n    \n      2\n      , twelve , thirteen , fourteen , fifteen , sixteen\n      ,\n    \n    \n      3\n      seventeen , eighteen , nineteen , twenty , twenty one\n      ,\n    \n    \n      4\n      twenty two , twenty three , twenty four , twenty\n      five\n    \n    \n      5\n      , twenty six , twenty seven , twenty eight ,\n      twenty\n    \n    \n      6\n      nine , thirty , thirty one , thirty two ,\n      thirty\n    \n    \n      7\n      three , thirty four , thirty five , thirty six\n      ,\n    \n    \n      8\n      thirty seven , thirty eight , thirty nine , forty\n      ,"
  },
  {
    "objectID": "posts/2021-05-14-text_dls_exploration.html#conclusion",
    "href": "posts/2021-05-14-text_dls_exploration.html#conclusion",
    "title": "fastai’s Text DLs Exploration",
    "section": "Conclusion",
    "text": "Conclusion\nCreating dls is an extremely important capability when using fastai because that is what a learn expects to deal with all of the data. There are many different ways to get a dls object created so this isn’t a comprehensive list, but at least shows a few ways to do the task. In a future blog post, I will be using this dls and exploring transformer models with it. Hopefully this will help others get their DLs working.\nI’d like to give a special thanks to Arto for helping me get things working properly and everybody in the fastai discord channel for dealing with my questions and for creating a great community to learn with every step of the way."
  },
  {
    "objectID": "posts/2021-05-14-text_dls_exploration.html#useful-links",
    "href": "posts/2021-05-14-text_dls_exploration.html#useful-links",
    "title": "fastai’s Text DLs Exploration",
    "section": "Useful Links",
    "text": "Useful Links\nhttps://arampacha.github.io/thoughtsamples/fastai/pytorch/2021/01/02/transformer-lm-from-scratch.html\nhttps://github.com/fastai/fastai/blob/ab154927696338741e59e0ffc4774777c4a9781c/nbs/39_tutorial.transformers.ipynb\nhttps://github.com/fastai/fastai/blob/ab154927696338741e59e0ffc4774777c4a9781c/dev_nbs/course/lesson7-human-numbers.ipynb"
  },
  {
    "objectID": "posts/2022-02-07-python-function-default-argument-value.html",
    "href": "posts/2022-02-07-python-function-default-argument-value.html",
    "title": "Python Function Default Argument Value",
    "section": "",
    "text": "This post is an exploration into when a python function gets the default argument from a function signature. Here is the scenario that got me to this point:\nI had a chunk of code that had a filename defined.\n\nfrom datetime import datetime\nfrom time import sleep\n\ndef save_file_w_timestamp(filename=f'{datetime.now().isoformat()}/file.csv'):\n    print(filename)\n\n\nsave_file_w_timestamp()\nsleep(5)\nsave_file_w_timestamp()\n\n2022-02-02T21:00:58.160733/file.csv\n2022-02-02T21:00:58.160733/file.csv\n\n\nMy initial thought was that these two function calls would return the time that the function was called, but it doesn’t. That is what we will explore in this blog post.\nLet’s explore how to make this work as expected."
  },
  {
    "objectID": "posts/2022-02-07-python-function-default-argument-value.html#if-youre-in-a-hurry",
    "href": "posts/2022-02-07-python-function-default-argument-value.html#if-youre-in-a-hurry",
    "title": "Python Function Default Argument Value",
    "section": "If you’re in a hurry:",
    "text": "If you’re in a hurry:\n\ndef save_file_w_None(filename=None):\n    if filename is None: filename=f'{datetime.now().isoformat()}/file.csv'\n    print(filename)"
  },
  {
    "objectID": "posts/2022-02-07-python-function-default-argument-value.html#failed-attempt-1-passing-through-a-function",
    "href": "posts/2022-02-07-python-function-default-argument-value.html#failed-attempt-1-passing-through-a-function",
    "title": "Python Function Default Argument Value",
    "section": "Failed Attempt #1: passing through a function",
    "text": "Failed Attempt #1: passing through a function\n\ndef get_current_datetime():\n    return datetime.now().isoformat()\n\ndef save_file_w_timestamp(filename=f'{get_current_datetime()}/file.csv'):\n    print(filename)\n\n\nsave_file_w_timestamp()\nsleep(5)\nsave_file_w_timestamp()\n\n2022-02-02T21:01:03.198774/file.csv\n2022-02-02T21:01:03.198774/file.csv"
  },
  {
    "objectID": "posts/2022-02-07-python-function-default-argument-value.html#attempt-2-brute-force",
    "href": "posts/2022-02-07-python-function-default-argument-value.html#attempt-2-brute-force",
    "title": "Python Function Default Argument Value",
    "section": "Attempt #2: Brute force",
    "text": "Attempt #2: Brute force\n\ndef save_file_w_timestamp(filename=None):\n    if filename is None: filename=f'{datetime.now().isoformat()}/file.csv'\n    print(filename)\n\n\nsave_file_w_timestamp()\nsleep(5)\nsave_file_w_timestamp()\n\n2022-02-02T21:01:17.708874/file.csv\n2022-02-02T21:01:22.712349/file.csv"
  },
  {
    "objectID": "posts/2022-02-07-python-function-default-argument-value.html#exciting-attempt-3-mutable-madness",
    "href": "posts/2022-02-07-python-function-default-argument-value.html#exciting-attempt-3-mutable-madness",
    "title": "Python Function Default Argument Value",
    "section": "Exciting Attempt #3: Mutable Madness",
    "text": "Exciting Attempt #3: Mutable Madness\nThis third example surprised me a lot (thank you to miwojc on the fastai discord for bringing it to my attention!). If you have an empty list as your default value, it seems innocent enough. Naive Kevin from yesterday would have assumed that this code would create an empty list if x was not passed. Naive Kevin would be sadly mistaken. This is a really good example of what is actually happening above. This creates a variable x that starts as an empty list, but let’s see what happens when we call the function.\n\ndef mutable_madness(x=[]):\n    x.append(1)\n    print(x)\n\n\nmutable_madness()\n\n[1]\n\n\nWhat do you think the value is going to be here?\n\n#collapse_output\nmutable_madness()\n\n[1, 1]\n\n\nHow about if we pass an empty list in?\n\n#collapse_output\nmutable_madness([])\n\n[1]\n\n\nAnd what about now?\n\n#collapse_output\nmutable_madness()\n\n[1, 1, 1]\n\n\nI got all of these wrong when I was initially coding this so if it doesn’t seem intuitive to you, just know you aren’t alone. This is a fairly common gotcha that can lead to frustrating bugs. Here is another good blog post for further reading: https://docs.python-guide.org/writing/gotchas/.\nJust to explore a few more ideas from this concept, I am going to add a few more examples below.\n\nj=1\ndef immutable_nonmadness(y=j):\n    #global j #This could be added to allow j to be used inside and outside the function.  \n    y+=1\n    print(y)\n\nMy initial thought with this example was that j would keep incrementing because we are setting j inside of our function. This is actually a good lesson about context which I won’t get into a ton except to mention that the j in line 1 and line 2 are the same j and the j in line 3 is a different j which is only accessible inside of the function. If we wanted this to behave similarly to the functions above which kept using the default value from above, the global argument would need to be added but this really is using a different concept to keep incrementing the value once we introduce global.\n\nimmutable_nonmadness()\n\n2\n\n\n\n#collapse_output\nimmutable_nonmadness()\n\n2\n\n\nIn this example, because the value 1 is an immutable object, it doesn’t hold onto the previous value but if instead, we had put an empty list in j, it would act the same way as the examples from above. This is because a variable is neither mutable nor immutable. A variable is give its type and therefor its mutability based on the object it is storing.\n\nj=[]\ndef function_1(y=j):\n    y+=[1]\n    print(y)\n\n\nfunction_1()\n\n[1]\n\n\n\nfunction_1()\n\n[1, 1]\n\n\nThis behavior will happen with lists, dicts, sets, and most custom classes.\n\ndef function_dict(value, x={}):\n    x[value] = len(x)\n    print(x)\n\n\nfunction_dict('thing 0')\n\n{'thing 0': 0}\n\n\n\nfunction_dict('thing 1')\n\n{'thing 0': 0, 'thing 1': 1}\n\n\n\ndef function_set(value, x=set()):\n    x.add(value)\n    print(x)\n\n\nfunction_set('thing 0')\n\n{'thing 0'}\n\n\n\nfunction_set('thing 1')\n\n{'thing 0', 'thing 1'}\n\n\nI hope you learned something going through this blog post and if you take one thing away from this it is to be mindful when setting your default arguments."
  },
  {
    "objectID": "posts/2022-02-05-GitHook_to_clean_notebook.html",
    "href": "posts/2022-02-05-GitHook_to_clean_notebook.html",
    "title": "Using Git Hook to Clean Jupyter Notebook on Commit",
    "section": "",
    "text": "To avoid merge issues and make PR review easier, we wanted a Git Hook which will: - Clear output of Jupyter Notebook - Only clean the files that were modified for this PR\nThere are many hook samples in .git/hooks folder. You want to create a file called pre-commit with the following code:\n#!/bin/sh\nfor file in $(git diff --diff-filter=d --cached --name-only | grep -E 'customers/.+\\.ipynb$')\ndo\n    jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace \"$file\"\n    git add \"$file\"\ndone\n[Download]\nRun chmod +x pre-commit to make the file executable.\nThat’s it!\nIn the next post, we will show you how to create this Git Hook with Makefile.\n\n\nIf the above code gives you the following error:\n[NbConvertApp] WARNING | Config option `template_path` not recognized by `NotebookExporter`.\nYou need to run pip install -U nbconvert to get a newer version (https://github.com/jupyter/nbconvert/issues/1229#issuecomment-608721332)."
  },
  {
    "objectID": "posts/2022-09-26-5-Ways-to-Improve-Your-Quality-Inspection-Process.html",
    "href": "posts/2022-09-26-5-Ways-to-Improve-Your-Quality-Inspection-Process.html",
    "title": "5 Ways to Improve Your Quality Inspection Process",
    "section": "",
    "text": "5 Ways to Improve Your Quality Inspection Process\nIf you work in manufacturing and are struggling to with rising costs, returned products, and unhappy customers, check out some of my ideas below on how to improve your quality inspection process. If you have any other ones to add, I would love to hear from you in the comments below!\n\n1. Identify KPIs and Loopholes\nKey Performance Indicators (KPIs) are used to measure a process’ success. One problem with KPIs is that while they may track progress towards a goal, they can often be gamed and measurements can be tweaked to make a metric look better than it truly is. A good KPI is something that is automatically captured from machines and requires little if any human input. To help identify a good KPI, first ask your team what the goal is. For some groups, this may be reducing the amount of defective product shipped to a customer. Others may just want to generate the most value for the plant. These are both great goals, but they are too slow to be effective KPIs.\nOnce you have the end goal in mind, dig a layer deeper. What actions go into improving that goal? Let’s use shipping less defective product to end customers as our example goal. This is a worthy goal, but how do you know if you are moving in the right direction? An effective KPI will help answer that question hourly or daily rather than quarterly. Maybe your plant currently has a lot of manual inspection throughout the production line. If this is the case, tracking the amount of product being discarded at each station may be a good KPI to track. A loophole here might be that more product is let through from one station to the next. Keep these potential loopholes in mind and mitigate them where possible. Each plant is different and the best KPIs to track will often differ. Consider what your goals are, what can be tracked to help reach that goal, and what types of unintended consequences could result.\n\n\n2. Create a Process Diagram\nWork with your team to create a Swim Lane Diagram. This will give you a better idea what tasks need to be done by what team at what point in the process. If there are multiple tasks that are done in parallel, which task takes longer to complete? Focusing on the tasks and teams that are involved in a process will help to understand where resources should be put in order to speed up the overall process.\n\n\n3. Simplify the Tasks\n\n“Make everything as simple as possible, but not simpler” - Albert Einstein\n\nOnce you have identified your process bottlenecks, work with your team to break the task down. The goal here is to ensure that any work that is being done is adding value to the process and requires a person. At this point, start considering whether a process can be either partially or fully automated.\n\n\n4. Focus on Quick Wins\nOne mistake that many companies make when improving their quality inspection process is that they try to tackle too hard of a problem right away. This is an advanced strategy and unless you have a very solid team, it is probably better to start with a smaller process and generate value quicker. Once your team has started improving the workflow, the larger processes will be easier to optimize. If you feel like there are no smaller processes that need to be optimized, this probably means that the tasks should be broken down into smaller pieces.\n\n\n5. Work with Your Subject Matter Experts\nDon’t try to improve the process without working with the people that know the process best. There are typically years of expertise ready and willing to assist in process improvement. If you want to generate the highest return on investment from your quality inspection improvements, these team members will help you get there.\n\n\nConclusion\nHopefully these tasks give you some ideas on how you can improve your quality inspection process. If you are thinking about diving into your quality inspection process and you want an outside perspective to help you, I would love to hear from you. We can help you automate your quality inspection process whether you need deep learning to differentiate defects, a custom application built to make it easier to track your data, or a web scraping tool to retrieve data for you before you even request it. We will work with you no matter what stage of your journey you are in and will align our pricing to ensure we are both working towards the same goal"
  }
]